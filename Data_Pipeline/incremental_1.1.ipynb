{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6052fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Training bottleneck AE on base data...\n",
      "[*] Extracting latent features for base training...\n",
      "[*] Starting stream...\n",
      "[+] Handling unknown buffer of size 20 at stream idx 19\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 34\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 49\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 64\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 79\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 94\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 109\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 124\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 139\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 154\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 169\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 184\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 199\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 214\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 229\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 244\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 259\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 274\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 289\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 304\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 319\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 334\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 349\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 364\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 379\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 394\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 409\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 424\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 439\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 454\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 469\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 484\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 499\n",
      "   -> clusters found: []; rep_labels: []\n",
      "[+] Handling unknown buffer of size 20 at stream idx 514\n",
      "   -> clusters found: []; rep_labels: []\n",
      "\\n=== Demo Summary ===\n",
      "Streamed samples processed: 520\n",
      "Final supervised-known accuracy on held-out known samples: 0.0000\n",
      "Zero-day detection recall on holdout: 1.0000\n",
      "Known-classes F1 (macro): 0.0\n"
     ]
    }
   ],
   "source": [
    "# incremental_ids_pipeline.py\n",
    "# Full incremental IDS pipeline combining bAE -> OCSVM -> AE -> XGBoost\n",
    "# with online detection -> buffer unknowns -> clustering -> simulated analyst labeling\n",
    "# -> incremental fine-tune using Replay + EWC to mitigate catastrophic forgetting.\n",
    "#\n",
    "# Usage: python incremental_ids_pipeline.py\n",
    "# Requirements: torch, scikit-learn, xgboost, numpy\n",
    "# pip install torch scikit-learn xgboost numpy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random, copy\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.cluster import DBSCAN, MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "class BottleneckAE(nn.Module):\n",
    "    def __init__(self, input_dim=64, latent_dim=16, hidden=128):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(input_dim, hidden), nn.ReLU(), nn.Linear(hidden, latent_dim))\n",
    "        self.decoder = nn.Sequential(nn.Linear(latent_dim, hidden), nn.ReLU(), nn.Linear(hidden, input_dim))\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "        return recon, z\n",
    "\n",
    "\n",
    "class DetectorAE(nn.Module):\n",
    "    def __init__(self, input_dim=16, hidden=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(input_dim, hidden), nn.ReLU(),\n",
    "                                 nn.Linear(hidden, int(hidden/2)), nn.ReLU(),\n",
    "                                 nn.Linear(int(hidden/2), input_dim))\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=2000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def add(self, x: np.ndarray, y: int):\n",
    "        self.buffer.append((x.copy(), int(y)))\n",
    "    def sample(self, k: int):\n",
    "        k = min(k, len(self.buffer))\n",
    "        if k == 0:\n",
    "            return np.zeros((0,)), np.zeros((0,))\n",
    "        batch = random.sample(self.buffer, k)\n",
    "        xs = np.stack([b[0] for b in batch], axis=0)\n",
    "        ys = np.array([b[1] for b in batch], dtype=np.int64)\n",
    "        return xs, ys\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    def all(self):\n",
    "        xs = np.stack([b[0] for b in self.buffer], axis=0) if len(self.buffer)>0 else np.zeros((0,))\n",
    "        ys = np.array([b[1] for b in self.buffer], dtype=np.int64) if len(self.buffer)>0 else np.zeros((0,))\n",
    "        return xs, ys\n",
    "\n",
    "\n",
    "class EWC:\n",
    "    def __init__(self, model: nn.Module, data: np.ndarray, device='cpu'):\n",
    "        self.device = device\n",
    "        self.model = copy.deepcopy(model).to(self.device)\n",
    "        self.params = {n: p.clone().detach() for n, p in self.model.named_parameters() if p.requires_grad}\n",
    "        self.fisher = {n: torch.zeros_like(p, device=self.device) for n, p in self.model.named_parameters() if p.requires_grad}\n",
    "        self._estimate_fisher(data)\n",
    "\n",
    "    def _estimate_fisher(self, data: np.ndarray, samples=200):\n",
    "        if data.size == 0:\n",
    "            return\n",
    "        idxs = np.random.choice(len(data), size=min(len(data), samples), replace=False)\n",
    "        self.model.train()\n",
    "        for i in idxs:\n",
    "            x = torch.from_numpy(data[i]).float().to(self.device).unsqueeze(0)\n",
    "            self.model.zero_grad()\n",
    "            out = self.model(x)\n",
    "            if isinstance(out, tuple):\n",
    "                recon = out[0]\n",
    "            else:\n",
    "                recon = out\n",
    "            loss = F.mse_loss(recon, x)\n",
    "            loss.backward()\n",
    "            for n, p in self.model.named_parameters():\n",
    "                if p.grad is not None:\n",
    "                    self.fisher[n] += (p.grad.detach() ** 2)\n",
    "            self.model.zero_grad()\n",
    "        for n in self.fisher:\n",
    "            self.fisher[n] = self.fisher[n] / float(len(idxs))\n",
    "\n",
    "    def penalty(self, model: nn.Module):\n",
    "        loss = 0.0\n",
    "        for n, p in model.named_parameters():\n",
    "            if n in self.fisher:\n",
    "                _loss = self.fisher[n] * (p - self.params[n]).pow(2)\n",
    "                loss += _loss.sum()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class OODDetectorOCSVM:\n",
    "    def __init__(self, ocsvm_model: OneClassSVM, threshold=None):\n",
    "        self.ocsvm = ocsvm_model\n",
    "        self.threshold = threshold\n",
    "    def fit(self, Z: np.ndarray):\n",
    "        self.ocsvm.fit(Z)\n",
    "    def predict(self, z: np.ndarray):\n",
    "        single = False\n",
    "        if z.ndim == 1:\n",
    "            z = z[np.newaxis, :]\n",
    "            single = True\n",
    "        pred = self.ocsvm.predict(z)\n",
    "        if single:\n",
    "            return int(pred[0])\n",
    "        return pred\n",
    "\n",
    "\n",
    "def cluster_unknowns(X: np.ndarray, method='dbscan', **kwargs):\n",
    "    if len(X) == 0:\n",
    "        return np.array([])\n",
    "    Xs = StandardScaler().fit_transform(X)\n",
    "    if method == 'dbscan':\n",
    "        eps = kwargs.get('eps', 0.6)\n",
    "        min_samples = kwargs.get('min_samples', 5)\n",
    "        cl = DBSCAN(eps=eps, min_samples=min_samples).fit(Xs)\n",
    "        return cl.labels_\n",
    "    elif method == 'mbk':\n",
    "        k = kwargs.get('k', 3)\n",
    "        mbk = MiniBatchKMeans(n_clusters=k, batch_size=64, random_state=SEED).fit(Xs)\n",
    "        return mbk.labels_\n",
    "    else:\n",
    "        return np.zeros(len(X), dtype=int)\n",
    "\n",
    "\n",
    "def make_synthetic_classes(input_dim=64, base_classes=3, samples_per_class=150):\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    Xs, Ys = [], []\n",
    "    for c in range(base_classes):\n",
    "        mu = rng.randn(input_dim) * (1 + c*0.5)\n",
    "        cov = np.eye(input_dim) * 0.12\n",
    "        xs = rng.multivariate_normal(mu, cov, size=samples_per_class)\n",
    "        ys = np.full((samples_per_class,), c, dtype=int)\n",
    "        Xs.append(xs); Ys.append(ys)\n",
    "    X = np.concatenate(Xs, axis=0)\n",
    "    Y = np.concatenate(Ys, axis=0)\n",
    "    perm = rng.permutation(len(X))\n",
    "    return X[perm], Y[perm]\n",
    "\n",
    "\n",
    "def make_zero_day(input_dim=64, shift=5.0, size=200):\n",
    "    rng = np.random.RandomState(999)\n",
    "    mu = rng.randn(input_dim) * 1.5 + shift\n",
    "    xs = rng.multivariate_normal(mu, np.eye(input_dim)*0.15, size=size)\n",
    "    ys = np.full((size,), 999, dtype=int)\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "def train_bAE(model: BottleneckAE, X: np.ndarray, epochs=10, batch_size=128, lr=1e-3, device='cpu', replay=None):\n",
    "    model.to(device); model.train()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    Xtensor = torch.from_numpy(X).float().to(device)\n",
    "    dataset = torch.utils.data.TensorDataset(Xtensor)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(epochs):\n",
    "        for (xb,) in loader:\n",
    "            opt.zero_grad()\n",
    "            recon, _ = model(xb)\n",
    "            loss = criterion(recon, xb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if replay is not None and len(replay) > 0:\n",
    "            rx, ry = replay.sample(min(256, len(replay)))\n",
    "            if len(rx) > 0:\n",
    "                rxt = torch.from_numpy(rx).float().to(device)\n",
    "                for _ in range(2):\n",
    "                    opt.zero_grad()\n",
    "                    recon_r, _ = model(rxt)\n",
    "                    loss_r = criterion(recon_r, rxt)\n",
    "                    loss_r.backward()\n",
    "                    opt.step()\n",
    "    model.to('cpu')\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_detector_ae(model: DetectorAE, Z_normal: np.ndarray, epochs=20, batch_size=64, lr=1e-3, device='cpu'):\n",
    "    model.to(device); model.train()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    Xt = torch.from_numpy(Z_normal).float().to(device)\n",
    "    loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(Xt), batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(epochs):\n",
    "        for (xb,) in loader:\n",
    "            opt.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, xb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "    model.to('cpu')\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_full_demo(verbose=True):\n",
    "    device = 'cpu'\n",
    "    input_dim = 64\n",
    "    latent_dim = 16\n",
    "    base_classes = 3\n",
    "\n",
    "    X_base, Y_base = make_synthetic_classes(input_dim=input_dim, base_classes=base_classes, samples_per_class=100)\n",
    "    X_zd, Y_zd = make_zero_day(input_dim=input_dim, shift=6.0, size=220)\n",
    "    X_all = np.concatenate([X_base, X_zd], axis=0)\n",
    "    Y_all = np.concatenate([Y_base, Y_zd], axis=0)\n",
    "\n",
    "    mask_base = Y_all != 999\n",
    "    X_base_only = X_all[mask_base]\n",
    "    Y_base_only = Y_all[mask_base]\n",
    "    X_train_sup, X_test_sup, y_train_sup, y_test_sup = train_test_split(X_base_only, Y_base_only, test_size=0.2, random_state=SEED)\n",
    "\n",
    "    X_zd_hold = X_zd[:80]; Y_zd_hold = Y_zd[:80]\n",
    "    X_test_eval = np.concatenate([X_test_sup, X_zd_hold], axis=0)\n",
    "    y_test_eval = np.concatenate([y_test_sup, Y_zd_hold], axis=0)\n",
    "\n",
    "    bAE = BottleneckAE(input_dim=input_dim, latent_dim=latent_dim, hidden=128)\n",
    "    detector_ae = DetectorAE(input_dim=latent_dim, hidden=64)\n",
    "\n",
    "    print(\"[*] Training bottleneck AE on base data...\")\n",
    "    train_bAE(bAE, X_train_sup, epochs=2, batch_size=256, lr=1e-3)\n",
    "    print(\"[*] Extracting latent features for base training...\")\n",
    "    bAE.eval()\n",
    "    with torch.no_grad():\n",
    "        Z_train = bAE.encoder(torch.from_numpy(X_train_sup).float()).numpy()\n",
    "        Z_test = bAE.encoder(torch.from_numpy(X_test_sup).float()).numpy()\n",
    "\n",
    "    ocsvm = OneClassSVM(kernel='rbf', gamma='scale', nu=0.03)\n",
    "    ood = OODDetectorOCSVM(ocsvm)\n",
    "    ood.fit(Z_train)\n",
    "\n",
    "    detector_ae = train_detector_ae(detector_ae, Z_train, epochs=4, batch_size=128, lr=1e-3)\n",
    "\n",
    "    Z_train_all = bAE.encoder(torch.from_numpy(X_train_sup).float()).detach().numpy()\n",
    "    xgb_clf = xgb.XGBClassifier(n_estimators=200, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss')\n",
    "    xgb_clf.fit(Z_train_all, y_train_sup)\n",
    "\n",
    "    replay = ReplayBuffer(capacity=2000)\n",
    "    for i in range(0, min(1000, len(X_train_sup))):\n",
    "        replay.add(X_train_sup[i], int(y_train_sup[i]))\n",
    "\n",
    "    rng = np.random.RandomState(SEED+1)\n",
    "    idxs = rng.permutation(len(X_all))\n",
    "    stream_X = X_all[idxs]\n",
    "    stream_Y = Y_all[idxs]\n",
    "\n",
    "    unknown_buffer = []\n",
    "    buffer_limit = 20 #90\n",
    "    cluster_method = 'dbscan'\n",
    "    cluster_eps = 0.7\n",
    "    cluster_min_samples = 6\n",
    "\n",
    "    pa_preds = []\n",
    "    pa_true = []\n",
    "    detection_flags = []\n",
    "    stream_count = 0\n",
    "\n",
    "    periodic_every = 40  #200\n",
    "    lam_ewc = 200.0\n",
    "\n",
    "    ewc_bae = None\n",
    "    ewc_detector = None\n",
    "\n",
    "    print(\"[*] Starting stream...\")\n",
    "    for i, (x_raw, y_true) in enumerate(zip(stream_X, stream_Y)):\n",
    "        stream_count += 1\n",
    "        with torch.no_grad():\n",
    "            z = bAE.encoder(torch.from_numpy(x_raw).float().unsqueeze(0)).numpy()[0]\n",
    "        oc_pred = ood.predict(z)\n",
    "        recon = detector_ae(torch.from_numpy(z).float().unsqueeze(0)).detach().numpy()[0]\n",
    "        recon_err = float(np.mean((recon - z)**2))\n",
    "        is_unknown = (oc_pred == -1) or (recon_err > np.percentile(np.mean(bAE.encoder(torch.from_numpy(X_train_sup).float()).detach().numpy(), axis=1), 95) + 0.0)\n",
    "        if is_unknown:\n",
    "            unknown_buffer.append(x_raw.copy())\n",
    "            detection_flags.append(1)\n",
    "            final_label = -1\n",
    "        else:\n",
    "            pred = xgb_clf.predict(z.reshape(1, -1))[0]\n",
    "            final_label = int(pred)\n",
    "            detection_flags.append(0)\n",
    "        pa_preds.append(final_label)\n",
    "        pa_true.append(int(y_true) if y_true != 999 else -2)\n",
    "\n",
    "        if (len(unknown_buffer) >= buffer_limit) or (stream_count % periodic_every == 0 and len(unknown_buffer) > 20):\n",
    "            print(f\"[+] Handling unknown buffer of size {len(unknown_buffer)} at stream idx {i}\")\n",
    "            Xbuf = np.stack(unknown_buffer, axis=0)\n",
    "            with torch.no_grad():\n",
    "                Zbuf = bAE.encoder(torch.from_numpy(Xbuf).float()).numpy()\n",
    "            labels = cluster_unknowns(Zbuf, method=cluster_method, eps=cluster_eps, min_samples=cluster_min_samples)\n",
    "            cluster_member_idxs = defaultdict(list)\n",
    "            for idxc, cl in enumerate(labels):\n",
    "                if cl != -1:\n",
    "                    cluster_member_idxs[cl].append(idxc)\n",
    "            reps = []\n",
    "            for cl, idxs_list in cluster_member_idxs.items():\n",
    "                centroid = Zbuf[idxs_list].mean(axis=0)\n",
    "                reps.append(centroid)\n",
    "            rep_labels = []\n",
    "            for rep in reps:\n",
    "                dists = np.linalg.norm(stream_X - rep, axis=1)\n",
    "                nearest_idx = int(np.argmin(dists))\n",
    "                rep_labels.append(int(stream_Y[nearest_idx]) if stream_Y[nearest_idx] != 999 else 999)\n",
    "            print(f\"   -> clusters found: {list(cluster_member_idxs.keys())}; rep_labels: {rep_labels}\")\n",
    "            new_labeled = []\n",
    "            for cl_idx, lab in zip(cluster_member_idxs.keys(), rep_labels):\n",
    "                member_idxs = cluster_member_idxs[cl_idx]\n",
    "                member_raw = Xbuf[member_idxs]\n",
    "                if lab == 999:\n",
    "                    new_label_id = max([int(v) for v in y_train_sup] + [0]) + 1 + random.randint(0,0)\n",
    "                    lab_assigned = new_label_id\n",
    "                else:\n",
    "                    lab_assigned = lab\n",
    "                for s in member_raw[:40]:\n",
    "                    new_labeled.append((s, lab_assigned))\n",
    "            if len(new_labeled) == 0:\n",
    "                unknown_buffer = unknown_buffer[-int(buffer_limit/4):]\n",
    "                continue\n",
    "            new_X = np.stack([t[0] for t in new_labeled], axis=0)\n",
    "            new_y = np.array([t[1] for t in new_labeled], dtype=int)\n",
    "            for xx, yy in zip(new_X, new_y):\n",
    "                replay.add(xx, int(yy))\n",
    "            rx, ry = replay.sample(min(600, len(replay)))\n",
    "            if len(rx) == 0:\n",
    "                rx = X_train_sup[:200]; ry = y_train_sup[:200]\n",
    "            with torch.no_grad():\n",
    "                Z_replay = bAE.encoder(torch.from_numpy(rx).float()).numpy()\n",
    "            print(\"   -> Building EWC from replay samples...\")\n",
    "            ewc_bae = EWC(bAE, rx, device=device)\n",
    "            ewc_detector = EWC(detector_ae, Z_replay, device=device)\n",
    "            combined_X = np.vstack([rx, new_X])\n",
    "            print(f\"   -> Fine-tuning bAE on {len(combined_X)} samples (replay+new) ...\")\n",
    "            bAE.to(device); bAE.train()\n",
    "            opt = optim.Adam(bAE.parameters(), lr=5e-4)\n",
    "            criterion = nn.MSELoss()\n",
    "            Xct = torch.from_numpy(combined_X).float().to(device)\n",
    "            loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(Xct), batch_size=128, shuffle=True)\n",
    "            for epoch in range(6):\n",
    "                for (xb,) in loader:\n",
    "                    opt.zero_grad()\n",
    "                    recon, _ = bAE(xb)\n",
    "                    loss = criterion(recon, xb)\n",
    "                    if ewc_bae is not None:\n",
    "                        loss = loss + (lam_ewc * 1e-3) * ewc_bae.penalty(bAE)\n",
    "                    loss.backward(); opt.step()\n",
    "            bAE.to('cpu')\n",
    "            with torch.no_grad():\n",
    "                Z_new = bAE.encoder(torch.from_numpy(new_X).float()).numpy()\n",
    "                Z_replay = bAE.encoder(torch.from_numpy(rx).float()).numpy()\n",
    "            Z_comb = np.vstack([Z_replay, Z_new])\n",
    "            detector_ae = train_detector_ae(detector_ae, Z_comb, epochs=8, batch_size=64, lr=1e-3)\n",
    "            with torch.no_grad():\n",
    "                Z_for_oc = bAE.encoder(torch.from_numpy(rx).float()).numpy()\n",
    "            try:\n",
    "                ood.fit(Z_for_oc)\n",
    "            except Exception as e:\n",
    "                print(\"   -> OCSVM retrain failed\", e)\n",
    "            rx_sup, ry_sup = replay.sample(min(800, len(replay)))\n",
    "            with torch.no_grad():\n",
    "                Z_sup = bAE.encoder(torch.from_numpy(rx_sup).float()).numpy()\n",
    "            try:\n",
    "                xgb_clf = xgb.XGBClassifier(n_estimators=200, learning_rate=0.05, use_label_encoder=False, eval_metric='logloss')\n",
    "                xgb_clf.fit(Z_sup, ry_sup)\n",
    "            except Exception as e:\n",
    "                print(\"   -> XGBoost incremental update failed:\", e)\n",
    "            unknown_buffer = []\n",
    "            print(\"   -> Incremental update done. Continue streaming...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Z_eval = bAE.encoder(torch.from_numpy(X_test_eval).float()).numpy()\n",
    "    preds = []\n",
    "    for z in Z_eval:\n",
    "        ocp = ood.predict(z)\n",
    "        recon = detector_ae(torch.from_numpy(z).float().unsqueeze(0)).detach().numpy()[0]\n",
    "        recon_err = float(np.mean((recon - z)**2))\n",
    "        threshold_recon = np.percentile(np.mean(bAE.encoder(torch.from_numpy(X_train_sup).float()).detach().numpy(), axis=1), 95)\n",
    "        if (ocp == -1) or (recon_err > threshold_recon):\n",
    "            preds.append(-1)\n",
    "        else:\n",
    "            p = xgb_clf.predict(z.reshape(1, -1))[0]\n",
    "            preds.append(int(p))\n",
    "    gt = np.array([int(v) if v!=999 else -1 for v in y_test_eval])\n",
    "    mask_known = gt != -1\n",
    "    known_acc = accuracy_score(gt[mask_known], np.array(preds)[mask_known]) if mask_known.sum()>0 else 0.0\n",
    "    mask_zd = gt == -1\n",
    "    zd_detected = np.array(preds)[mask_zd] == -1 if mask_zd.sum()>0 else np.array([])\n",
    "    zd_recall = float(np.mean(zd_detected)) if mask_zd.sum()>0 else 0.0\n",
    "    print(\"\\\\n=== Demo Summary ===\")\n",
    "    print(f\"Streamed samples processed: {len(stream_X)}\")\n",
    "    print(f\"Final supervised-known accuracy on held-out known samples: {known_acc:.4f}\")\n",
    "    print(f\"Zero-day detection recall on holdout: {zd_recall:.4f}\")\n",
    "    if mask_known.sum()>0:\n",
    "        print(\"Known-classes F1 (macro):\", f1_score(gt[mask_known], np.array(preds)[mask_known], average='macro'))\n",
    "    return {\"known_acc\": known_acc, \"zd_recall\": zd_recall, \"preds\": preds, \"gt\": gt}\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_full_demo(verbose=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kltn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
