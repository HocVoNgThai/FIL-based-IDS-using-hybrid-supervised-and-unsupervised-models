{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13776143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import typing as T\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ML libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "## --- CLASS / FUNC from file --- ##\n",
    "from bAE import AdvancedDimReducerAE\n",
    "from DeepAE import DeepAnomalyAE\n",
    "from EWC import EWC\n",
    "from train_model import train_model, plot_evaluation, calculate_metrics\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Hyperparameters\n",
    "LATENT_DIM = 16\n",
    "AE_HIDDEN = 64\n",
    "BATCH_SIZE = 128\n",
    "AE_LR = 1e-3\n",
    "AE_EPOCHS = 30\n",
    "BATCH_KMEANS = 1000\n",
    "\n",
    "BUFFER_MAX = 10000\n",
    "CLUSTER_PERIOD = 300  # seconds or you can make it call-based\n",
    "CLUSTER_MIN_SAMPLES = 50  # minimum buffered samples to run clustering\n",
    "NUM_CLUSTERS = 8  # fallback K means clusters\n",
    "EWC_LAMBDA = 100.0  # regularization weight (tweak)\n",
    "REPLAY_SIZE = 1000\n",
    "\n",
    "# ----------------------------\n",
    "# Utils\n",
    "# ----------------------------\n",
    "def to_device(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.to(DEVICE)\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ff043",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from collections import deque\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Optional: HDBSCAN for clustering\n",
    "try:\n",
    "    import hdbscan\n",
    "    HAS_HDBSCAN = True\n",
    "except:\n",
    "    HAS_HDBSCAN = False\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "AE_HIDDEN = 64\n",
    "LATENT_DIM = 16\n",
    "BATCH_SIZE = 128\n",
    "AE_LR = 1e-3\n",
    "AE_EPOCHS = 25\n",
    "EWC_LAMBDA = 50.0\n",
    "CLUSTER_MIN = 30\n",
    "NUM_CLUSTERS = 6\n",
    "\n",
    "BUFFER_MAX = 2000\n",
    "REPLAY_SIZE = 1000\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# MODEL DEFINITIONS\n",
    "# -----------------------------------------------------\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden=AE_HIDDEN, latent_dim=LATENT_DIM):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        out = self.decoder(z)\n",
    "        return out\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "class EWC:\n",
    "    \"\"\"Elastic Weight Consolidation regularizer\"\"\"\n",
    "    def __init__(self, model, dataloader, device=DEVICE):\n",
    "        self.model = model\n",
    "        self.params = {n: p.clone().detach() for n, p in model.named_parameters() if p.requires_grad}\n",
    "        self.fisher = {n: torch.zeros_like(p) for n, p in model.named_parameters() if p.requires_grad}\n",
    "        self.device = device\n",
    "        self._compute_fisher(dataloader)\n",
    "\n",
    "    def _compute_fisher(self, dataloader):\n",
    "        self.model.eval()\n",
    "        for x_batch, in dataloader:\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            out = self.model(x_batch)\n",
    "            loss = nn.MSELoss()(out, x_batch)\n",
    "            self.model.zero_grad()\n",
    "            loss.backward()\n",
    "            for n, p in self.model.named_parameters():\n",
    "                if p.grad is not None:\n",
    "                    self.fisher[n] += p.grad.detach() ** 2\n",
    "        for n in self.fisher:\n",
    "            self.fisher[n] /= len(dataloader)\n",
    "\n",
    "    def penalty(self, model):\n",
    "        loss = 0.0\n",
    "        for n, p in model.named_parameters():\n",
    "            if n in self.fisher:\n",
    "                loss += (self.fisher[n] * (p - self.params[n])**2).sum()\n",
    "        return loss\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# MAIN PIPELINE\n",
    "# -----------------------------------------------------\n",
    "class IncrementalIDS:\n",
    "    def __init__(self, input_dim):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.bae = AE(input_dim).to(DEVICE)\n",
    "        self.ae = AE(input_dim).to(DEVICE)\n",
    "        self.ocsvm = None\n",
    "        self.xgb = None\n",
    "        self.ewc_bae = None\n",
    "        self.ewc_ae = None\n",
    "        self.buffer = deque(maxlen=BUFFER_MAX)\n",
    "        self.replay = deque(maxlen=REPLAY_SIZE)\n",
    "\n",
    "    # ---------------- Offline ----------------\n",
    "    def train_autoencoder(self, model, X, epochs=AE_EPOCHS):\n",
    "        ds = DataLoader(TensorDataset(torch.tensor(X, dtype=torch.float32)),\n",
    "                        batch_size=BATCH_SIZE, shuffle=True)\n",
    "        opt = optim.Adam(model.parameters(), lr=AE_LR)\n",
    "        model.train()\n",
    "        for ep in range(epochs):\n",
    "            total_loss = 0\n",
    "            for (xb,) in ds:\n",
    "                xb = xb.to(DEVICE)\n",
    "                rec = model(xb)\n",
    "                loss = nn.MSELoss()(rec, xb)\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                total_loss += loss.item()\n",
    "            if ep % 10 == 0 or ep == epochs - 1:\n",
    "                print(f\"[AE] epoch {ep}/{epochs} loss={total_loss/len(ds):.6f}\")\n",
    "\n",
    "    def offline_train(self, X, y):\n",
    "        print(\"→ Scaling data...\")\n",
    "        Xs = self.scaler.fit_transform(X)\n",
    "\n",
    "        print(\"→ Training bAE & AE ...\")\n",
    "        self.train_autoencoder(self.bae, Xs)\n",
    "        self.train_autoencoder(self.ae, Xs)\n",
    "\n",
    "        print(\"→ Latent features ...\")\n",
    "        with torch.no_grad():\n",
    "            z = self.bae.encode(torch.tensor(Xs, dtype=torch.float32).to(DEVICE)).cpu().numpy()\n",
    "\n",
    "        print(\"→ Training OCSVM (on benign samples)...\")\n",
    "        benign_idx = (y == 0)\n",
    "        train_z = z[benign_idx] if benign_idx.sum() > 10 else z\n",
    "        self.ocsvm = OneClassSVM(nu=0.05, gamma='scale').fit(train_z)\n",
    "\n",
    "        print(\"→ Training XGBoost...\")\n",
    "        self.xgb = xgb.XGBClassifier(\n",
    "            n_estimators=150,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "        self.xgb.fit(Xs, y.astype(int))\n",
    "\n",
    "        # Estimate Fisher info for EWC\n",
    "        print(\"→ Estimating Fisher for EWC...\")\n",
    "        ds = DataLoader(TensorDataset(torch.tensor(Xs[:2000], dtype=torch.float32)), batch_size=128, shuffle=True)\n",
    "        self.ewc_bae = EWC(self.bae, ds)\n",
    "        self.ewc_ae = EWC(self.ae, ds)\n",
    "        print(\"✔ Offline training complete.\")\n",
    "\n",
    "    # ---------------- Online Detection ----------------\n",
    "    def detect(self, x, recon_th=0.5, xgb_conf=0.7):\n",
    "        xs = self.scaler.transform(x.reshape(1, -1))\n",
    "        xt = torch.tensor(xs, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = self.bae.encode(xt).cpu().numpy()\n",
    "            xrec = self.ae(xt)\n",
    "            rec_err = float(((xrec - xt)**2).mean().cpu())\n",
    "\n",
    "        oc = int(self.ocsvm.predict(z)[0])\n",
    "        proba = self.xgb.predict_proba(xs)[0]\n",
    "        conf = float(np.max(proba))\n",
    "        pred = int(np.argmax(proba))\n",
    "\n",
    "        unknown = (oc == -1 or rec_err > recon_th or conf < xgb_conf)\n",
    "        if unknown:\n",
    "            self.buffer.append({'x': xs[0], 'latent': z[0], 'label_pred': pred})\n",
    "            self.replay.append(xs[0])\n",
    "\n",
    "        return {\n",
    "            \"ocsvm\": oc, \"recon\": rec_err, \"xgb_pred\": pred,\n",
    "            \"xgb_conf\": conf, \"unknown\": unknown\n",
    "        }\n",
    "\n",
    "    # ---------------- Clustering & Labeling ----------------\n",
    "    def cluster_unknowns(self):\n",
    "        if len(self.buffer) < CLUSTER_MIN:\n",
    "            print(f\"[Cluster] Buffer too small ({len(self.buffer)}/{CLUSTER_MIN})\")\n",
    "            return []\n",
    "        X_latent = np.stack([b['latent'] for b in self.buffer])\n",
    "        if HAS_HDBSCAN:\n",
    "            clt = hdbscan.HDBSCAN(min_cluster_size=10)\n",
    "            labels = clt.fit_predict(X_latent)\n",
    "        else:\n",
    "            clt = MiniBatchKMeans(n_clusters=NUM_CLUSTERS)\n",
    "            labels = clt.fit_predict(X_latent)\n",
    "\n",
    "        clusters = []\n",
    "        for cid in np.unique(labels):\n",
    "            idxs = np.where(labels == cid)[0]\n",
    "            if cid == -1:  # noise\n",
    "                continue\n",
    "            # Simulate analyst: assign new label\n",
    "            new_label = int(self.xgb.classes_.max()) + 1\n",
    "            for i in idxs:\n",
    "                clusters.append({'x': self.buffer[i]['x'], 'label': new_label})\n",
    "        self.buffer.clear()\n",
    "        print(f\"[Cluster] labeled {len(clusters)} samples for incremental learning.\")\n",
    "        return clusters\n",
    "\n",
    "    # ---------------- Incremental update ----------------\n",
    "    def incremental_update(self, new_samples):\n",
    "        if len(new_samples) == 0:\n",
    "            return\n",
    "        X_new = np.stack([s['x'] for s in new_samples])\n",
    "        y_new = np.array([s['label'] for s in new_samples])\n",
    "\n",
    "        # update XGBoost\n",
    "        self.xgb.fit(X_new, y_new, xgb_model=self.xgb.get_booster())\n",
    "\n",
    "        # retrain OCSVM on new latent\n",
    "        with torch.no_grad():\n",
    "            z_new = self.bae.encode(torch.tensor(X_new, dtype=torch.float32).to(DEVICE)).cpu().numpy()\n",
    "        self.ocsvm.fit(z_new)\n",
    "\n",
    "        # fine-tune AE/bAE with EWC + replay\n",
    "        replay_data = list(self.replay)\n",
    "        if len(replay_data) > 0:\n",
    "            X_comb = np.vstack([np.stack(replay_data), X_new])\n",
    "        else:\n",
    "            X_comb = X_new\n",
    "        ds = DataLoader(TensorDataset(torch.tensor(X_comb, dtype=torch.float32)), batch_size=128, shuffle=True)\n",
    "\n",
    "        for model, ewc_obj, name in [(self.bae, self.ewc_bae, \"bAE\"), (self.ae, self.ewc_ae, \"AE\")]:\n",
    "            opt = optim.Adam(model.parameters(), lr=AE_LR/10)\n",
    "            model.train()\n",
    "            for ep in range(5):\n",
    "                tot = 0\n",
    "                for (xb,) in ds:\n",
    "                    xb = xb.to(DEVICE)\n",
    "                    rec = model(xb)\n",
    "                    loss = nn.MSELoss()(rec, xb)\n",
    "                    loss += EWC_LAMBDA * ewc_obj.penalty(model)\n",
    "                    opt.zero_grad()\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "                    tot += loss.item()\n",
    "                print(f\"[{name} fine-tune] epoch {ep}: {tot/len(ds):.6f}\")\n",
    "\n",
    "        print(\"✔ Incremental fine-tune complete.\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# LOAD CSV + DEMO\n",
    "# -----------------------------------------------------\n",
    "def main():\n",
    "    # --- Load dataset ---\n",
    "    df = pd.read_csv(\"dataset.csv\")\n",
    "    print(\"Loaded:\", df.shape)\n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[:, -1].values\n",
    "\n",
    "    # --- Offline training ---\n",
    "    ids = IncrementalIDS(input_dim=X.shape[1])\n",
    "    ids.offline_train(X, y)\n",
    "\n",
    "    # --- Online simulation ---\n",
    "    print(\"\\n[Simulation] Online detection ...\")\n",
    "    for i in range(0, len(X)):\n",
    "        out = ids.detect(X[i])\n",
    "        if i % 500 == 0:\n",
    "            print(f\"→ Processed {i} samples | Unknown buffer: {len(ids.buffer)}\")\n",
    "        if i % 1000 == 999:\n",
    "            labeled = ids.cluster_unknowns()\n",
    "            ids.incremental_update(labeled)\n",
    "\n",
    "    # Final cluster\n",
    "    labeled = ids.cluster_unknowns()\n",
    "    ids.incremental_update(labeled)\n",
    "\n",
    "    print(\"✅ Pipeline complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b05c21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling data...\n",
      "Training bottleneck AE (bAE)...\n",
      "[AE] epoch 0/30 loss=0.840614\n",
      "[AE] epoch 10/30 loss=0.346224\n",
      "[AE] epoch 20/30 loss=0.331581\n",
      "[AE] epoch 29/30 loss=0.323878\n",
      "Training auxiliary AE (AE)...\n",
      "[AE] epoch 0/30 loss=0.833965\n",
      "[AE] epoch 10/30 loss=0.347178\n",
      "[AE] epoch 20/30 loss=0.329390\n",
      "[AE] epoch 29/30 loss=0.323730\n",
      "Training OneClassSVM on latent features...\n",
      "Training XGBoost on known labels...\n",
      "Estimating Fisher for EWC (this may take time)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoang\\.conda\\envs\\kltn\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:47:49] WARNING: C:\\b\\abs_52v3kadn8m\\croot\\xgboost-split_1748343554494\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offline training complete.\n",
      "[Stream] processed 0 samples, buffer size 0\n",
      "[Stream] processed 200 samples, buffer size 1\n",
      "[Cluster] not enough unknowns (1 < 50)\n",
      "[Incremental] no new samples.\n",
      "[Stream] processed 400 samples, buffer size 2\n",
      "[Cluster] not enough unknowns (2 < 50)\n",
      "[Incremental] no new samples.\n",
      "[Stream] processed 600 samples, buffer size 2\n",
      "[Stream] processed 800 samples, buffer size 2\n",
      "[Cluster] not enough unknowns (2 < 50)\n",
      "[Incremental] no new samples.\n",
      "[Cluster] not enough unknowns (2 < 10)\n",
      "[Incremental] no new samples.\n",
      "Demo complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import typing as T\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ML libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "try:\n",
    "    import hdbscan\n",
    "    HAS_HDBSCAN = True\n",
    "except Exception:\n",
    "    HAS_HDBSCAN = False\n",
    "\n",
    "# ----------------------------\n",
    "# Config (tweak as needed)\n",
    "# ----------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Hyperparameters\n",
    "LATENT_DIM = 16\n",
    "AE_HIDDEN = 64\n",
    "BATCH_SIZE = 128\n",
    "AE_LR = 1e-3\n",
    "AE_EPOCHS = 30\n",
    "BATCH_KMEANS = 1000\n",
    "\n",
    "BUFFER_MAX = 10000\n",
    "CLUSTER_PERIOD = 300  # seconds or you can make it call-based\n",
    "CLUSTER_MIN_SAMPLES = 50  # minimum buffered samples to run clustering\n",
    "NUM_CLUSTERS = 8  # fallback K means clusters\n",
    "EWC_LAMBDA = 100.0  # regularization weight (tweak)\n",
    "REPLAY_SIZE = 1000\n",
    "\n",
    "# ----------------------------\n",
    "# Utils\n",
    "# ----------------------------\n",
    "def to_device(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.to(DEVICE)\n",
    "    return None\n",
    "\n",
    "# ----------------------------\n",
    "# Simple Autoencoder (bottleneck AE)\n",
    "# ----------------------------\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden=AE_HIDDEN, latent_dim=LATENT_DIM):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        xrec = self.decoder(z)\n",
    "        return xrec\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# ----------------------------\n",
    "# EWC helper (approx Fisher diag)\n",
    "# ----------------------------\n",
    "class EWC:\n",
    "    def __init__(self, model: nn.Module, dataloader: DataLoader, device=DEVICE):\n",
    "        self.model = copy.deepcopy(model).to(device)\n",
    "        self.device = device\n",
    "        self.params = {n: p.clone().detach() for n, p in self.model.named_parameters() if p.requires_grad}\n",
    "        # compute diagonal Fisher info\n",
    "        self.fisher = {n: torch.zeros_like(p) for n, p in self.model.named_parameters() if p.requires_grad}\n",
    "        self._compute_fisher(dataloader)\n",
    "\n",
    "    def _compute_fisher(self, dataloader):\n",
    "        self.model.eval()\n",
    "        for x_batch, in dataloader:\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            self.model.zero_grad()\n",
    "            # use negative log-likelihood surrogate: here MSE loss as proxy\n",
    "            out = self.model(x_batch)\n",
    "            loss = nn.MSELoss()(out, x_batch)\n",
    "            loss.backward()\n",
    "            for n, p in self.model.named_parameters():\n",
    "                if p.requires_grad and p.grad is not None:\n",
    "                    self.fisher[n] += p.grad.detach() ** 2\n",
    "        # normalize\n",
    "        for n in self.fisher:\n",
    "            self.fisher[n] = self.fisher[n] / len(dataloader)\n",
    "\n",
    "    def penalty(self, model):\n",
    "        loss = 0.0\n",
    "        for n, p in model.named_parameters():\n",
    "            if n in self.fisher:\n",
    "                _loss = (self.fisher[n] * (p - self.params[n])**2).sum()\n",
    "                loss += _loss\n",
    "        return loss\n",
    "\n",
    "# ----------------------------\n",
    "# Pipeline Manager\n",
    "# ----------------------------\n",
    "class IncrementalPipeline:\n",
    "    def __init__(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "        # models (to be trained offline)\n",
    "        self.bae = AE(input_dim).to(DEVICE)  # bottleneck AE\n",
    "        self.ae = AE(input_dim).to(DEVICE)   # auxiliary AE for recon-error detection\n",
    "        self.scaler = StandardScaler()\n",
    "        self.ocsvm = None\n",
    "        self.xgb = None\n",
    "\n",
    "        # Buffers\n",
    "        self.unknown_buffer = deque(maxlen=BUFFER_MAX)\n",
    "        self.replay_buffer = deque(maxlen=REPLAY_SIZE)  # for replay during fine-tune\n",
    "        # bookkeeping for EWC\n",
    "        self.ewc_bae = None\n",
    "        self.ewc_ae = None\n",
    "\n",
    "    # ----------------------------\n",
    "    # Offline training\n",
    "    # ----------------------------\n",
    "    def train_autoencoder(self, model, X, epochs=AE_EPOCHS, lr=AE_LR):\n",
    "        model = model.to(DEVICE)\n",
    "        ds = TensorDataset(torch.tensor(X, dtype=torch.float32))\n",
    "        dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        opt = optim.Adam(model.parameters(), lr=lr)\n",
    "        model.train()\n",
    "        for ep in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for (x_batch,) in dl:\n",
    "                x_batch = x_batch.to(DEVICE)\n",
    "                xrec = model(x_batch)\n",
    "                loss = nn.MSELoss()(xrec, x_batch)\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                epoch_loss += loss.item() * x_batch.size(0)\n",
    "            # simple print\n",
    "            if ep % 10 == 0 or ep == epochs - 1:\n",
    "                print(f\"[AE] epoch {ep}/{epochs} loss={epoch_loss/len(X):.6f}\")\n",
    "        return model\n",
    "\n",
    "    def train_offline(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"X: (n, d), y: (n,) where known attacks have labels; unknown/benign may be 0/1 etc\n",
    "           This function fits scaler, trains BAE, AE, OCSVM (on latent of benign?) and XGBoost.\n",
    "        \"\"\"\n",
    "        print(\"Scaling data...\")\n",
    "        Xs = self.scaler.fit_transform(X)\n",
    "        # split for XGBoost: use available supervised labels for known attacks\n",
    "        known_idx = ~np.isnan(y)\n",
    "        if known_idx.sum() == 0:\n",
    "            raise ValueError(\"No known labels provided for supervised training (XGBoost).\")\n",
    "        # Train bAE and AE on all data (or only benign) — here use whole Xs to learn general features\n",
    "        print(\"Training bottleneck AE (bAE)...\")\n",
    "        self.bae = self.train_autoencoder(self.bae, Xs)\n",
    "        print(\"Training auxiliary AE (AE)...\")\n",
    "        self.ae = self.train_autoencoder(self.ae, Xs)\n",
    "\n",
    "        # Build latent features (use bae.encode)\n",
    "        with torch.no_grad():\n",
    "            z = self.bae.encode(torch.tensor(Xs, dtype=torch.float32).to(DEVICE)).cpu().numpy()\n",
    "        # Train OCSVM on latent of *benign* samples if you have such labels; fallback: train on all\n",
    "        try:\n",
    "            benign_idx = (y == 0)  # convention: 0 benign, 1 attack (change to your labels)\n",
    "            if benign_idx.sum() < 10:\n",
    "                print(\"Not enough benign labeled examples for OCSVM; training on all latent features.\")\n",
    "                train_latent = z\n",
    "            else:\n",
    "                train_latent = z[benign_idx]\n",
    "        except Exception:\n",
    "            train_latent = z\n",
    "        print(\"Training OneClassSVM on latent features...\")\n",
    "        self.ocsvm = OneClassSVM(gamma='scale', nu=0.05).fit(train_latent)\n",
    "\n",
    "        # Train XGBoost classifier for known attacks\n",
    "        print(\"Training XGBoost on known labels...\")\n",
    "        xgb_train_X = Xs[known_idx]\n",
    "        xgb_train_y = y[known_idx].astype(int)\n",
    "        model = xgb.XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss')\n",
    "        model.fit(xgb_train_X, xgb_train_y)\n",
    "        self.xgb = model\n",
    "\n",
    "        # Initialize EWC objects (estimate Fisher)\n",
    "        print(\"Estimating Fisher for EWC (this may take time)...\")\n",
    "        ds_bae = DataLoader(TensorDataset(torch.tensor(Xs[:min(2000, len(Xs))], dtype=torch.float32)),\n",
    "                            batch_size=BATCH_SIZE, shuffle=True)\n",
    "        ds_ae = DataLoader(TensorDataset(torch.tensor(Xs[:min(2000, len(Xs))], dtype=torch.float32)),\n",
    "                           batch_size=BATCH_SIZE, shuffle=True)\n",
    "        self.ewc_bae = EWC(self.bae, ds_bae, device=DEVICE)\n",
    "        self.ewc_ae = EWC(self.ae, ds_ae, device=DEVICE)\n",
    "        print(\"Offline training complete.\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Online detection\n",
    "    # ----------------------------\n",
    "    def detect_single(self, x_raw: np.ndarray, unknown_thresholds=None):\n",
    "        \"\"\"\n",
    "        Process one sample (1D array) and return dict of detection outputs\n",
    "        unknown_thresholds: dict('recon': float, 'ocsvm': int (label), 'xgb_prob': float)\n",
    "        \"\"\"\n",
    "        if unknown_thresholds is None:\n",
    "            unknown_thresholds = {'recon': 0.01, 'xgb_prob': 0.5}\n",
    "        x_s = self.scaler.transform(x_raw.reshape(1, -1))\n",
    "        x_tensor = torch.tensor(x_s, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "        # latent / OCSVM\n",
    "        with torch.no_grad():\n",
    "            z = self.bae.encode(x_tensor).cpu().numpy()\n",
    "        ocsvm_label = self.ocsvm.predict(z)[0]  # 1 inlier, -1 outlier\n",
    "\n",
    "        # AE reconstruction error\n",
    "        with torch.no_grad():\n",
    "            xrec = self.ae(x_tensor)\n",
    "            recon_err = float(((xrec - x_tensor) ** 2).mean().cpu().numpy())\n",
    "\n",
    "        # XGBoost prediction (prob)\n",
    "        xgb_proba = None\n",
    "        xgb_pred = None\n",
    "        try:\n",
    "            proba = self.xgb.predict_proba(x_s)[0]\n",
    "            xgb_proba = float(np.max(proba))\n",
    "            xgb_pred = int(np.argmax(proba))\n",
    "        except Exception:\n",
    "            xgb_proba = None\n",
    "            xgb_pred = None\n",
    "\n",
    "        # Decide unknown: if XGBoost low confidence AND ocsvm says outlier OR recon error high\n",
    "        is_unknown = False\n",
    "        reasons = []\n",
    "        if xgb_proba is not None and xgb_proba > unknown_thresholds['xgb_prob']:\n",
    "            # known\n",
    "            reasons.append('xgb_confident')\n",
    "        else:\n",
    "            # not confident from xgb\n",
    "            if ocsvm_label == -1:\n",
    "                is_unknown = True\n",
    "                reasons.append('ocsvm_outlier')\n",
    "            if recon_err > unknown_thresholds['recon']:\n",
    "                is_unknown = True\n",
    "                reasons.append('high_recon')\n",
    "            if xgb_proba is not None and xgb_proba <= unknown_thresholds['xgb_prob']:\n",
    "                reasons.append('xgb_not_confident')\n",
    "\n",
    "        output = {\n",
    "            'x_raw': x_raw,\n",
    "            'x_scaled': x_s[0],\n",
    "            'latent_z': z[0],\n",
    "            'ocsvm_label': int(ocsvm_label),\n",
    "            'recon_err': recon_err,\n",
    "            'xgb_proba': xgb_proba,\n",
    "            'xgb_pred': xgb_pred,\n",
    "            'is_unknown': is_unknown,\n",
    "            'reasons': reasons\n",
    "        }\n",
    "\n",
    "        # buffer if unknown\n",
    "        if is_unknown:\n",
    "            self.buffer_unknown(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def buffer_unknown(self, detection_output):\n",
    "        # store scaled features and latent\n",
    "        rec = {\n",
    "            'ts': time.time(),\n",
    "            'x_scaled': detection_output['x_scaled'],\n",
    "            'latent': detection_output['latent_z'],\n",
    "            'meta': detection_output\n",
    "        }\n",
    "        self.unknown_buffer.append(rec)\n",
    "        # also add to replay buffer for later fine-tune (optionally)\n",
    "        self.replay_buffer.append(rec['x_scaled'])\n",
    "        # debug\n",
    "        if len(self.unknown_buffer) % 100 == 0:\n",
    "            print(f\"[Buffer] unknown buffer size = {len(self.unknown_buffer)}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Clustering + analyst labeling\n",
    "    # ----------------------------\n",
    "    def cluster_and_label(self, min_samples_for_cluster=CLUSTER_MIN_SAMPLES, use_hdbscan=True):\n",
    "        n_buf = len(self.unknown_buffer)\n",
    "        if n_buf < min_samples_for_cluster:\n",
    "            print(f\"[Cluster] not enough unknowns ({n_buf} < {min_samples_for_cluster})\")\n",
    "            return []\n",
    "\n",
    "        # Build matrix\n",
    "        X_latent = np.stack([r['latent'] for r in self.unknown_buffer])\n",
    "        # Use HDBSCAN if available and desired\n",
    "        if HAS_HDBSCAN and use_hdbscan:\n",
    "            clusterer = hdbscan.HDBSCAN(min_cluster_size=10)\n",
    "            labels = clusterer.fit_predict(X_latent)\n",
    "        else:\n",
    "            # fallback to MiniBatchKMeans\n",
    "            k = min(NUM_CLUSTERS, max(2, n_buf // 10))\n",
    "            clusterer = MiniBatchKMeans(n_clusters=k, batch_size=BATCH_KMEANS)\n",
    "            labels = clusterer.fit_predict(X_latent)\n",
    "\n",
    "        # group samples per cluster\n",
    "        clusters = defaultdict(list)\n",
    "        for idx, lab in enumerate(labels):\n",
    "            clusters[lab].append(idx)\n",
    "\n",
    "        # Simulated analyst labeling per cluster:\n",
    "        # - If majority of cluster is predicted by XGB or ocsvm as known, use that label\n",
    "        # - Otherwise, mark as new label (simulate analyst)\n",
    "        labeled_clusters = []\n",
    "        for lab, indices in clusters.items():\n",
    "            if lab == -1:\n",
    "                # HDBSCAN noise label\n",
    "                continue\n",
    "            # collect original metas\n",
    "            metas = [self.unknown_buffer[i]['meta'] for i in indices]\n",
    "            # try to infer a label from xgb_pred majority\n",
    "            preds = [m['xgb_pred'] for m in metas if m['xgb_pred'] is not None]\n",
    "            if len(preds) > 0:\n",
    "                # majority label\n",
    "                maj = max(set(preds), key=preds.count)\n",
    "                assigned_label = maj\n",
    "                analyst_note = 'majority_xgb'\n",
    "            else:\n",
    "                # fallback: analyst says \"new attack class\" -> label as next int\n",
    "                assigned_label = self._simulate_new_label()\n",
    "                analyst_note = 'simulated_new_label'\n",
    "            labeled_clusters.append({\n",
    "                'cluster_id': lab,\n",
    "                'indices': indices,\n",
    "                'assigned_label': assigned_label,\n",
    "                'analyst_note': analyst_note\n",
    "            })\n",
    "        # Remove labeled samples from unknown_buffer and return assignments\n",
    "        # We will extract their feature vectors to use for incremental update\n",
    "        labeled_samples = []\n",
    "        # Remove by reversing sorted indices to pop safely\n",
    "        removed_indices = []\n",
    "        for cluster in labeled_clusters:\n",
    "            for idx in sorted(cluster['indices'], reverse=True):\n",
    "                rec = self.unknown_buffer[idx]\n",
    "                labeled_samples.append({'x_scaled': rec['x_scaled'], 'label': cluster['assigned_label']})\n",
    "                removed_indices.append(idx)\n",
    "                # remove\n",
    "                del self.unknown_buffer[idx]\n",
    "        print(f\"[Cluster] produced {len(labeled_clusters)} clusters, labeled {len(labeled_samples)} samples.\")\n",
    "        return labeled_samples\n",
    "\n",
    "    def _simulate_new_label(self):\n",
    "        \"\"\"Simulate a new label integer (choose > existing classes).\"\"\"\n",
    "        # if xgb exists, see known classes\n",
    "        if self.xgb is None:\n",
    "            return 2  # arbitrary\n",
    "        known_classes = self.xgb.classes_.tolist()\n",
    "        new_label = max(known_classes) + 1\n",
    "        return new_label\n",
    "\n",
    "    # ----------------------------\n",
    "    # Incremental fine-tuning\n",
    "    # ----------------------------\n",
    "    def incremental_update(self, new_samples: T.List[dict]):\n",
    "        \"\"\"\n",
    "        new_samples: list of {'x_scaled': np.array, 'label': int}\n",
    "        \"\"\"\n",
    "        if len(new_samples) == 0:\n",
    "            print(\"[Incremental] no new samples.\")\n",
    "            return\n",
    "\n",
    "        X_new = np.stack([s['x_scaled'] for s in new_samples])\n",
    "        y_new = np.array([s['label'] for s in new_samples])\n",
    "\n",
    "        # 1) Update XGBoost incrementally: use xgb_model param\n",
    "        print(\"[Incremental] updating XGBoost...\")\n",
    "        try:\n",
    "            # prepare dataset; fit with xgb_model to continue\n",
    "            model = xgb.XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss')\n",
    "            # use previous model as warm start if exists\n",
    "            if self.xgb is not None:\n",
    "                model.fit(X_new, y_new, xgb_model=self.xgb.get_booster())\n",
    "            else:\n",
    "                model.fit(X_new, y_new)\n",
    "            self.xgb = model\n",
    "            print(\"[Incremental] XGBoost updated.\")\n",
    "        except Exception as e:\n",
    "            print(\"[Incremental] XGBoost incremental failed:\", e)\n",
    "            # fallback: retrain from scratch? keep old model\n",
    "\n",
    "        # 2) Retrain / fine-tune OCSVM on subset (combine some old latent + new)\n",
    "        print(\"[Incremental] retraining OCSVM subset...\")\n",
    "        try:\n",
    "            # Build latent for new data\n",
    "            with torch.no_grad():\n",
    "                z_new = self.bae.encode(torch.tensor(X_new, dtype=torch.float32).to(DEVICE)).cpu().numpy()\n",
    "            # sample some previous latent from replay buffer if available\n",
    "            sample_old = []\n",
    "            for _ in range(min(200, len(self.replay_buffer))):\n",
    "                sample_old.append(self.replay_buffer[random.randrange(len(self.replay_buffer))])\n",
    "            if len(sample_old) > 0:\n",
    "                sample_old = self.scaler.transform(np.stack(sample_old))\n",
    "                with torch.no_grad():\n",
    "                    z_old = self.bae.encode(torch.tensor(sample_old, dtype=torch.float32).to(DEVICE)).cpu().numpy()\n",
    "                z_comb = np.vstack([z_old, z_new])\n",
    "            else:\n",
    "                z_comb = z_new\n",
    "            # retrain OCSVM on z_comb (or only on samples labeled as benign if available)\n",
    "            self.ocsvm = OneClassSVM(gamma='scale', nu=0.05).fit(z_comb)\n",
    "            print(\"[Incremental] OCSVM retrained.\")\n",
    "        except Exception as e:\n",
    "            print(\"[Incremental] OCSVM retrain failed:\", e)\n",
    "\n",
    "        # 3) Fine-tune bAE and AE with replay and EWC\n",
    "        print(\"[Incremental] fine-tuning bAE and AE with EWC + replay...\")\n",
    "        # assemble replay set: include some previous replay + new samples\n",
    "        replay_items = list(self.replay_buffer)\n",
    "        if len(replay_items) > 0:\n",
    "            replay_items = np.stack(replay_items)\n",
    "            # combine with new\n",
    "            combined_X = np.vstack([replay_items, X_new])\n",
    "        else:\n",
    "            combined_X = X_new\n",
    "        # create dataloader\n",
    "        ds = DataLoader(TensorDataset(torch.tensor(combined_X, dtype=torch.float32)),\n",
    "                        batch_size=BATCH_SIZE, shuffle=True)\n",
    "        # fine-tune bae\n",
    "        opt = optim.Adam(self.bae.parameters(), lr=AE_LR/5)\n",
    "        self.bae.train()\n",
    "        for ep in range(5):  # a few epochs of incremental fine-tune\n",
    "            tot_loss = 0.0\n",
    "            for (x_batch,) in ds:\n",
    "                x_batch = x_batch.to(DEVICE)\n",
    "                xrec = self.bae(x_batch)\n",
    "                loss_rec = nn.MSELoss()(xrec, x_batch)\n",
    "                # EWC penalty\n",
    "                ewc_pen = self.ewc_bae.penalty(self.bae) if self.ewc_bae is not None else 0.0\n",
    "                loss = loss_rec + (EWC_LAMBDA * ewc_pen)\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                tot_loss += float(loss.detach().cpu().numpy()) * x_batch.size(0)\n",
    "            # print\n",
    "            print(f\"[bAE finetune] ep {ep} loss {tot_loss/len(combined_X):.6f}\")\n",
    "\n",
    "        # update EWC after fine-tune: re-estimate fisher on combined dataset (small sample)\n",
    "        ds_sample = DataLoader(TensorDataset(torch.tensor(combined_X[:min(2000, len(combined_X))], dtype=torch.float32)),\n",
    "                               batch_size=BATCH_SIZE, shuffle=True)\n",
    "        self.ewc_bae = EWC(self.bae, ds_sample, device=DEVICE)\n",
    "\n",
    "        # fine-tune AE similarly\n",
    "        opt = optim.Adam(self.ae.parameters(), lr=AE_LR/5)\n",
    "        self.ae.train()\n",
    "        for ep in range(5):\n",
    "            tot_loss = 0.0\n",
    "            for (x_batch,) in ds:\n",
    "                x_batch = x_batch.to(DEVICE)\n",
    "                xrec = self.ae(x_batch)\n",
    "                loss_rec = nn.MSELoss()(xrec, x_batch)\n",
    "                ewc_pen = self.ewc_ae.penalty(self.ae) if self.ewc_ae is not None else 0.0\n",
    "                loss = loss_rec + (EWC_LAMBDA * ewc_pen)\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                tot_loss += float(loss.detach().cpu().numpy()) * x_batch.size(0)\n",
    "            print(f\"[AE finetune] ep {ep} loss {tot_loss/len(combined_X):.6f}\")\n",
    "        ds_sample2 = DataLoader(TensorDataset(torch.tensor(combined_X[:min(2000, len(combined_X))], dtype=torch.float32)),\n",
    "                                batch_size=BATCH_SIZE, shuffle=True)\n",
    "        self.ewc_ae = EWC(self.ae, ds_sample2, device=DEVICE)\n",
    "\n",
    "        print(\"[Incremental] fine-tune complete.\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Convenience: run periodic clustering loop (callable from external sched)\n",
    "    # ----------------------------\n",
    "    def periodic_clustering_and_update(self):\n",
    "        labeled = self.cluster_and_label()\n",
    "        if len(labeled) > 0:\n",
    "            self.incremental_update(labeled)\n",
    "\n",
    "# ----------------------------\n",
    "# Demo / Example usage\n",
    "# ----------------------------\n",
    "def load_demo_data(n_samples=5000, n_features=30, attack_ratio=0.05):\n",
    "    \"\"\"Create synthetic data: Gaussian normal for benign, added anomalies for attacks.\"\"\"\n",
    "    X_benign = np.random.normal(0, 1, size=(int(n_samples*(1-attack_ratio)), n_features))\n",
    "    X_att = np.random.normal(3, 1.5, size=(int(n_samples*attack_ratio), n_features))\n",
    "    X = np.vstack([X_benign, X_att])\n",
    "    y = np.hstack([np.zeros(len(X_benign)), np.ones(len(X_att))])\n",
    "    # shuffle\n",
    "    perm = np.random.permutation(len(X))\n",
    "    return X[perm], y[perm]\n",
    "\n",
    "def main_demo():\n",
    "    X, y = load_demo_data()\n",
    "    # For offline, we only provide labels for a subset (simulate that only some attacks are labeled)\n",
    "    y_offline = np.copy(y)\n",
    "    # Let's make 50% of labels unknown -> NaN\n",
    "    mask = np.random.rand(len(y)) < 0.5\n",
    "    y_offline[mask] = np.nan\n",
    "\n",
    "    pipe = IncrementalPipeline(input_dim=X.shape[1])\n",
    "    pipe.train_offline(X, y_offline)\n",
    "\n",
    "    # simulate online stream\n",
    "    stream_X, stream_y = load_demo_data(n_samples=1000, n_features=X.shape[1], attack_ratio=0.08)\n",
    "    for i, x in enumerate(stream_X):\n",
    "        out = pipe.detect_single(x, unknown_thresholds={'recon': 0.5, 'xgb_prob': 0.7})\n",
    "        if i % 200 == 0:\n",
    "            print(f\"[Stream] processed {i} samples, buffer size {len(pipe.unknown_buffer)}\")\n",
    "        # periodically run clustering every 300 samples\n",
    "        if i % 300 == 299:\n",
    "            labeled = pipe.cluster_and_label()\n",
    "            pipe.incremental_update(labeled)\n",
    "\n",
    "    # final forced clustering\n",
    "    labeled = pipe.cluster_and_label(min_samples_for_cluster=10)\n",
    "    pipe.incremental_update(labeled)\n",
    "    print(\"Demo complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_demo()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kltn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
