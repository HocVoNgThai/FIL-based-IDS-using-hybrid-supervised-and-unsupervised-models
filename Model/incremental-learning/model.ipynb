{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4ec61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ª≠ d·ª•ng thi·∫øt b·ªã: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler # <<< Quay l·∫°i StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_fscore_support, roc_curve, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score\n",
    "import joblib\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Thi·∫øt l·∫≠p thi·∫øt b·ªã t√≠nh to√°n (GPU n·∫øu c√≥)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {DEVICE}\\n\")\n",
    "\n",
    "# ===================================================================\n",
    "# C√ÅC L·ªöP MODEL (Deep AE Enhanced Capacity)\n",
    "# ===================================================================\n",
    "\n",
    "class AdvancedDimReducerAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Model 1: Gi·∫£m chi·ªÅu d·ªØ li·ªáu (46D -> 32D). Input -> 128 -> 64 -> 32.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, latent_dim=32):\n",
    "        super(AdvancedDimReducerAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.Linear(64, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "        # self.encoder = nn.Sequential(\n",
    "        #     nn.Linear(input_dim, 128), nn.LayerNorm(128), nn.ReLU(),\n",
    "        #     nn.Linear(128, 64), nn.ReLU(),\n",
    "        #     nn.Linear(64, latent_dim)\n",
    "        # )\n",
    "        # self.decoder = nn.Sequential(\n",
    "        #     nn.Linear(latent_dim, 64), nn.LayerNorm(64), nn.ReLU(),\n",
    "        #     nn.Linear(64, 128), nn.ReLU(),\n",
    "        #     nn.Linear(128, input_dim)\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4292b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAnomalyAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Model 2: Ph√°t hi·ªán b·∫•t th∆∞·ªùng (Ki·∫øn tr√∫c robust 32D -> 8D).\n",
    "    Increased layer width (capacity), tight bottleneck, increased dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=32, latent_dim=8): # <<< Bottleneck 8D\n",
    "        super(DeepAnomalyAE, self).__init__()\n",
    "        # Input (32) -> 256 -> 128 -> 8 (Latent)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),      # <<< Increased width\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3),                # <<< Increased dropout\n",
    "            nn.Linear(256, 128),      # <<< Increased width\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3),                # <<< Increased dropout\n",
    "            nn.Linear(128, latent_dim) # To 8\n",
    "        )\n",
    "        # 8 (Latent) -> 128 -> 256 -> 32 (Output)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),      # <<< Increased width\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3),                # <<< Increased dropout\n",
    "            nn.Linear(128, 256),      # <<< Increased width\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3),                # <<< Increased dropout\n",
    "            nn.Linear(256, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55e78e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# H√ÄM HU·∫§N LUY·ªÜN (Optimized)\n",
    "# ===================================================================\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, patience, learning_rate, weight_decay, model_save_path, loss_fn):\n",
    "    model.to(DEVICE)\n",
    "    criterion = loss_fn\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay) # Adam optimizer\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=8) # Increased patience\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    print(f\"B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán model '{model.__class__.__name__}' v·ªõi loss '{criterion.__class__.__name__}'...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        for (data,) in train_loader:\n",
    "            data = data.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            reconstructed = output[0] if isinstance(output, tuple) else output\n",
    "            loss = criterion(reconstructed, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        avg_epoch_train_loss = epoch_train_loss / len(train_loader.dataset) if len(train_loader.dataset) > 0 else 0\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for (data,) in val_loader:\n",
    "                data = data.to(DEVICE)\n",
    "                output = model(data)\n",
    "                reconstructed = output[0] if isinstance(output, tuple) else output\n",
    "                loss = criterion(reconstructed, data)\n",
    "                val_loss += loss.item() * data.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset) if len(val_loader.dataset) > 0 else 0\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_epoch_train_loss:.6f} | Val Loss: {avg_val_loss:.6f} | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss - 1e-6:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping ƒë∆∞·ª£c k√≠ch ho·∫°t t·∫°i epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    best_loss_str = f\"{best_val_loss:.6f}\" if best_val_loss != float('inf') else \"N/A\"\n",
    "    print(f\"Hu·∫•n luy·ªán ho√†n t·∫•t trong {end_time - start_time:.2f} gi√¢y. Best Val Loss: {best_loss_str}. Model t·ªët nh·∫•t ƒë√£ l∆∞u v√†o '{model_save_path}'.\\n\")\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_save_path))\n",
    "    except FileNotFoundError: print(f\"Warning: Could not load {model_save_path}.\")\n",
    "    except Exception as e: print(f\"Warning: Error loading {model_save_path}: {e}.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687a1f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# H√ÄM H·ªñ TR·ª¢ ƒê√ÅNH GI√Å & TR·ª∞C QUAN H√ìA (More Robust)\n",
    "# ===================================================================\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    except ValueError:\n",
    "        unique_preds=np.unique(y_pred); unique_true=np.unique(y_true); tn,fp,fn,tp=0,0,0,0\n",
    "        if len(unique_true)==1:\n",
    "            if unique_true[0]==0: tn=len(y_true);\n",
    "            else: tp=len(y_true);\n",
    "        elif len(unique_preds)==1:\n",
    "             if unique_preds[0]==0: tn=np.sum(y_true==0); fn=np.sum(y_true==1);\n",
    "             else: fp=np.sum(y_true==0); tp=np.sum(y_true==1);\n",
    "        print(\"Warning: CM calculation issue.\")\n",
    "    fpr = fp/(fp+tn) if (fp+tn)>0 else 0.0\n",
    "    fnr = fn/(fn+tp) if (fn+tp)>0 else 0.0\n",
    "    return accuracy, precision, recall, f1, fpr, fnr, (tn, fp, fn, tp)\n",
    "\n",
    "def plot_evaluation(y_true, scores, y_pred, threshold, title):\n",
    "    print(f\"\\n--- ƒêang v·∫Ω bi·ªÉu ƒë·ªì cho: {title} ---\")\n",
    "    auc = 0.5\n",
    "    try:\n",
    "        y_true=np.asarray(y_true).astype(int); scores=np.asarray(scores); y_pred=np.asarray(y_pred).astype(int);\n",
    "        if not np.issubdtype(scores.dtype, np.number): raise TypeError(\"scores non-numeric\")\n",
    "        scores=np.nan_to_num(scores, nan=np.nanmedian(scores), posinf=np.nanmax(scores[np.isfinite(scores)]), neginf=np.nanmin(scores[np.isfinite(scores)]))\n",
    "        if not np.all(np.isfinite(scores)): raise ValueError(\"Non-finite scores remain\")\n",
    "        unique_labels_true = np.unique(y_true)\n",
    "        if len(unique_labels_true)<2: print(f\"Warning: Only one class ({unique_labels_true}). Cannot calc AUC.\")\n",
    "        else: auc=roc_auc_score(y_true, scores)\n",
    "    except ValueError as e: print(f\"Warning: Cannot calc AUC. Error: {e}\")\n",
    "    except TypeError as e: print(f\"Error calc AUC: {e}\")\n",
    "\n",
    "    accuracy, precision, recall, f1, fpr, fnr, (tn, fp, fn, tp) = calculate_metrics(y_true, y_pred)\n",
    "    print(f\"AUC: {auc:.4f} | F1-Score: {f1:.4f}\"); print(f\"Precision: {precision:.4f} | Recall (TPR): {recall:.4f}\"); print(f\"FPR: {fpr:.4f} | FNR: {fnr:.4f}\"); print(f\"TP: {tp} | FP: {fp} | TN: {tn} | FN: {fn}\")\n",
    "    plt.figure(figsize=(18, 6)); plt.suptitle(title, fontsize=16); plt.subplot(1, 3, 1)\n",
    "    unique_labels_plot = np.unique(y_true)\n",
    "    if len(unique_labels_plot) > 1:\n",
    "        if 0 in unique_labels_plot: sns.histplot(scores[y_true==0], color='blue', label='Normal Scores', kde=True, bins=50, stat=\"density\")\n",
    "        if 1 in unique_labels_plot: sns.histplot(scores[y_true==1], color='red', label='Abnormal Scores', kde=True, bins=50, stat=\"density\")\n",
    "    elif len(unique_labels_plot) == 1: label_text = 'Normal' if unique_labels_plot[0] == 0 else 'Abnormal'; sns.histplot(scores, color='purple', label=f'{label_text} Scores Only', kde=True, bins=50, stat=\"density\")\n",
    "    else: plt.text(0.5, 0.5, 'No data', ha='center')\n",
    "    plt.axvline(threshold, color='green', linestyle='--', label=f'Threshold ({threshold:.4f})'); plt.title('Ph√¢n ph·ªëi ƒêi·ªÉm/L·ªói B·∫•t th∆∞·ªùng'); plt.legend(); plt.subplot(1, 3, 2)\n",
    "    labels_cm = sorted(np.unique(y_true))\n",
    "    if not np.all(np.isin(np.unique(y_pred), labels_cm)): print(f\"Warning: Predictions contain labels not in y_true.\")\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels_cm); sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[f'{l}' for l in labels_cm], yticklabels=[f'{l}' for l in labels_cm]); plt.title('Confusion Matrix'); plt.xlabel('Predicted'); plt.ylabel('Actual'); plt.subplot(1, 3, 3)\n",
    "    if auc > 0.5 and len(unique_labels_plot) > 1 :\n",
    "        fpr_roc, tpr_roc, _ = roc_curve(y_true, scores); plt.plot(fpr_roc, tpr_roc, label=f'AUC = {auc:.4f}')\n",
    "        if (tn + fp) > 0 and (tp + fn) > 0: plt.scatter(fpr, recall, marker='o', color='red', zorder=5, s=100, label=f'Operating Point\\n(FPR={fpr:.2f}, TPR={recall:.2f})')\n",
    "    else: plt.text(0.5, 0.5, 'Cannot draw ROC Curve', ha='center')\n",
    "    plt.title('ROC Curve'); plt.xlabel('FPR'); plt.ylabel('TPR'); plt.legend(); plt.grid(True); plt.tight_layout(rect=[0, 0.03, 1, 0.93]); plt.savefig(f\"evaluation_{title.replace(' ', '_')}.png\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6e26f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before drop cols: \n",
      " Flow ID              float64\n",
      "Src IP               float64\n",
      "Src Port             float64\n",
      "Dst IP               float64\n",
      "Dst Port             float64\n",
      "Protocol             float64\n",
      "Timestamp             object\n",
      "Flow Duration        float64\n",
      "Tot Fwd Pkts         float64\n",
      "Tot Bwd Pkts         float64\n",
      "TotLen Fwd Pkts      float64\n",
      "TotLen Bwd Pkts      float64\n",
      "Fwd Pkt Len Max      float64\n",
      "Fwd Pkt Len Min      float64\n",
      "Fwd Pkt Len Mean     float64\n",
      "Fwd Pkt Len Std      float64\n",
      "Bwd Pkt Len Max      float64\n",
      "Bwd Pkt Len Min      float64\n",
      "Bwd Pkt Len Mean     float64\n",
      "Bwd Pkt Len Std      float64\n",
      "Flow Byts/s          float64\n",
      "Flow Pkts/s          float64\n",
      "Flow IAT Mean        float64\n",
      "Flow IAT Std         float64\n",
      "Flow IAT Max         float64\n",
      "Flow IAT Min         float64\n",
      "Fwd IAT Tot          float64\n",
      "Fwd IAT Mean         float64\n",
      "Fwd IAT Std          float64\n",
      "Fwd IAT Max          float64\n",
      "Fwd IAT Min          float64\n",
      "Bwd IAT Tot          float64\n",
      "Bwd IAT Mean         float64\n",
      "Bwd IAT Std          float64\n",
      "Bwd IAT Max          float64\n",
      "Bwd IAT Min          float64\n",
      "Bwd PSH Flags        float64\n",
      "Fwd Header Len       float64\n",
      "Bwd Header Len       float64\n",
      "Fwd Pkts/s           float64\n",
      "Bwd Pkts/s           float64\n",
      "Pkt Len Min          float64\n",
      "Pkt Len Max          float64\n",
      "Pkt Len Mean         float64\n",
      "Pkt Len Std          float64\n",
      "Pkt Len Var          float64\n",
      "FIN Flag Cnt         float64\n",
      "SYN Flag Cnt         float64\n",
      "RST Flag Cnt         float64\n",
      "PSH Flag Cnt         float64\n",
      "ACK Flag Cnt         float64\n",
      "Down/Up Ratio        float64\n",
      "Pkt Size Avg         float64\n",
      "Fwd Seg Size Avg     float64\n",
      "Bwd Seg Size Avg     float64\n",
      "Fwd Byts/b Avg       float64\n",
      "Fwd Pkts/b Avg       float64\n",
      "Fwd Blk Rate Avg     float64\n",
      "Bwd Byts/b Avg       float64\n",
      "Bwd Pkts/b Avg       float64\n",
      "Bwd Blk Rate Avg     float64\n",
      "Subflow Fwd Pkts     float64\n",
      "Subflow Fwd Byts     float64\n",
      "Subflow Bwd Pkts     float64\n",
      "Subflow Bwd Byts     float64\n",
      "Init Fwd Win Byts    float64\n",
      "Init Bwd Win Byts    float64\n",
      "Fwd Act Data Pkts    float64\n",
      "Fwd Seg Size Min     float64\n",
      "Active Mean          float64\n",
      "Active Std           float64\n",
      "Active Max           float64\n",
      "Active Min           float64\n",
      "Idle Mean            float64\n",
      "Idle Std             float64\n",
      "Idle Max             float64\n",
      "Idle Min             float64\n",
      "Label                  int64\n",
      "dtype: object\n",
      "Flow ID              float64\n",
      "Src IP               float64\n",
      "Src Port             float64\n",
      "Dst IP               float64\n",
      "Dst Port             float64\n",
      "Protocol             float64\n",
      "Flow Duration        float64\n",
      "Tot Fwd Pkts         float64\n",
      "Tot Bwd Pkts         float64\n",
      "TotLen Fwd Pkts      float64\n",
      "TotLen Bwd Pkts      float64\n",
      "Fwd Pkt Len Max      float64\n",
      "Fwd Pkt Len Min      float64\n",
      "Fwd Pkt Len Mean     float64\n",
      "Fwd Pkt Len Std      float64\n",
      "Bwd Pkt Len Max      float64\n",
      "Bwd Pkt Len Min      float64\n",
      "Bwd Pkt Len Mean     float64\n",
      "Bwd Pkt Len Std      float64\n",
      "Flow Byts/s          float64\n",
      "Flow Pkts/s          float64\n",
      "Flow IAT Mean        float64\n",
      "Flow IAT Std         float64\n",
      "Flow IAT Max         float64\n",
      "Fwd IAT Tot          float64\n",
      "Fwd IAT Mean         float64\n",
      "Fwd IAT Std          float64\n",
      "Fwd IAT Max          float64\n",
      "Bwd IAT Tot          float64\n",
      "Bwd IAT Mean         float64\n",
      "Bwd IAT Std          float64\n",
      "Bwd IAT Max          float64\n",
      "Bwd IAT Min          float64\n",
      "Bwd PSH Flags        float64\n",
      "Fwd Header Len       float64\n",
      "Bwd Header Len       float64\n",
      "Fwd Pkts/s           float64\n",
      "Bwd Pkts/s           float64\n",
      "Pkt Len Min          float64\n",
      "Pkt Len Max          float64\n",
      "Pkt Len Mean         float64\n",
      "Pkt Len Std          float64\n",
      "Pkt Len Var          float64\n",
      "FIN Flag Cnt         float64\n",
      "SYN Flag Cnt         float64\n",
      "RST Flag Cnt         float64\n",
      "PSH Flag Cnt         float64\n",
      "ACK Flag Cnt         float64\n",
      "Down/Up Ratio        float64\n",
      "Pkt Size Avg         float64\n",
      "Fwd Seg Size Avg     float64\n",
      "Bwd Seg Size Avg     float64\n",
      "Fwd Byts/b Avg       float64\n",
      "Fwd Pkts/b Avg       float64\n",
      "Fwd Blk Rate Avg     float64\n",
      "Bwd Byts/b Avg       float64\n",
      "Bwd Pkts/b Avg       float64\n",
      "Bwd Blk Rate Avg     float64\n",
      "Subflow Fwd Pkts     float64\n",
      "Subflow Fwd Byts     float64\n",
      "Subflow Bwd Pkts     float64\n",
      "Subflow Bwd Byts     float64\n",
      "Init Fwd Win Byts    float64\n",
      "Init Bwd Win Byts    float64\n",
      "Fwd Act Data Pkts    float64\n",
      "Fwd Seg Size Min     float64\n",
      "Active Mean          float64\n",
      "Active Std           float64\n",
      "Active Max           float64\n",
      "Active Min           float64\n",
      "Idle Mean            float64\n",
      "Idle Std             float64\n",
      "Idle Max             float64\n",
      "Idle Min             float64\n",
      "Label                  int64\n",
      "dtype: object\n",
      "üß© Ki·ªÉm tra gi√° tr·ªã b·∫•t th∆∞·ªùng trong d·ªØ li·ªáu...\n",
      "NaN: Flow ID              0\n",
      "Src IP               0\n",
      "Src Port             0\n",
      "Dst IP               0\n",
      "Dst Port             0\n",
      "Protocol             0\n",
      "Flow Duration        0\n",
      "Tot Fwd Pkts         0\n",
      "Tot Bwd Pkts         0\n",
      "TotLen Fwd Pkts      0\n",
      "TotLen Bwd Pkts      0\n",
      "Fwd Pkt Len Max      0\n",
      "Fwd Pkt Len Min      0\n",
      "Fwd Pkt Len Mean     0\n",
      "Fwd Pkt Len Std      0\n",
      "Bwd Pkt Len Max      0\n",
      "Bwd Pkt Len Min      0\n",
      "Bwd Pkt Len Mean     0\n",
      "Bwd Pkt Len Std      0\n",
      "Flow Byts/s          0\n",
      "Flow Pkts/s          0\n",
      "Flow IAT Mean        0\n",
      "Flow IAT Std         0\n",
      "Flow IAT Max         0\n",
      "Fwd IAT Tot          0\n",
      "Fwd IAT Mean         0\n",
      "Fwd IAT Std          0\n",
      "Fwd IAT Max          0\n",
      "Bwd IAT Tot          0\n",
      "Bwd IAT Mean         0\n",
      "Bwd IAT Std          0\n",
      "Bwd IAT Max          0\n",
      "Bwd IAT Min          0\n",
      "Bwd PSH Flags        0\n",
      "Fwd Header Len       0\n",
      "Bwd Header Len       0\n",
      "Fwd Pkts/s           0\n",
      "Bwd Pkts/s           0\n",
      "Pkt Len Min          0\n",
      "Pkt Len Max          0\n",
      "Pkt Len Mean         0\n",
      "Pkt Len Std          0\n",
      "Pkt Len Var          0\n",
      "FIN Flag Cnt         0\n",
      "SYN Flag Cnt         0\n",
      "RST Flag Cnt         0\n",
      "PSH Flag Cnt         0\n",
      "ACK Flag Cnt         0\n",
      "Down/Up Ratio        0\n",
      "Pkt Size Avg         0\n",
      "Fwd Seg Size Avg     0\n",
      "Bwd Seg Size Avg     0\n",
      "Fwd Byts/b Avg       0\n",
      "Fwd Pkts/b Avg       0\n",
      "Fwd Blk Rate Avg     0\n",
      "Bwd Byts/b Avg       0\n",
      "Bwd Pkts/b Avg       0\n",
      "Bwd Blk Rate Avg     0\n",
      "Subflow Fwd Pkts     0\n",
      "Subflow Fwd Byts     0\n",
      "Subflow Bwd Pkts     0\n",
      "Subflow Bwd Byts     0\n",
      "Init Fwd Win Byts    0\n",
      "Init Bwd Win Byts    0\n",
      "Fwd Act Data Pkts    0\n",
      "Fwd Seg Size Min     0\n",
      "Active Mean          0\n",
      "Active Std           0\n",
      "Active Max           0\n",
      "Active Min           0\n",
      "Idle Mean            0\n",
      "Idle Std             0\n",
      "Idle Max             0\n",
      "Idle Min             0\n",
      "Label                0\n",
      "dtype: int64\n",
      "Inf: Flow ID              0\n",
      "Src IP               0\n",
      "Src Port             0\n",
      "Dst IP               0\n",
      "Dst Port             0\n",
      "Protocol             0\n",
      "Flow Duration        0\n",
      "Tot Fwd Pkts         0\n",
      "Tot Bwd Pkts         0\n",
      "TotLen Fwd Pkts      0\n",
      "TotLen Bwd Pkts      0\n",
      "Fwd Pkt Len Max      0\n",
      "Fwd Pkt Len Min      0\n",
      "Fwd Pkt Len Mean     0\n",
      "Fwd Pkt Len Std      0\n",
      "Bwd Pkt Len Max      0\n",
      "Bwd Pkt Len Min      0\n",
      "Bwd Pkt Len Mean     0\n",
      "Bwd Pkt Len Std      0\n",
      "Flow Byts/s          0\n",
      "Flow Pkts/s          0\n",
      "Flow IAT Mean        0\n",
      "Flow IAT Std         0\n",
      "Flow IAT Max         0\n",
      "Fwd IAT Tot          0\n",
      "Fwd IAT Mean         0\n",
      "Fwd IAT Std          0\n",
      "Fwd IAT Max          0\n",
      "Bwd IAT Tot          0\n",
      "Bwd IAT Mean         0\n",
      "Bwd IAT Std          0\n",
      "Bwd IAT Max          0\n",
      "Bwd IAT Min          0\n",
      "Bwd PSH Flags        0\n",
      "Fwd Header Len       0\n",
      "Bwd Header Len       0\n",
      "Fwd Pkts/s           0\n",
      "Bwd Pkts/s           0\n",
      "Pkt Len Min          0\n",
      "Pkt Len Max          0\n",
      "Pkt Len Mean         0\n",
      "Pkt Len Std          0\n",
      "Pkt Len Var          0\n",
      "FIN Flag Cnt         0\n",
      "SYN Flag Cnt         0\n",
      "RST Flag Cnt         0\n",
      "PSH Flag Cnt         0\n",
      "ACK Flag Cnt         0\n",
      "Down/Up Ratio        0\n",
      "Pkt Size Avg         0\n",
      "Fwd Seg Size Avg     0\n",
      "Bwd Seg Size Avg     0\n",
      "Fwd Byts/b Avg       0\n",
      "Fwd Pkts/b Avg       0\n",
      "Fwd Blk Rate Avg     0\n",
      "Bwd Byts/b Avg       0\n",
      "Bwd Pkts/b Avg       0\n",
      "Bwd Blk Rate Avg     0\n",
      "Subflow Fwd Pkts     0\n",
      "Subflow Fwd Byts     0\n",
      "Subflow Bwd Pkts     0\n",
      "Subflow Bwd Byts     0\n",
      "Init Fwd Win Byts    0\n",
      "Init Bwd Win Byts    0\n",
      "Fwd Act Data Pkts    0\n",
      "Fwd Seg Size Min     0\n",
      "Active Mean          0\n",
      "Active Std           0\n",
      "Active Max           0\n",
      "Active Min           0\n",
      "Idle Mean            0\n",
      "Idle Std             0\n",
      "Idle Max             0\n",
      "Idle Min             0\n",
      "Label                0\n",
      "dtype: int64\n",
      "Max: 83767163.70985197\n",
      "Min: -4.956795857726011\n",
      "0     1\n",
      "1     1\n",
      "2     0\n",
      "3     0\n",
      "4     1\n",
      "5     1\n",
      "6     1\n",
      "7     1\n",
      "8     0\n",
      "9     1\n",
      "10    1\n",
      "11    0\n",
      "12    1\n",
      "13    1\n",
      "14    1\n",
      "15    1\n",
      "16    1\n",
      "17    1\n",
      "18    1\n",
      "19    1\n",
      "20    1\n",
      "21    0\n",
      "22    1\n",
      "23    1\n",
      "24    1\n",
      "25    1\n",
      "26    1\n",
      "27    1\n",
      "28    1\n",
      "29    1\n",
      "30    0\n",
      "31    1\n",
      "32    1\n",
      "33    1\n",
      "34    1\n",
      "35    1\n",
      "36    1\n",
      "37    0\n",
      "38    1\n",
      "39    1\n",
      "40    1\n",
      "41    1\n",
      "42    1\n",
      "43    1\n",
      "44    1\n",
      "45    1\n",
      "46    1\n",
      "47    1\n",
      "48    1\n",
      "49    0\n",
      "Name: Label, dtype: int32\n",
      "Loaded X shape: (551125, 74), y distribution: [ 70188 480937]\n",
      "Train: (418855, 74) \n",
      "Val:  (22045, 74) \n",
      "Test: (110225, 74)\n",
      "Input dimension: 74\n",
      "Training bottleneck AutoEncoder (bAE)...\n",
      "B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán model 'AdvancedDimReducerAE' v·ªõi loss 'MSELoss'...\n",
      "Epoch 10/100 | Train Loss: 750547037071.978638 | Val Loss: 763931005190.166260 | LR: 1.00e-04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 247\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline finished. Models and evaluation saved in:\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_dir)\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbAE\u001b[39m\u001b[38;5;124m\"\u001b[39m: bAE,\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepAE\u001b[39m\u001b[38;5;124m\"\u001b[39m: deepAE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m: best_thr\n\u001b[0;32m    245\u001b[0m     }\n\u001b[1;32m--> 247\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_full_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/hoang/Documents/Dataset_KLTN/gotham2025_extracted/gotham2025_proced_merged.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 126\u001b[0m, in \u001b[0;36mrun_full_pipeline\u001b[1;34m(csv_path, label_col, latent_dim, bAE_epochs, bAE_lr, deepAE_epochs, deepAE_lr, ocsvm_nu, batch_size, test_size, val_size, model_dir)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining bottleneck AutoEncoder (bAE)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m bAE_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbAE_latent\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatent_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 126\u001b[0m bAE \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbAE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbAE_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbAE_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbAE_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMSELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# 4) Encode latent for train/val/test\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoding latent representations...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 26\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, epochs, patience, learning_rate, weight_decay, model_save_path, loss_fn)\u001b[0m\n\u001b[0;32m     24\u001b[0m reconstructed \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m output\n\u001b[0;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(reconstructed, data)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     28\u001b[0m epoch_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hoang\\.conda\\envs\\kltn\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hoang\\.conda\\envs\\kltn\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hoang\\.conda\\envs\\kltn\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def make_dataloader_from_array(X, batch_size=128, shuffle=True):\n",
    "    tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    ds = TensorDataset(tensor)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n",
    "\n",
    "def compute_reconstruction_errors(model, X, batch_size=1024):\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    dloader = make_dataloader_from_array(X, batch_size=batch_size, shuffle=False)\n",
    "    errors = []\n",
    "    with torch.no_grad():\n",
    "        for (batch,) in dloader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            output = model(batch)\n",
    "            recon = output[0] if isinstance(output, tuple) else output\n",
    "            # MSE per sample\n",
    "            per_sample = torch.mean((recon - batch)**2, dim=1).cpu().numpy()\n",
    "            errors.append(per_sample)\n",
    "    return np.concatenate(errors, axis=0)\n",
    "\n",
    "def get_latent(encoder_model, X, batch_size=1024):\n",
    "    encoder_model.to(DEVICE)\n",
    "    encoder_model.eval()\n",
    "    dloader = make_dataloader_from_array(X, batch_size=batch_size, shuffle=False)\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for (batch,) in dloader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            out = encoder_model(batch)\n",
    "            # encoder may be a Sequential returning z directly or model.forward returns tuple\n",
    "            if isinstance(out, tuple):\n",
    "                z = out[1]  # in case forward returns (reconstructed, z)\n",
    "            else:\n",
    "                # If whole AE provided, call encoder separately\n",
    "                try:\n",
    "                    z = encoder_model.encoder(batch)\n",
    "                except Exception:\n",
    "                    z = out\n",
    "            latents.append(z.cpu().numpy())\n",
    "    return np.concatenate(latents, axis=0)\n",
    "\n",
    "def choose_threshold_by_val(scores_val, y_val, maximize=\"f1\"):\n",
    "    \"\"\"Search threshold that maximizes F1 on validation set.\n",
    "       scores: higher -> more anomalous\n",
    "       returns best_threshold, best_metric\n",
    "    \"\"\"\n",
    "    best_thr = None\n",
    "    best_metric = -1\n",
    "    # grid = unique values quantiles\n",
    "    grid = np.unique(np.percentile(scores_val, np.linspace(0, 100, 200)))\n",
    "    for thr in grid:\n",
    "        pred = (scores_val >= thr).astype(int)  # 1 = anomaly\n",
    "        if len(np.unique(y_val)) < 2:\n",
    "            metric = 0\n",
    "        else:\n",
    "            metric = f1_score(y_val, pred, zero_division=0)\n",
    "        if metric > best_metric:\n",
    "            best_metric = metric\n",
    "            best_thr = thr\n",
    "    return best_thr, best_metric\n",
    "\n",
    "def run_full_pipeline(csv_path,\n",
    "                      label_col=None,\n",
    "                      latent_dim=64,\n",
    "                      bAE_epochs=100, bAE_lr=1e-4,\n",
    "                      deepAE_epochs=80, deepAE_lr=1e-4,\n",
    "                      ocsvm_nu=0.01,\n",
    "                      batch_size=128,\n",
    "                      test_size=0.2, val_size=0.2,\n",
    "                      model_dir=\"models\"):\n",
    "\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # 1) Load CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"Before drop cols: \\n\", df.dtypes)\n",
    "    \n",
    "    df = df.drop(['Timestamp','Flow IAT Min','Fwd IAT Min'], axis=1)\n",
    "    # - CH·ªàNH LABEL ƒê·ªÇ D·ªÑ TRAIN TEST --\n",
    "    \n",
    "    print(df.dtypes)\n",
    "    \n",
    "    df['Label'] = (df['Label'] > 0).astype(int)\n",
    "    \n",
    "    print(\"üß© Ki·ªÉm tra gi√° tr·ªã b·∫•t th∆∞·ªùng trong d·ªØ li·ªáu...\")\n",
    "    print(\"NaN:\", np.isnan(df).sum())\n",
    "    print(\"Inf:\", np.isinf(df).sum())\n",
    "    print(\"Max:\", np.nanmax(df))\n",
    "    print(\"Min:\", np.nanmin(df))\n",
    "    \n",
    "    print(df['Label'].head(50))\n",
    "    X_train = df.drop(columns=['Label']).values.astype(np.float32)\n",
    "    y_train = df['Label'].astype(int).values\n",
    "    print(f\"Loaded X shape: {X_train.shape}, y distribution: {np.bincount(y_train)}\")\n",
    "\n",
    "    # Chia d·ªØ li·ªáu train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.05, stratify=y_train, random_state=42)\n",
    "    \n",
    "    print(\"Train:\", X_train.shape, \"\\nVal: \", X_val.shape, \"\\nTest:\", X_test.shape)\n",
    "\n",
    "    # Loader cho bAE\n",
    "    train_raw_loader = DataLoader(TensorDataset(torch.tensor(X_train)), batch_size=256, shuffle=True)\n",
    "    val_raw_loader   = DataLoader(TensorDataset(torch.tensor(X_test)), batch_size=256, shuffle=False)\n",
    "    \n",
    "    #####\n",
    "    # X = df.drop(columns=[label_col]).values.astype(np.float32)\n",
    "    # y = df[label_col].astype(int).values\n",
    "    # print(f\"Loaded X shape: {X.shape}, y distribution: {np.bincount(y)}\")\n",
    "\n",
    "    # 3) Train bAE (unsupervised) on ALL train samples (both normal + abnormal is OK for dim-reducer)\n",
    "    input_dim = X_train.shape[1]\n",
    "    print(f\"Input dimension: {input_dim}\")\n",
    "    \n",
    "    bAE = AdvancedDimReducerAE(input_dim=input_dim, latent_dim=latent_dim)\n",
    "    train_loader = make_dataloader_from_array(X_train, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = make_dataloader_from_array(X_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(\"Training bottleneck AutoEncoder (bAE)...\")\n",
    "    bAE_path = os.path.join(model_dir, f\"bAE_latent{latent_dim}.pth\")\n",
    "    bAE = train_model(bAE, train_loader, val_loader,\n",
    "                      epochs=bAE_epochs, patience=15,\n",
    "                      learning_rate=bAE_lr, weight_decay=1e-5,\n",
    "                      model_save_path=bAE_path, loss_fn=nn.MSELoss())\n",
    "\n",
    "    # 4) Encode latent for train/val/test\n",
    "    print(\"Encoding latent representations...\")\n",
    "    # We want encoder only ‚Äî if model returns (reconstructed, z) in forward, we can call forward and extract z\n",
    "    \n",
    "    def encode_with_bAE(model, X_arr):\n",
    "        model.to(DEVICE); model.eval()\n",
    "        loader = make_dataloader_from_array(X_arr, batch_size=1024, shuffle=False)\n",
    "        zs = []\n",
    "        with torch.no_grad():\n",
    "            for (batch,) in loader:\n",
    "                batch = batch.to(DEVICE)\n",
    "                out = model(batch)\n",
    "                # out might be (recons, z) per your class\n",
    "                if isinstance(out, tuple) and len(out) == 2:\n",
    "                    z = out[1]\n",
    "                else:\n",
    "                    # try to access encoder\n",
    "                    try:\n",
    "                        z = model.encoder(batch)\n",
    "                    except Exception:\n",
    "                        z = out\n",
    "                zs.append(z.cpu().numpy())\n",
    "        return np.concatenate(zs, axis=0)\n",
    "\n",
    "    Z_train = encode_with_bAE(bAE, X_train)\n",
    "    Z_val   = encode_with_bAE(bAE, X_val)\n",
    "    Z_test  = encode_with_bAE(bAE, X_test)\n",
    "    print(\"Latent shapes:\", Z_train.shape, Z_test.shape)\n",
    "\n",
    "\n",
    "    # 5) Train OneClassSVM on latent of normals only (y==0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    NU_PARAM = 0.01\n",
    "    print(f\"Hu·∫•n luy·ªán OCSVM v·ªõi nu = {NU_PARAM} (∆∞u ti√™n gi·∫£m FP)...\")\n",
    "    ocsvm = OneClassSVM(nu=NU_PARAM, kernel=\"rbf\", gamma='auto', cache_size=1000)\n",
    "    \n",
    "    # OCSVM TRAINING\n",
    "    start_ocsvm = time.time()    \n",
    "    ocsvm.fit(Z_train)\n",
    "    end_ocsvm = time.time()\n",
    "    \n",
    "    print(f\"OCSVM training took {end_ocsvm - start_ocsvm:.2f} seconds.\")\n",
    "\n",
    "    joblib.dump(ocsvm, os.path.join(model_dir, \"ocsvm_latent.joblib\"))\n",
    "\n",
    "    # OCSVM scores: decision_function (higher -> more inlier). We invert so higher -> more anomalous\n",
    "    \n",
    "    scores_ocsvm_val = -ocsvm.decision_function(Z_val)\n",
    "    scores_ocsvm_test = -ocsvm.decision_function(Z_test)\n",
    "    print(f\"Ng∆∞·ª°ng OCSVM ƒë∆∞·ª£c x√°c ƒë·ªãnh l√†: {scores_ocsvm_test:.4f}\\n\")\n",
    "\n",
    "    # 6) Train DeepAnomalyAE on latent normals only (unsupervised AE detector)\n",
    "    deepAE = DeepAnomalyAE(input_dim=latent_dim, latent_dim=max(8, latent_dim//4))\n",
    "    # Use only normal latent vectors from train and val for validation\n",
    "    Z_val_norm = Z_val[y_val == 0] if np.sum(y_val == 0) > 0 else Z_val\n",
    "    train_loader_deep = make_dataloader_from_array(Z_train, batch_size=batch_size, shuffle=True)\n",
    "    val_loader_deep = make_dataloader_from_array(Z_val_norm, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    deepAE_path = os.path.join(model_dir, f\"deepAE_latent{latent_dim}.pth\")\n",
    "    print(\"Training DeepAnomalyAE (detector) on latent normals ...\")\n",
    "    deepAE = train_model(deepAE, train_loader_deep, val_loader_deep,\n",
    "                         epochs=deepAE_epochs, patience=12,\n",
    "                         learning_rate=deepAE_lr, weight_decay=1e-5,\n",
    "                         model_save_path=deepAE_path, loss_fn=nn.MSELoss())\n",
    "\n",
    "    # 7) Compute reconstruction errors (per-sample MSE) from DeepAE on val and test latent\n",
    "    recon_err_val = compute_reconstruction_errors(deepAE, Z_val, batch_size=1024)\n",
    "    recon_err_test = compute_reconstruction_errors(deepAE, Z_test, batch_size=1024)\n",
    "\n",
    "    # 8) Combine scores: normalize each to 0-1 then average\n",
    "    def minmax_scale(arr):\n",
    "        if np.all(np.isclose(arr, arr[0])):\n",
    "            return np.zeros_like(arr)\n",
    "        a = np.array(arr, dtype=float)\n",
    "        a = (a - np.nanmin(a)) / (np.nanmax(a) - np.nanmin(a))\n",
    "        a = np.nan_to_num(a, nan=0.0)\n",
    "        return a\n",
    "\n",
    "    s_ocsvm_val = minmax_scale(scores_ocsvm_val)\n",
    "    s_recon_val = minmax_scale(recon_err_val)\n",
    "    combined_val = 0.5 * s_ocsvm_val + 0.5 * s_recon_val\n",
    "\n",
    "    s_ocsvm_test = minmax_scale(scores_ocsvm_test)\n",
    "    s_recon_test = minmax_scale(recon_err_test)\n",
    "    combined_test = 0.5 * s_ocsvm_test + 0.5 * s_recon_test\n",
    "\n",
    "    # 9) choose threshold on validation set (maximize F1)\n",
    "    best_thr, best_f1 = choose_threshold_by_val(combined_val, y_val)\n",
    "    print(f\"Chosen threshold on validation: {best_thr:.6f} (val F1={best_f1:.4f})\")\n",
    "\n",
    "    y_pred_test = (combined_test >= best_thr).astype(int)  # 1 = anomalous\n",
    "    # convert to same convention: y_true: 1=anomaly\n",
    "    # evaluate and plot\n",
    "    plot_evaluation(y_true=y_test, scores=combined_test, y_pred=y_pred_test, threshold=best_thr, title=f\"Pipeline_Result_latent{latent_dim}\")\n",
    "\n",
    "    # Save models & artifacts\n",
    "    torch.save(bAE.state_dict(), bAE_path)\n",
    "    torch.save(deepAE.state_dict(), deepAE_path)\n",
    "    joblib.dump({'scaler': None}, os.path.join(model_dir, \"artifacts.joblib\"))  # if you used scalers, save them here\n",
    "\n",
    "    print(\"Pipeline finished. Models and evaluation saved in:\", model_dir)\n",
    "    return {\n",
    "        \"bAE\": bAE,\n",
    "        \"deepAE\": deepAE,\n",
    "        \"ocsvm\": ocsvm,\n",
    "        \"Z_train\": Z_train,\n",
    "        \"Z_val\": Z_val,\n",
    "        \"Z_test\": Z_test,\n",
    "        \"combined_test_scores\": combined_test,\n",
    "        \"y_test\": y_test,\n",
    "        \"y_pred_test\": y_pred_test,\n",
    "        \"threshold\": best_thr\n",
    "    }\n",
    "\n",
    "result = run_full_pipeline(\"C:/Users/hoang/Documents/Dataset_KLTN/gotham2025_extracted/gotham2025_proced_merged.csv\", label_col=\"label\", latent_dim=64)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kltn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
