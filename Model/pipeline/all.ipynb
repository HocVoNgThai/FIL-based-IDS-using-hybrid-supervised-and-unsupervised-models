{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd4403-d6aa-4aea-bdd5-bfd1a483f7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sử dụng thiết bị: cuda/n\n",
      "--- BƯỚC 1: LOAD VÀ TIỀN XỬ LÝ DỮ LIỆU (Fill Median + StandardScaler) ---\n",
      "Using 60188 training samples.\n",
      "Read 20000 test samples with 77 features.\n",
      "Unique labels in test set: [0 1]\n",
      "Tiền xử lý dữ liệu thô (StandardScaler) hoàn tất.\n",
      "\n",
      "--- BƯỚC 2: HUẤN LUYỆN MODEL GIẢM CHIỀU (AdvancedDimReducerAE) ---\n",
      "Input dimension: 77 D\n",
      "Bắt đầu huấn luyện model 'AdvancedDimReducerAE' với loss 'MSELoss'...\n",
      "Epoch 10/100 | Train Loss: nan | Val Loss: nan | LR: 5.00e-04\n",
      "Early stopping được kích hoạt tại epoch 15.\n",
      "Huấn luyện hoàn tất trong 122.77 giây. Best Val Loss: N/A. Model tốt nhất đã lưu vào 'best_dim_reducer_ae.pth'./n\n",
      "Warning: Could not load best_dim_reducer_ae.pth.\n",
      "--- BƯỚC 3: TẠO DỮ LIỆU TRONG KHÔNG GIAN TIỀM ẨN (LATENT SPACE) ---\n",
      "Đã giảm chiều dữ liệu từ 77D xuống 64D.\n",
      "\n",
      "--- BƯỚC 4: TIỀN XỬ LÝ DỮ LIỆU (LATENT) ---\n",
      "Tiền xử lý dữ liệu latent (StandardScaler) hoàn tất.\n",
      "\n",
      "--- BƯỚC 5: HUẤN LUYỆN GIAI ĐOẠN 1 (OCSVM) ---\n",
      "Huấn luyện OCSVM với nu = 0.01 (ưu tiên giảm FP)...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nOneClassSVM does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 311\u001b[0m\n\u001b[0;32m    309\u001b[0m anomaly_model_ocsvm \u001b[38;5;241m=\u001b[39m OneClassSVM(nu\u001b[38;5;241m=\u001b[39mNU_PARAM, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m\"\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, cache_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m    310\u001b[0m start_ocsvm \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 311\u001b[0m \u001b[43manomaly_model_ocsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_latent_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    312\u001b[0m end_ocsvm \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOCSVM training took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_ocsvm\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_ocsvm\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hoang\\.conda\\envs\\kltn\\lib\\site-packages\\sklearn\\svm\\_classes.py:1734\u001b[0m, in \u001b[0;36mOneClassSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1709\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Detect the soft boundary of the set of samples X.\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \n\u001b[0;32m   1712\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;124;03m    If X is not a C-ordered contiguous array it is copied.\u001b[39;00m\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1734\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_num_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intercept_\n\u001b[0;32m   1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\hoang\\.conda\\envs\\kltn\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hoang\\.conda\\envs\\kltn\\lib\\site-packages\\sklearn\\svm\\_base.py:197\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    195\u001b[0m     check_consistent_length(X, y)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_targets(y)\n\u001b[0;32m    209\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[0;32m    210\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[0;32m    211\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hoang\\.conda\\envs\\kltn\\lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\hoang\\.conda\\envs\\kltn\\lib\\site-packages\\sklearn\\utils\\validation.py:1370\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m     )\n\u001b[0;32m   1368\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[1;32m-> 1370\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1389\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\hoang\\.conda\\envs\\kltn\\lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[1;32m-> 1107\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hoang\\.conda\\envs\\kltn\\lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hoang\\.conda\\envs\\kltn\\lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nOneClassSVM does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler # <<< Quay lại StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_fscore_support, roc_curve, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Thiết lập thiết bị tính toán (GPU nếu có)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Sử dụng thiết bị: {DEVICE}\\n\")\n",
    "\n",
    "# ===================================================================\n",
    "# CÁC LỚP MODEL (Deep AE Enhanced Capacity)\n",
    "# ===================================================================\n",
    "\n",
    "class AdvancedDimReducerAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Model 1: Giảm chiều dữ liệu (46D -> 32D). Input -> 128 -> 64 -> 32.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, latent_dim=32):\n",
    "        super(AdvancedDimReducerAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.Linear(64, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, z\n",
    "\n",
    "class DeepAnomalyAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Model 2: Phát hiện bất thường (Kiến trúc robust 32D -> 8D).\n",
    "    Increased layer width (capacity), tight bottleneck, increased dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=32, latent_dim=8): # <<< Bottleneck 8D\n",
    "        super(DeepAnomalyAE, self).__init__()\n",
    "        # Input (32) -> 256 -> 128 -> 8 (Latent)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),      # <<< Increased width\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3),                # <<< Increased dropout\n",
    "            nn.Linear(256, 128),      # <<< Increased width\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3),                # <<< Increased dropout\n",
    "            nn.Linear(128, latent_dim) # To 8\n",
    "        )\n",
    "        # 8 (Latent) -> 128 -> 256 -> 32 (Output)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),      # <<< Increased width\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3),                # <<< Increased dropout\n",
    "            nn.Linear(128, 256),      # <<< Increased width\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3),                # <<< Increased dropout\n",
    "            nn.Linear(256, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed\n",
    "\n",
    "# ===================================================================\n",
    "# HÀM HUẤN LUYỆN (Optimized)\n",
    "# ===================================================================\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, patience, learning_rate, weight_decay, model_save_path, loss_fn):\n",
    "    model.to(DEVICE)\n",
    "    criterion = loss_fn\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay) # Adam optimizer\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=8) # Increased patience\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    print(f\"Bắt đầu huấn luyện model '{model.__class__.__name__}' với loss '{criterion.__class__.__name__}'...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        for (data,) in train_loader:\n",
    "            data = data.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            reconstructed = output[0] if isinstance(output, tuple) else output\n",
    "            loss = criterion(reconstructed, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        avg_epoch_train_loss = epoch_train_loss / len(train_loader.dataset) if len(train_loader.dataset) > 0 else 0\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for (data,) in val_loader:\n",
    "                data = data.to(DEVICE)\n",
    "                output = model(data)\n",
    "                reconstructed = output[0] if isinstance(output, tuple) else output\n",
    "                loss = criterion(reconstructed, data)\n",
    "                val_loss += loss.item() * data.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset) if len(val_loader.dataset) > 0 else 0\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_epoch_train_loss:.6f} | Val Loss: {avg_val_loss:.6f} | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss - 1e-6:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping được kích hoạt tại epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    best_loss_str = f\"{best_val_loss:.6f}\" if best_val_loss != float('inf') else \"N/A\"\n",
    "    print(f\"Huấn luyện hoàn tất trong {end_time - start_time:.2f} giây. Best Val Loss: {best_loss_str}. Model tốt nhất đã lưu vào '{model_save_path}'.\\n\")\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_save_path))\n",
    "    except FileNotFoundError: print(f\"Warning: Could not load {model_save_path}.\")\n",
    "    except Exception as e: print(f\"Warning: Error loading {model_save_path}: {e}.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# HÀM HỖ TRỢ ĐÁNH GIÁ & TRỰC QUAN HÓA (More Robust)\n",
    "# ===================================================================\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    except ValueError:\n",
    "        unique_preds=np.unique(y_pred); unique_true=np.unique(y_true); tn,fp,fn,tp=0,0,0,0\n",
    "        if len(unique_true)==1:\n",
    "            if unique_true[0]==0: tn=len(y_true);\n",
    "            else: tp=len(y_true);\n",
    "        elif len(unique_preds)==1:\n",
    "             if unique_preds[0]==0: tn=np.sum(y_true==0); fn=np.sum(y_true==1);\n",
    "             else: fp=np.sum(y_true==0); tp=np.sum(y_true==1);\n",
    "        print(\"Warning: CM calculation issue.\")\n",
    "    fpr = fp/(fp+tn) if (fp+tn)>0 else 0.0\n",
    "    fnr = fn/(fn+tp) if (fn+tp)>0 else 0.0\n",
    "    return accuracy, precision, recall, f1, fpr, fnr, (tn, fp, fn, tp)\n",
    "\n",
    "def plot_evaluation(y_true, scores, y_pred, threshold, title):\n",
    "    print(f\"\\n--- Đang vẽ biểu đồ cho: {title} ---\")\n",
    "    auc = 0.5\n",
    "    try:\n",
    "        y_true=np.asarray(y_true).astype(int); scores=np.asarray(scores); y_pred=np.asarray(y_pred).astype(int);\n",
    "        if not np.issubdtype(scores.dtype, np.number): raise TypeError(\"scores non-numeric\")\n",
    "        scores=np.nan_to_num(scores, nan=np.nanmedian(scores), posinf=np.nanmax(scores[np.isfinite(scores)]), neginf=np.nanmin(scores[np.isfinite(scores)]))\n",
    "        if not np.all(np.isfinite(scores)): raise ValueError(\"Non-finite scores remain\")\n",
    "        unique_labels_true = np.unique(y_true)\n",
    "        if len(unique_labels_true)<2: print(f\"Warning: Only one class ({unique_labels_true}). Cannot calc AUC.\")\n",
    "        else: auc=roc_auc_score(y_true, scores)\n",
    "    except ValueError as e: print(f\"Warning: Cannot calc AUC. Error: {e}\")\n",
    "    except TypeError as e: print(f\"Error calc AUC: {e}\")\n",
    "\n",
    "    accuracy, precision, recall, f1, fpr, fnr, (tn, fp, fn, tp) = calculate_metrics(y_true, y_pred)\n",
    "    print(f\"AUC: {auc:.4f} | F1-Score: {f1:.4f}\"); print(f\"Precision: {precision:.4f} | Recall (TPR): {recall:.4f}\"); print(f\"FPR: {fpr:.4f} | FNR: {fnr:.4f}\"); print(f\"TP: {tp} | FP: {fp} | TN: {tn} | FN: {fn}\")\n",
    "    plt.figure(figsize=(18, 6)); plt.suptitle(title, fontsize=16); plt.subplot(1, 3, 1)\n",
    "    unique_labels_plot = np.unique(y_true)\n",
    "    if len(unique_labels_plot) > 1:\n",
    "        if 0 in unique_labels_plot: sns.histplot(scores[y_true==0], color='blue', label='Normal Scores', kde=True, bins=50, stat=\"density\")\n",
    "        if 1 in unique_labels_plot: sns.histplot(scores[y_true==1], color='red', label='Abnormal Scores', kde=True, bins=50, stat=\"density\")\n",
    "    elif len(unique_labels_plot) == 1: label_text = 'Normal' if unique_labels_plot[0] == 0 else 'Abnormal'; sns.histplot(scores, color='purple', label=f'{label_text} Scores Only', kde=True, bins=50, stat=\"density\")\n",
    "    else: plt.text(0.5, 0.5, 'No data', ha='center')\n",
    "    plt.axvline(threshold, color='green', linestyle='--', label=f'Threshold ({threshold:.4f})'); plt.title('Phân phối Điểm/Lỗi Bất thường'); plt.legend(); plt.subplot(1, 3, 2)\n",
    "    labels_cm = sorted(np.unique(y_true))\n",
    "    if not np.all(np.isin(np.unique(y_pred), labels_cm)): print(f\"Warning: Predictions contain labels not in y_true.\")\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels_cm); sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[f'{l}' for l in labels_cm], yticklabels=[f'{l}' for l in labels_cm]); plt.title('Confusion Matrix'); plt.xlabel('Predicted'); plt.ylabel('Actual'); plt.subplot(1, 3, 3)\n",
    "    if auc > 0.5 and len(unique_labels_plot) > 1 :\n",
    "        fpr_roc, tpr_roc, _ = roc_curve(y_true, scores); plt.plot(fpr_roc, tpr_roc, label=f'AUC = {auc:.4f}')\n",
    "        if (tn + fp) > 0 and (tp + fn) > 0: plt.scatter(fpr, recall, marker='o', color='red', zorder=5, s=100, label=f'Operating Point\\n(FPR={fpr:.2f}, TPR={recall:.2f})')\n",
    "    else: plt.text(0.5, 0.5, 'Cannot draw ROC Curve', ha='center')\n",
    "    plt.title('ROC Curve'); plt.xlabel('FPR'); plt.ylabel('TPR'); plt.legend(); plt.grid(True); plt.tight_layout(rect=[0, 0.03, 1, 0.93]); plt.savefig(f\"evaluation_{title.replace(' ', '_')}.png\"); plt.show()\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# SCRIPT CHÍNH (PIPELINE 2 GIAI ĐOẠN)\n",
    "# ===================================================================\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # --- BƯỚC 1: LOAD VÀ TIỀN XỬ LÝ DỮ LIỆU (RAW) ---\n",
    "    print(\"--- BƯỚC 1: LOAD VÀ TIỀN XỬ LÝ DỮ LIỆU (Fill Median + StandardScaler) ---\")\n",
    "    try:\n",
    "        train_df = pd.read_csv('training_dataset.csv', header=0)\n",
    "        test_df = pd.read_csv('test_dataset.csv', header=0)\n",
    "\n",
    "        feature_names_train = train_df.columns.tolist()\n",
    "        feature_names_test = test_df.columns.tolist()\n",
    "        label_column = feature_names_test[-1]\n",
    "        features_only_names = feature_names_test[:-1]\n",
    "\n",
    "        if label_column in feature_names_train: train_features = [col for col in feature_names_train if col != label_column]\n",
    "        else: train_features = feature_names_train\n",
    "        if set(train_features) != set(features_only_names): raise ValueError(\"Feature mismatch.\")\n",
    "\n",
    "        X_train_df = train_df[train_features].apply(pd.to_numeric, errors='coerce')\n",
    "        X_test_df = test_df[features_only_names].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "        train_median = X_train_df.median()\n",
    "        X_train_raw = X_train_df.fillna(train_median).values\n",
    "        X_test_raw = X_test_df.fillna(train_median).values\n",
    "\n",
    "        y_test_series = test_df[label_column]\n",
    "        y_test_numeric = pd.to_numeric(y_test_series, errors='coerce')\n",
    "        if y_test_numeric.isna().any(): raise ValueError(f\"Label column non-numeric.\")\n",
    "        y_test_binary = y_test_numeric.astype(int).values\n",
    "        unique_labels = np.unique(y_test_binary)\n",
    "        if not np.all(np.isin(unique_labels, [0, 1])): print(f\"Warning: Labels not 0/1: {unique_labels}.\")\n",
    "\n",
    "        print(f\"Using {X_train_raw.shape[0]} training samples.\")\n",
    "        print(f\"Read {X_test_raw.shape[0]} test samples with {X_test_raw.shape[1]} features.\")\n",
    "        print(f\"Unique labels in test set: {np.unique(y_test_binary)}\")\n",
    "\n",
    "    except FileNotFoundError: print(\"Error: Dataset file not found.\"); exit()\n",
    "    except ValueError as e: print(f\"Data processing error: {e}\"); exit()\n",
    "    except Exception as e: print(f\"An unexpected error occurred: {e}\"); exit()\n",
    "\n",
    "    X_train_split_raw, X_val_split_raw = train_test_split(X_train_raw, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Use StandardScaler globally\n",
    "    scaler_raw = StandardScaler()\n",
    "    X_train_scaled = scaler_raw.fit_transform(X_train_split_raw)\n",
    "    X_val_scaled = scaler_raw.transform(X_val_split_raw)\n",
    "    X_test_scaled = scaler_raw.transform(X_test_raw)\n",
    "\n",
    "    BATCH_SIZE = 512\n",
    "    train_raw_loader = DataLoader(TensorDataset(torch.FloatTensor(X_train_scaled)), batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_raw_loader = DataLoader(TensorDataset(torch.FloatTensor(X_val_scaled)), batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    print(\"Tiền xử lý dữ liệu thô (StandardScaler) hoàn tất.\\n\")\n",
    "\n",
    "    # --- BƯỚC 2: HUẤN LUYỆN MODEL GIẢM CHIỀU (DIM-RED AE) ---\n",
    "    print(\"--- BƯỚC 2: HUẤN LUYỆN MODEL GIẢM CHIỀU (AdvancedDimReducerAE) ---\")\n",
    "    INPUT_DIM_RAW = X_train_scaled.shape[1]\n",
    "    print(f\"Input dimension: {INPUT_DIM_RAW}D\")\n",
    "    LATENT_DIM_REDUCER = 32\n",
    "    dim_reducer_ae = AdvancedDimReducerAE(input_dim=INPUT_DIM_RAW, latent_dim=LATENT_DIM_REDUCER)\n",
    "\n",
    "    dim_reducer_ae = train_model(dim_reducer_ae, train_raw_loader, val_raw_loader,\n",
    "                                 epochs=100, patience=15, learning_rate=1e-3,\n",
    "                                 weight_decay=1e-5, model_save_path=\"best_dim_reducer_ae.pth\",\n",
    "                                 loss_fn=nn.MSELoss())\n",
    "\n",
    "    # --- BƯỚC 3: TẠO DỮ LIỆU TRONG KHÔNG GIAN TIỀM ẨN (LATENT SPACE) ---\n",
    "    print(\"--- BƯỚC 3: TẠO DỮ LIỆU TRONG KHÔNG GIAN TIỀM ẨN (LATENT SPACE) ---\")\n",
    "    dim_reducer_ae.eval()\n",
    "    with torch.no_grad():\n",
    "        X_train_scaled_full = scaler_raw.transform(X_train_raw)\n",
    "        _, latent_train = dim_reducer_ae(torch.FloatTensor(X_train_scaled_full).to(DEVICE))\n",
    "        _, latent_val = dim_reducer_ae(torch.FloatTensor(X_val_scaled).to(DEVICE))\n",
    "        _, latent_test = dim_reducer_ae(torch.FloatTensor(X_test_scaled).to(DEVICE))\n",
    "    latent_train_np = latent_train.cpu().numpy()\n",
    "    latent_val_np = latent_val.cpu().numpy()\n",
    "    latent_test_np = latent_test.cpu().numpy()\n",
    "    print(f\"Đã giảm chiều dữ liệu từ {INPUT_DIM_RAW}D xuống {LATENT_DIM_REDUCER}D.\\n\")\n",
    "\n",
    "    # --- BƯỚC 4: TIỀN XỬ LÝ DỮ LIỆU (LATENT) ---\n",
    "    print(\"--- BƯỚC 4: TIỀN XỬ LÝ DỮ LIỆU (LATENT) ---\")\n",
    "    scaler_latent = StandardScaler()\n",
    "    X_train_latent_scaled = scaler_latent.fit_transform(latent_train_np)\n",
    "    X_val_latent_scaled = scaler_latent.transform(latent_val_np)\n",
    "    X_test_latent_scaled = scaler_latent.transform(latent_test_np)\n",
    "\n",
    "    train_indices, val_indices = train_test_split(range(len(X_train_raw)), test_size=0.2, random_state=42)\n",
    "    train_latent_loader = DataLoader(TensorDataset(torch.FloatTensor(X_train_latent_scaled[train_indices])), batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_latent_loader = DataLoader(TensorDataset(torch.FloatTensor(X_val_latent_scaled)), batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    print(\"Tiền xử lý dữ liệu latent (StandardScaler) hoàn tất.\\n\")\n",
    "\n",
    "    # --- BƯỚC 5: HUẤN LUYỆN GIAI ĐOẠN 1 (OCSVM) ---\n",
    "    print(\"--- BƯỚC 5: HUẤN LUYỆN GIAI ĐOẠN 1 (OCSVM) ---\")\n",
    "\n",
    "    NU_PARAM = 0.01\n",
    "\n",
    "    print(f\"Huấn luyện OCSVM với nu = {NU_PARAM} (ưu tiên giảm FP)...\")\n",
    "    \n",
    "    anomaly_model_ocsvm = OneClassSVM(nu=NU_PARAM, kernel=\"rbf\", gamma='auto', cache_size=1000)\n",
    "    start_ocsvm = time.time()\n",
    "    anomaly_model_ocsvm.fit(X_train_latent_scaled)\n",
    "    end_ocsvm = time.time()\n",
    "    print(f\"OCSVM training took {end_ocsvm - start_ocsvm:.2f} seconds.\")\n",
    "    train_scores_ocsvm = -anomaly_model_ocsvm.decision_function(X_train_latent_scaled)\n",
    "    threshold_ocsvm = np.quantile(train_scores_ocsvm, 1 - NU_PARAM)\n",
    "    print(f\"Ngưỡng OCSVM (từ quantile {1-NU_PARAM}) được xác định là: {threshold_ocsvm:.4f}\\n\")\n",
    "\n",
    "    # --- BƯỚC 6: HUẤN LUYỆN GIAI ĐOẠN 2 (DEEP ANOMALY AE) ---\n",
    "    print(\"--- BƯỚC 6: HUẤN LUYỆN GIAI ĐOẠN 2 (DeepAnomalyAE - Enhanced Capacity) ---\")\n",
    "\n",
    "    # Instantiate ENHANCED Deep AE (32D input, 8D latent)\n",
    "    anomaly_model_ae = DeepAnomalyAE(input_dim=LATENT_DIM_REDUCER, latent_dim=8)\n",
    "\n",
    "    anomaly_model_ae = train_model(anomaly_model_ae, train_latent_loader, val_latent_loader,\n",
    "                                   epochs=150, patience=20, learning_rate=1e-4,\n",
    "                                   weight_decay=1e-5, model_save_path=\"best_anomaly_ae_enhanced.pth\", # New save path\n",
    "                                   loss_fn=nn.L1Loss())\n",
    "\n",
    "    print(\"Đang tính toán ngưỡng DAE chuyên biệt (không rò rỉ) cho Giai đoạn 2...\")\n",
    "    anomaly_model_ae.eval()\n",
    "    with torch.no_grad():\n",
    "        X_train_latent_tensor_full = torch.FloatTensor(X_train_latent_scaled).to(DEVICE)\n",
    "        reconstructed_train = anomaly_model_ae(X_train_latent_tensor_full)\n",
    "        train_errors_ae = torch.mean(torch.abs(X_train_latent_tensor_full - reconstructed_train), dim=1).cpu().numpy()\n",
    "\n",
    "    idx_train_normal_ocsvm = np.where(train_scores_ocsvm <= threshold_ocsvm)[0]\n",
    "\n",
    "    if len(idx_train_normal_ocsvm) == 0:\n",
    "        print(\"Warning: OCSVM classified all training samples as abnormal. Using fallback threshold.\")\n",
    "        threshold_ae = np.quantile(train_errors_ae, 0.98)\n",
    "    else:\n",
    "        train_errors_ae_filtered = train_errors_ae[idx_train_normal_ocsvm]\n",
    "        print(f\"Tính toán ngưỡng AE trên {len(train_errors_ae_filtered)} mẫu train (thay vì {len(train_errors_ae)} mẫu)\")\n",
    "        AE_QUANTILE = 0.98 # Keep 0.98\n",
    "        threshold_ae = np.quantile(train_errors_ae_filtered, AE_QUANTILE)\n",
    "\n",
    "    print(f\"Ngưỡng Deep AE (từ quantile {AE_QUANTILE}) được xác định là: {threshold_ae:.4f}\\n\")\n",
    "    # --- KẾT THÚC BƯỚC 6 ---\n",
    "\n",
    "\n",
    "    # --- BƯỚC 7: ĐÁNH GIÁ (STAGE 1 - OCSVM-ONLY) ---\n",
    "    print(\"=\"*20 + \" BƯỚC 7: ĐÁNH GIÁ (STAGE 1 - OCSVM-ONLY) \" + \"=\"*20)\n",
    "    test_scores_ocsvm = -anomaly_model_ocsvm.decision_function(X_test_latent_scaled)\n",
    "    y_pred_ocsvm = (test_scores_ocsvm > threshold_ocsvm).astype(int)\n",
    "    plot_evaluation(y_test_binary, test_scores_ocsvm, y_pred_ocsvm, threshold_ocsvm,\n",
    "                    \"Stage 1 - OCSVM Filter Results\")\n",
    "\n",
    "    # --- BƯỚC 8: ĐÁNH GIÁ (STAGE 2 - CASCADE PIPELINE) ---\n",
    "    print(\"\\n\" + \"=\"*20 + \" BƯỚC 8: ĐÁNH GIÁ (STAGE 2 - PIPELINE) \" + \"=\"*20)\n",
    "    print(\"Thực hiện pipeline: Các mẫu OCSVM dự đoán là Normal (0) sẽ được Deep AE dự đoán lại...\")\n",
    "\n",
    "    anomaly_model_ae.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_latent_tensor_full = torch.FloatTensor(X_test_latent_scaled).to(DEVICE)\n",
    "        reconstructed_test = anomaly_model_ae(X_test_latent_tensor_full)\n",
    "        test_errors_ae = torch.mean(torch.abs(X_test_latent_tensor_full - reconstructed_test), dim=1).cpu().numpy()\n",
    "\n",
    "    idx_normal_ocsvm = np.where(y_pred_ocsvm == 0)[0]\n",
    "    print(f\"OCSVM (Stage 1) phát hiện {len(y_pred_ocsvm) - len(idx_normal_ocsvm)} mẫu bất thường.\")\n",
    "    print(f\"Đưa {len(idx_normal_ocsvm)} mẫu 'Normal' vào Deep AE (Stage 2) để kiểm tra lại...\")\n",
    "\n",
    "    if len(idx_normal_ocsvm) > 0:\n",
    "        y_test_filtered = y_test_binary[idx_normal_ocsvm]\n",
    "        test_errors_filtered = test_errors_ae[idx_normal_ocsvm]\n",
    "        y_pred_ae_on_normals = (test_errors_filtered > threshold_ae).astype(int)\n",
    "\n",
    "        plot_evaluation(y_test_filtered, test_errors_filtered, y_pred_ae_on_normals, threshold_ae,\n",
    "                        \"Stage 2 - Deep AE on OCSVM-Normals\")\n",
    "    else:\n",
    "        print(\"OCSVM classified all test samples as abnormal. Stage 2 evaluation skipped.\")\n",
    "        y_pred_ae_on_normals = np.array([], dtype=int)\n",
    "\n",
    "\n",
    "    # --- BƯỚC 9: KẾT QUẢ CUỐI CÙNG CỦA PIPELINE ---\n",
    "    print(\"\\n\" + \"=\"*20 + \" BƯỚC 9: KẾT QUẢ CUỐI CÙNG (COMBINED) \" + \"=\"*20)\n",
    "\n",
    "    y_pred_final = np.copy(y_pred_ocsvm)\n",
    "    if len(idx_normal_ocsvm) > 0 and len(y_pred_ae_on_normals) == len(idx_normal_ocsvm):\n",
    "         y_pred_final[idx_normal_ocsvm] = y_pred_ae_on_normals\n",
    "    elif len(idx_normal_ocsvm) > 0:\n",
    "         print(\"Warning: Size mismatch during final prediction update.\")\n",
    "\n",
    "    print(\"Các chỉ số cuối cùng của toàn bộ pipeline (OCSVM -> Deep AE):\")\n",
    "    accuracy_final, precision_final, recall_final, f1_final, fpr_final, fnr_final, (tn_final, fp_final, fn_final, tp_final) = calculate_metrics(y_test_binary, y_pred_final)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy_final:.4f}\")\n",
    "    print(f\"Precision: {precision_final:.4f}\")\n",
    "    print(f\"Recall (TPR): {recall_final:.4f}\")\n",
    "    print(f\"F1-Score: {f1_final:.4f}\")\n",
    "    print(f\"False Positive Rate (FPR): {fpr_final:.4f}\")\n",
    "    print(f\"False Negative Rate (FNR): {fnr_final:.4f}\")\n",
    "    print(f\"TP: {tp_final} | FP: {fp_final} | TN: {tn_final} | FN: {fn_final}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm_final = confusion_matrix(y_test_binary, y_pred_final)\n",
    "    sns.heatmap(cm_final, annot=True, fmt='d', cmap='Oranges',\n",
    "                xticklabels=['Normal (0)', 'Abnormal (1)'],\n",
    "                yticklabels=['Normal (0)', 'Abnormal (1)'])\n",
    "    plt.title('Final Confusion Matrix (Pipeline: OCSVM -> Deep AE)')\n",
    "    plt.xlabel('Predicted'); plt.ylabel('Actual')\n",
    "    plt.savefig(\"final_pipeline_confusion_matrix.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d6106-c97f-43b5-a298-5ad82ccd610e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kltn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
