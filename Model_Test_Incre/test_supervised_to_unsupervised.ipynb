{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09a699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INCRE SGDOCSVM =====\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd \n",
    "import dask.dataframe as dd\n",
    "\n",
    "# ----- THƯ VIỆN XỬ LÝ CHÍNH\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, Normalizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, precision_score,\n",
    "    recall_score, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    accuracy_score, log_loss\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ----- CÁC THƯ VIỆN HỖ TRỢ -----\n",
    "import hashlib  \n",
    "import ipaddress\n",
    "import json\n",
    "import glob\n",
    "import gc\n",
    "import pickle\n",
    "import tempfile\n",
    "\n",
    "# ----- THƯ VIỆN PLT -----\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Model\n",
    "from sklearn.linear_model import SGDOneClassSVM\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import xgboost as xgb\n",
    "\n",
    "# ---------------- CONFIG -----------------\n",
    "dir_in = [f\"C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/Incremental_1.3/session{i}.parquet\" for i in range(0, 3)]\n",
    "dir_in_train = [f\"C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/Incremental_1.3/session{i}_train.parquet\" for i in range(0, 3)]\n",
    "dir_in_test = [f\"C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/Incremental_1.3/session{i}_test.parquet\" for i in range(0, 3)]\n",
    "\n",
    "gc.collect()\n",
    "QUERY_N = 200          # số mẫu được query mỗi session (n trong paper)\n",
    "K_NEIGHBORS = 100      # KNN outlier scoring\n",
    "CLASS_WEIGHTS = {0:1.0, 1:10.0}   # normal=1, attack=10\n",
    "RANDOM_STATE = 42\n",
    "# -----------------------------------------\n",
    "\n",
    "\n",
    "# ====== FUNCTION =====\n",
    "dtypes = {}    \n",
    "with open('features.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for key, type in data.items():\n",
    "        if type == \"int8\":\n",
    "            dtypes[key]= np.int8\n",
    "        elif type == \"float32\":\n",
    "            dtypes[key] = np.float32\n",
    "    json_file.close()\n",
    "\n",
    "print(dtypes)\n",
    "\n",
    "def astype(df):\n",
    "    for key, type in df.dtypes.items():\n",
    "        # print(f\"Key: {key} \\t {type}\")\n",
    "        if type == \"int8\":\n",
    "            df[key] = df[key].astype(np.int8)\n",
    "        elif type == \"float32\":\n",
    "            df[key] = df[key].astype(np.float32)\n",
    "            \n",
    "    return df\n",
    "# def load_df(path):\n",
    "#     if path.endswith(\".parquet\"):\n",
    "#         return pd.read_parquet(path)\n",
    "#     return pd.read_csv(path)\n",
    "\n",
    "# def preprocess(df, feature_cols, scaler=None):\n",
    "#     df = df.dropna().copy()\n",
    "#     X = df[feature_cols].values\n",
    "\n",
    "#     if scaler is None:\n",
    "#         scaler = StandardScaler()\n",
    "#         X = scaler.fit_transform(X)\n",
    "#     else:\n",
    "#         X = scaler.transform(X)\n",
    "\n",
    "#     y = df[\"label\"].values\n",
    "#     return X, y, scaler\n",
    "\n",
    "def outlierness_scores(X, k=K_NEIGHBORS):\n",
    "    k = min(k, X.shape[0]-1)\n",
    "    nbrs = NearestNeighbors(n_neighbors=k).fit(X)\n",
    "    distances, _ = nbrs.kneighbors(X)\n",
    "    return distances[:, -1]\n",
    "\n",
    "def uncertainty_from_proba(probas):\n",
    "    return 1 - np.max(probas, axis=1)\n",
    "\n",
    "def query_function(X, probas, out_scores, n=QUERY_N):\n",
    "    half = n // 2\n",
    "    idx_out = np.argsort(-out_scores)[:half]\n",
    "    uncert = uncertainty_from_proba(probas)\n",
    "    idx_unc = np.argsort(-uncert)[:half]\n",
    "    idx = np.unique(np.concatenate([idx_out, idx_unc]))\n",
    "\n",
    "    if len(idx) < n:\n",
    "        need = n - len(idx)\n",
    "        remaining = [i for i in np.argsort(-uncert) if i not in idx]\n",
    "        idx = np.concatenate([idx, remaining[:need]])\n",
    "\n",
    "    return idx.astype(int)\n",
    "\n",
    "def train_xgb(X_train, y_train):\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"max_depth\": 6,\n",
    "        \"seed\": RANDOM_STATE,\n",
    "        \"verbosity\": 0\n",
    "    }\n",
    "    weights = np.array([CLASS_WEIGHTS[int(y)] for y in y_train])\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train, weight=weights)\n",
    "    bst = xgb.train(params, dtrain, num_boost_round=100)\n",
    "    return bst\n",
    "\n",
    "def predict_proba_xgb(bst, X):\n",
    "    d = xgb.DMatrix(X)\n",
    "    preds = bst.predict(d)\n",
    "    return np.vstack([1 - preds, preds]).T\n",
    "\n",
    "# ================= MAIN PIPELINE ==================\n",
    "\n",
    "def run_incremental_AL(train_files, test_files):\n",
    "    \n",
    "    # ---- Load all sessions ----\n",
    "    df_train = pd.read_parquet(train_files[0])\n",
    "    df_train = astype(df_train)\n",
    "    X_train = df_train.drop([\"Label\", \"Binary Label\"], axis=1)\n",
    "    y_train = df_train[\"Label\"]\n",
    "\n",
    "    # ---- Preprocess session 0 + train initial model ----\n",
    "    # X0, y0, scaler = preprocess(df0, feature_cols)\n",
    "    print(f\"[INFO] Training initial model on Session 0: {X_train.shape[0]} samples\")\n",
    "    \n",
    "    model = train_xgb(X_train, y_train)\n",
    "    \n",
    "    del df_train, X_train, y_train\n",
    "    gc.collect()\n",
    "\n",
    "    # ---- Evaluate initial ----\n",
    "    df_test = pd.read_parquet(test_files[0])\n",
    "    df_test = astype(df_test)\n",
    "    \n",
    "    X_test = df_test.drop([\"Label\", \"Binary Label\"], axis=1)\n",
    "    y_test = df_train[\"Label\"]\n",
    "    \n",
    "    p_test = predict_proba_xgb(model, X_test)[:,1]\n",
    "    auc = roc_auc_score(y_test, p_test)\n",
    "    print(f\"[AUC] After Session 0: {auc:.4f}\")\n",
    "    \n",
    "    del df_test, X_test, y_test\n",
    "    gc.collect()\n",
    "    \n",
    "    # ---------- SESSION 1 ----------\n",
    "    df_train = pd.read_parquet(train_files[1])\n",
    "    df_train = astype(df_train)\n",
    "    X1 = df_train.drop([\"Label\", \"Binary Label\"], axis=1)\n",
    "    y1 = df_train[\"Label\"]\n",
    "    print(f\"\\n[INFO] Active learning on Session 1: {df_train.shape[0]} samples\")\n",
    "\n",
    "    probas1 = predict_proba_xgb(model, X1)\n",
    "    out1 = outlierness_scores(X1)\n",
    "\n",
    "    idx = query_function(X1, probas1, out1)\n",
    "\n",
    "    # Add queried samples to pool\n",
    "    pool_X = np.vstack([pool_X, X1[idx]])\n",
    "    pool_y = np.concatenate([pool_y, y1[idx]])\n",
    "\n",
    "    print(f\"[INFO] Added {len(idx)} labeled samples from Session 1\")\n",
    "    model = train_xgb(pool_X, pool_y)\n",
    "    \n",
    "    \n",
    "    df_test = pd.read_parquet(test_files[1])\n",
    "    df_test = astype(df_test)\n",
    "    \n",
    "    X_test = df_test.drop([\"Label\", \"Binary Label\"], axis=1)\n",
    "    y_test = df_train[\"Label\"]\n",
    "    \n",
    "    p_test = predict_proba_xgb(model, X_test)[:,1]\n",
    "    auc = roc_auc_score(y_test, p_test)\n",
    "    print(f\"[AUC] After Session 1: {auc:.4f}\")\n",
    "\n",
    "    del X1, y1, df_train, df_test, X_test, y_test\n",
    "    gc.collect()\n",
    "    \n",
    "    # ---------- SESSION 2 ----------\n",
    "    \n",
    "    df2 = pd.read_parquet(train_files[1])\n",
    "    df2 = astype(df_train)\n",
    "    X2 = df_train.drop([\"Label\", \"Binary Label\"], axis=1)\n",
    "    y2 = df_train[\"Label\"]\n",
    "    \n",
    "    print(f\"\\n[INFO] Active learning on Session 2: {df2.shape[0]} samples\")\n",
    "\n",
    "    probas2 = predict_proba_xgb(model, X2)\n",
    "    out2 = outlierness_scores(X2)\n",
    "\n",
    "    idx = query_function(X2, probas2, out2)\n",
    "\n",
    "    # Add queried samples\n",
    "    pool_X = np.vstack([pool_X, X2[idx]])\n",
    "    pool_y = np.concatenate([pool_y, y2[idx]])\n",
    "\n",
    "    print(f\"[INFO] Added {len(idx)} labeled samples from Session 2\")\n",
    "    model = train_xgb(pool_X, pool_y)\n",
    "\n",
    "    p_test = predict_proba_xgb(model, X_test)[:,1]\n",
    "    auc = roc_auc_score(y_test, p_test)\n",
    "    print(f\"[AUC] After Session 2: {auc:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def incre_xgboost(train_files: str,  test_files):\n",
    "    \n",
    "    history = {\"loss\": [], \"acc\": [], \"auc\": []}\n",
    "    models = []\n",
    "    \n",
    "    for index, filepath in enumerate(train_files):\n",
    "        df_train =  pd.read_parquet(filepath)\n",
    "        df_train = astype(df_train)\n",
    "        \n",
    "        numClasses = len(df_train[\"Label\"].value_counts())\n",
    "    \n",
    "        trainX = df_train.drop([\"Label\", \"Binary Label\"], axis =1)\n",
    "        trainy = df_train[\"Label\"]\n",
    "        \n",
    "        trainD = xgb.DMatrix(trainX, label=trainy)\n",
    "        \n",
    "        del trainX, trainy, df_train\n",
    "        gc.collect()\n",
    "        \n",
    "        df_test = pd.read_parquet(test_files[index])\n",
    "        df_test =astype(df_test)\n",
    "        \n",
    "        testX = df_test.drop([\"Label\", \"Binary Label\"], axis =1)\n",
    "        testy = df_test[\"Label\"]\n",
    "        \n",
    "        testD = xgb.DMatrix(testX)\n",
    "\n",
    "        del testX, df_test\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"\\n=== Session {index} Training ===\")\n",
    "        if index == 0:\n",
    "            params = {\n",
    "            'objective': 'multi:softprob',      # hoặc reg:squarederror nếu regression / binary:logistic / multi:softmax\n",
    "            'num_class': numClasses,\n",
    "            'eval_metric': 'auc',                # hoặc rmse \n",
    "            'eta': 0.001,                         # giảm learning rate để tránh phá mô hình cũ\n",
    "            'max_depth': 4,                      # tránh overfit session mới\n",
    "            'subsample': 0.9,                    # giảm ảnh hưởng session mới\n",
    "            'colsample_bytree': 0.9,             # giảm cập nhật lớn\n",
    "            'tree_method': 'hist',               # ổn định hơn khi update\n",
    "            'process_type': 'default'\n",
    "            }\n",
    "            model = xgb.train(params, trainD, num_boost_round =96) # , num_boost_round=20\n",
    "        else:\n",
    "            # params = {\n",
    "            # 'objective': 'multi:softprob',      # hoặc reg:squarederror nếu regression / binary:logistic / multi:softmax\n",
    "            # 'num_class': numClasses,\n",
    "            # 'eval_metric': 'auc',                # hoặc rmse \n",
    "            # 'eta': 0.001,                         # giảm learning rate để tránh phá mô hình cũ\n",
    "            # 'max_depth': 4,                      # tránh overfit session mới\n",
    "            # 'subsample': 0.9,                    # giảm ảnh hưởng session mới\n",
    "            # 'colsample_bytree': 0.9,             # giảm cập nhật lớn\n",
    "            # 'tree_method': 'hist',               # ổn định hơn khi update\n",
    "            # 'process_type': 'update',            # (QUAN TRỌNG) incremental-style\n",
    "            # 'updater': 'updater',         \n",
    "            # 'refresh_leaf': True                 # cập nhật leaf values thay vì cấu trúc\n",
    "            # }\n",
    "            params = {\n",
    "            'objective': 'multi:softprob',      # hoặc reg:squarederror nếu regression / binary:logistic / multi:softmax\n",
    "            'num_class': numClasses,\n",
    "            'eval_metric': 'auc',                # hoặc rmse \n",
    "            'eta': 0.001,                         # giảm learning rate để tránh phá mô hình cũ\n",
    "            'max_depth': 4,                      # tránh overfit session mới\n",
    "            'subsample': 0.9,                    # giảm ảnh hưởng session mới\n",
    "            'colsample_bytree': 0.9,             # giảm cập nhật lớn\n",
    "            'tree_method': 'hist',               # ổn định hơn khi update\n",
    "            'process_type': 'default'\n",
    "            }\n",
    "            model = xgb.train(params, trainD, num_boost_round=32,\n",
    "                                xgb_model=models[index -1])\n",
    "        \n",
    "        models.append(model)\n",
    "        \n",
    "        # =====================================================\n",
    "        #  PREDICT\n",
    "        # =====================================================\n",
    "        preds_proba = model.predict(testD)  # shape: (N, numClasses)\n",
    "        preds = np.argmax(preds_proba, axis=1)\n",
    "\n",
    "        # =====================================================\n",
    "        #  METRICS\n",
    "        # =====================================================\n",
    "        loss = log_loss(testy, preds_proba)\n",
    "        acc = accuracy_score(testy, preds)\n",
    "\n",
    "        # AUC multi-class\n",
    "        auc = roc_auc_score(\n",
    "            pd.get_dummies(testy), \n",
    "            preds_proba,\n",
    "            average=\"macro\",\n",
    "            multi_class=\"ovr\"\n",
    "        )\n",
    "        \n",
    "        history[\"loss\"].append(loss)\n",
    "        history[\"acc\"].append(acc)\n",
    "        history[\"auc\"].append(auc)\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Step {index+1}/{len(train_files)} | Loss={loss:.4f} | Acc={acc:.4f} | AUC={auc:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    history = incre_xgboost(dir_in_train, dir_in_test)\n",
    "    steps = np.arange(1, len(dir_in_test) +1)\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(steps, history[\"loss\"])\n",
    "    plt.title(\"XGBoost Incremental - Loss\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(steps, history[\"acc\"])\n",
    "    plt.title(\"XGBoost Incremental - Accuracy\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(steps, history[\"auc\"])\n",
    "    plt.title(\"XGBoost Incremental - AUC\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81cd4381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Src IP': <class 'numpy.float32'>, 'Src Port': <class 'numpy.int8'>, 'Dst IP': <class 'numpy.float32'>, 'Dst Port': <class 'numpy.int8'>, 'Protocol': <class 'numpy.int8'>, 'Flow Duration': <class 'numpy.float32'>, 'Total Fwd Packet': <class 'numpy.float32'>, 'Total Bwd packets': <class 'numpy.float32'>, 'Total Length of Fwd Packet': <class 'numpy.float32'>, 'Total Length of Bwd Packet': <class 'numpy.float32'>, 'Fwd Packet Length Max': <class 'numpy.float32'>, 'Fwd Packet Length Min': <class 'numpy.float32'>, 'Fwd Packet Length Mean': <class 'numpy.float32'>, 'Fwd Packet Length Std': <class 'numpy.float32'>, 'Bwd Packet Length Max': <class 'numpy.float32'>, 'Bwd Packet Length Min': <class 'numpy.float32'>, 'Bwd Packet Length Mean': <class 'numpy.float32'>, 'Bwd Packet Length Std': <class 'numpy.float32'>, 'Flow Bytes/s': <class 'numpy.float32'>, 'Flow Packets/s': <class 'numpy.float32'>, 'Flow IAT Mean': <class 'numpy.float32'>, 'Flow IAT Std': <class 'numpy.float32'>, 'Flow IAT Max': <class 'numpy.float32'>, 'Flow IAT Min': <class 'numpy.float32'>, 'Fwd IAT Total': <class 'numpy.float32'>, 'Fwd IAT Mean': <class 'numpy.float32'>, 'Fwd IAT Std': <class 'numpy.float32'>, 'Fwd IAT Max': <class 'numpy.float32'>, 'Fwd IAT Min': <class 'numpy.float32'>, 'Bwd IAT Total': <class 'numpy.float32'>, 'Bwd IAT Mean': <class 'numpy.float32'>, 'Bwd IAT Std': <class 'numpy.float32'>, 'Bwd IAT Max': <class 'numpy.float32'>, 'Bwd IAT Min': <class 'numpy.float32'>, 'Fwd PSH Flags': <class 'numpy.float32'>, 'Bwd PSH Flags': <class 'numpy.float32'>, 'Fwd URG Flags': <class 'numpy.float32'>, 'Bwd URG Flags': <class 'numpy.float32'>, 'Fwd Header Length': <class 'numpy.float32'>, 'Bwd Header Length': <class 'numpy.float32'>, 'Fwd Packets/s': <class 'numpy.float32'>, 'Bwd Packets/s': <class 'numpy.float32'>, 'Packet Length Min': <class 'numpy.float32'>, 'Packet Length Max': <class 'numpy.float32'>, 'Packet Length Mean': <class 'numpy.float32'>, 'Packet Length Std': <class 'numpy.float32'>, 'Packet Length Variance': <class 'numpy.float32'>, 'FIN Flag Count': <class 'numpy.float32'>, 'SYN Flag Count': <class 'numpy.float32'>, 'RST Flag Count': <class 'numpy.float32'>, 'PSH Flag Count': <class 'numpy.float32'>, 'ACK Flag Count': <class 'numpy.float32'>, 'URG Flag Count': <class 'numpy.float32'>, 'CWR Flag Count': <class 'numpy.float32'>, 'ECE Flag Count': <class 'numpy.float32'>, 'Down/Up Ratio': <class 'numpy.float32'>, 'Average Packet Size': <class 'numpy.float32'>, 'Fwd Segment Size Avg': <class 'numpy.float32'>, 'Bwd Segment Size Avg': <class 'numpy.float32'>, 'Fwd Bytes/Bulk Avg': <class 'numpy.float32'>, 'Fwd Packet/Bulk Avg': <class 'numpy.float32'>, 'Fwd Bulk Rate Avg': <class 'numpy.float32'>, 'Bwd Bytes/Bulk Avg': <class 'numpy.float32'>, 'Bwd Packet/Bulk Avg': <class 'numpy.float32'>, 'Bwd Bulk Rate Avg': <class 'numpy.float32'>, 'Subflow Fwd Packets': <class 'numpy.float32'>, 'Subflow Fwd Bytes': <class 'numpy.float32'>, 'Subflow Bwd Packets': <class 'numpy.float32'>, 'Subflow Bwd Bytes': <class 'numpy.float32'>, 'FWD Init Win Bytes': <class 'numpy.float32'>, 'Bwd Init Win Bytes': <class 'numpy.float32'>, 'Fwd Act Data Pkts': <class 'numpy.float32'>, 'Fwd Seg Size Min': <class 'numpy.float32'>, 'Active Mean': <class 'numpy.float32'>, 'Active Std': <class 'numpy.float32'>, 'Active Max': <class 'numpy.float32'>, 'Active Min': <class 'numpy.float32'>, 'Idle Mean': <class 'numpy.float32'>, 'Idle Std': <class 'numpy.float32'>, 'Idle Max': <class 'numpy.float32'>, 'Idle Min': <class 'numpy.float32'>, 'Label': <class 'numpy.int8'>, 'Binary Label': <class 'numpy.int8'>}\n",
      "Training Deep AE on 1020000 samples...\n",
      "AE Threshold set: 0.060991\n",
      "Kết quả dự đoán anomaly (0=normal, 1=outlier):\n",
      "{0: 67082, 1: 418}\n"
     ]
    }
   ],
   "source": [
    "# ===== INCRE SGDOCSVM =====\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd \n",
    "import dask.dataframe as dd\n",
    "\n",
    "# ----- THƯ VIỆN XỬ LÝ CHÍNH\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, Normalizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, precision_score,\n",
    "    recall_score, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    accuracy_score, log_loss\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ----- CÁC THƯ VIỆN HỖ TRỢ -----\n",
    "import hashlib  \n",
    "import ipaddress\n",
    "import json\n",
    "import glob\n",
    "import gc\n",
    "import pickle\n",
    "import tempfile\n",
    "\n",
    "# ----- THƯ VIỆN PLT -----\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ----- Models -----\n",
    "from tensorflow.keras import layers, models\n",
    "from models import AETrainer\n",
    "\n",
    "# ---------------- CONFIG -----------------\n",
    "dir_in = [f\"C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/Incremental_1.3/session{i}.parquet\" for i in range(0, 3)]\n",
    "dir_in_train = [f\"C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/Incremental_1.3/session{i}_train.parquet\" for i in range(0, 3)]\n",
    "dir_in_test = [f\"C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/Incremental_1.3/session{i}_test.parquet\" for i in range(0, 3)]\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# ====== FUNCTION =====\n",
    "dtypes = {}    \n",
    "with open('features.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for key, type in data.items():\n",
    "        if type == \"int8\":\n",
    "            dtypes[key]= np.int8\n",
    "        elif type == \"float32\":\n",
    "            dtypes[key] = np.float32\n",
    "    json_file.close()\n",
    "\n",
    "print(dtypes)\n",
    "\n",
    "def astype(df):\n",
    "    for key, type in df.dtypes.items():\n",
    "        # print(f\"Key: {key} \\t {type}\")\n",
    "        if type == \"int8\":\n",
    "            df[key] = df[key].astype(np.int8)\n",
    "        elif type == \"float32\":\n",
    "            df[key] = df[key].astype(np.float32)\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "encoding_dim = 64\n",
    "\n",
    "df_train =  pd.read_parquet(dir_in_train[0])\n",
    "df_train = astype(df_train)\n",
    "\n",
    "# numClasses = len(df_train[\"Label\"].value_counts())\n",
    "\n",
    "trainX = df_train.drop([\"Label\", \"Binary Label\"], axis =1)\n",
    "# trainy = df_train[\"Label\"]\n",
    "\n",
    "df_test =  pd.read_parquet(dir_in_test[1])\n",
    "df_test = astype(df_test)\n",
    "\n",
    "testX = df_test.drop([\"Label\", \"Binary Label\"], axis =1)\n",
    "\n",
    "del  df_train, df_test\n",
    "gc.collect()\n",
    "\n",
    "input_dim = trainX.shape[1]\n",
    "\n",
    "# ======= TRAIN WITH PYTORCH DAE (Your AETrainer) =======\n",
    "trainer = AETrainer(input_dim=input_dim, encoding_dim=encoding_dim, lr=1e-3)\n",
    "\n",
    "# Train AE on known (benign) data: trainX\n",
    "trainer.train_on_known_data(trainX.values, epochs=50, batch_size=1024, verbose=True)\n",
    "\n",
    "# === Predict reconstruction errors on test ===\n",
    "errors = trainer.get_reconstruction_errors(testX.values)\n",
    "\n",
    "# === Nếu không muốn dùng threshold mặc định mean+0.5*std, \n",
    "#     có thể override bằng percentile giống Keras 99%\n",
    "#     → uncomment dòng dưới:\n",
    "# trainer.known_threshold = np.percentile(errors, 99)\n",
    "\n",
    "# === Predict outlier ===\n",
    "y_pred = (errors > trainer.known_threshold).astype(int)\n",
    "\n",
    "# === Print số lượng 0 - 1 ===\n",
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "print(\"Kết quả dự đoán anomaly (0=normal, 1=outlier):\")\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a691851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Predict reconstruction errors on test ===\n",
    "errors = trainer.get_reconstruction_errors(testX.values)\n",
    "\n",
    "# === Nếu không muốn dùng threshold mặc định mean+0.5*std, \n",
    "#     có thể override bằng percentile giống Keras 99%\n",
    "#     → uncomment dòng dưới:\n",
    "# trainer.known_threshold = np.percentile(errors, 99)\n",
    "\n",
    "# === Predict outlier ===\n",
    "y_pred = (errors > trainer.known_threshold).astype(int)\n",
    "\n",
    "# === Print số lượng 0 - 1 ===\n",
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "print(\"Kết quả dự đoán anomaly (0=normal, 1=outlier):\")\n",
    "print(dict(zip(unique, counts)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kltn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
