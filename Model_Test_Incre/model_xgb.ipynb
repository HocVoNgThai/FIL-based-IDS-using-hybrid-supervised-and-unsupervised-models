{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d48754a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Src IP': <class 'numpy.float32'>, 'Src Port': <class 'numpy.int8'>, 'Dst IP': <class 'numpy.float32'>, 'Dst Port': <class 'numpy.int8'>, 'Protocol': <class 'numpy.int8'>, 'Flow Duration': <class 'numpy.float32'>, 'Total Fwd Packet': <class 'numpy.float32'>, 'Total Bwd packets': <class 'numpy.float32'>, 'Total Length of Fwd Packet': <class 'numpy.float32'>, 'Total Length of Bwd Packet': <class 'numpy.float32'>, 'Fwd Packet Length Max': <class 'numpy.float32'>, 'Fwd Packet Length Min': <class 'numpy.float32'>, 'Fwd Packet Length Mean': <class 'numpy.float32'>, 'Fwd Packet Length Std': <class 'numpy.float32'>, 'Bwd Packet Length Max': <class 'numpy.float32'>, 'Bwd Packet Length Min': <class 'numpy.float32'>, 'Bwd Packet Length Mean': <class 'numpy.float32'>, 'Bwd Packet Length Std': <class 'numpy.float32'>, 'Flow Bytes/s': <class 'numpy.float32'>, 'Flow Packets/s': <class 'numpy.float32'>, 'Flow IAT Mean': <class 'numpy.float32'>, 'Flow IAT Std': <class 'numpy.float32'>, 'Flow IAT Max': <class 'numpy.float32'>, 'Flow IAT Min': <class 'numpy.float32'>, 'Fwd IAT Total': <class 'numpy.float32'>, 'Fwd IAT Mean': <class 'numpy.float32'>, 'Fwd IAT Std': <class 'numpy.float32'>, 'Fwd IAT Max': <class 'numpy.float32'>, 'Fwd IAT Min': <class 'numpy.float32'>, 'Bwd IAT Total': <class 'numpy.float32'>, 'Bwd IAT Mean': <class 'numpy.float32'>, 'Bwd IAT Std': <class 'numpy.float32'>, 'Bwd IAT Max': <class 'numpy.float32'>, 'Bwd IAT Min': <class 'numpy.float32'>, 'Fwd PSH Flags': <class 'numpy.float32'>, 'Bwd PSH Flags': <class 'numpy.float32'>, 'Fwd URG Flags': <class 'numpy.float32'>, 'Bwd URG Flags': <class 'numpy.float32'>, 'Fwd Header Length': <class 'numpy.float32'>, 'Bwd Header Length': <class 'numpy.float32'>, 'Fwd Packets/s': <class 'numpy.float32'>, 'Bwd Packets/s': <class 'numpy.float32'>, 'Packet Length Min': <class 'numpy.float32'>, 'Packet Length Max': <class 'numpy.float32'>, 'Packet Length Mean': <class 'numpy.float32'>, 'Packet Length Std': <class 'numpy.float32'>, 'Packet Length Variance': <class 'numpy.float32'>, 'FIN Flag Count': <class 'numpy.float32'>, 'SYN Flag Count': <class 'numpy.float32'>, 'RST Flag Count': <class 'numpy.float32'>, 'PSH Flag Count': <class 'numpy.float32'>, 'ACK Flag Count': <class 'numpy.float32'>, 'URG Flag Count': <class 'numpy.float32'>, 'CWR Flag Count': <class 'numpy.float32'>, 'ECE Flag Count': <class 'numpy.float32'>, 'Down/Up Ratio': <class 'numpy.float32'>, 'Average Packet Size': <class 'numpy.float32'>, 'Fwd Segment Size Avg': <class 'numpy.float32'>, 'Bwd Segment Size Avg': <class 'numpy.float32'>, 'Fwd Bytes/Bulk Avg': <class 'numpy.float32'>, 'Fwd Packet/Bulk Avg': <class 'numpy.float32'>, 'Fwd Bulk Rate Avg': <class 'numpy.float32'>, 'Bwd Bytes/Bulk Avg': <class 'numpy.float32'>, 'Bwd Packet/Bulk Avg': <class 'numpy.float32'>, 'Bwd Bulk Rate Avg': <class 'numpy.float32'>, 'Subflow Fwd Packets': <class 'numpy.float32'>, 'Subflow Fwd Bytes': <class 'numpy.float32'>, 'Subflow Bwd Packets': <class 'numpy.float32'>, 'Subflow Bwd Bytes': <class 'numpy.float32'>, 'FWD Init Win Bytes': <class 'numpy.float32'>, 'Bwd Init Win Bytes': <class 'numpy.float32'>, 'Fwd Act Data Pkts': <class 'numpy.float32'>, 'Fwd Seg Size Min': <class 'numpy.float32'>, 'Active Mean': <class 'numpy.float32'>, 'Active Std': <class 'numpy.float32'>, 'Active Max': <class 'numpy.float32'>, 'Active Min': <class 'numpy.float32'>, 'Idle Mean': <class 'numpy.float32'>, 'Idle Std': <class 'numpy.float32'>, 'Idle Max': <class 'numpy.float32'>, 'Idle Min': <class 'numpy.float32'>, 'Label': <class 'numpy.int8'>, 'Binary Label': <class 'numpy.int8'>}\n",
      "\n",
      "Incremental Learning XGB | Round 1 | Seen classes: [0, 1, 2]\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cupy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 243\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m models, history\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 243\u001b[0m     \u001b[43mincre_xgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdir_in_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_files\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdir_in_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 205\u001b[0m, in \u001b[0;36mincre_xgb\u001b[1;34m(train_files, test_files)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# ----- INIT FIT -----\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;66;03m# ===== INIT MODEL =====\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m models[index \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\hoang\\.conda\\envs\\kltn\\lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hoang\\.conda\\envs\\kltn\\lib\\site-packages\\xgboost\\sklearn.py:1478\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1476\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)\n\u001b[0;32m   1477\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_cupy_alike(y):\n\u001b[1;32m-> 1478\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcupy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcp\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=E0401\u001b[39;00m\n\u001b[0;32m   1480\u001b[0m     classes \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39munique(y)\n\u001b[0;32m   1481\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(classes)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cupy'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== INCRE SGDOCSVM =====\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd \n",
    "import dask.dataframe as dd\n",
    "\n",
    "# ----- THƯ VIỆN XỬ LÝ CHÍNH\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, Normalizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, precision_score,\n",
    "    recall_score, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    accuracy_score, log_loss\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# ----- MODEL -----\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "device = \"cuda\"\n",
    "\n",
    "# ----- CÁC THƯ VIỆN HỖ TRỢ -----\n",
    "import hashlib  \n",
    "import ipaddress\n",
    "import json\n",
    "import glob\n",
    "import gc\n",
    "import pickle\n",
    "import tempfile\n",
    "\n",
    "\n",
    "# ----- THƯ VIỆN PLT -----\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------- CONFIG -----------------\n",
    "dir_in = [f\"C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/Incremental_1.3/session{i}.parquet\" for i in range(0, 3)]\n",
    "dir_in_train = [f\"C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/Incremental_1.3/session{i}_train.parquet\" for i in range(0, 3)]\n",
    "dir_in_test = [f\"C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/Incremental_1.3/session{i}_test.parquet\" for i in range(0, 3)]\n",
    "\n",
    "\n",
    "max_epochs_per_round = 50\n",
    "tol = 1e-4  # độ thay đổi loss để dừng\n",
    "\n",
    "\n",
    "# ====== FUNCTION =====\n",
    "dtypes = {}    \n",
    "with open('features.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for key, type in data.items():\n",
    "        if type == \"int8\":\n",
    "            dtypes[key]= np.int8\n",
    "        elif type == \"float32\":\n",
    "            dtypes[key] = np.float32\n",
    "    json_file.close()\n",
    "\n",
    "print(dtypes)\n",
    "\n",
    "def astype(df):\n",
    "    for key, type in df.dtypes.items():\n",
    "        # print(f\"Key: {key} \\t {type}\")\n",
    "        if type == \"int8\":\n",
    "            df[key] = df[key].astype(np.int8)\n",
    "        elif type == \"float32\":\n",
    "            df[key] = df[key].astype(np.float32)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def torch_tensor(df):\n",
    "    for key, type in df.dtypes.items():\n",
    "        # print(f\"Key: {key} \\t {type}\")\n",
    "        if type == \"int8\":\n",
    "            df[key] = torch.tensor(df[key].to_numpy()).int()\n",
    "        elif type == \"float32\":\n",
    "            df[key] = torch.tensor(df[key].to_numpy()).float()\n",
    "            \n",
    "    return df\n",
    "\n",
    "# ----- DUMMY SAMPLE  -----\n",
    "def add_dummy_unknown_classes(X, y, seen_classes):\n",
    "    existing_classes = np.unique(y)\n",
    "    missing_classes = [c for c in seen_classes if c not in existing_classes]\n",
    "    if not missing_classes:\n",
    "        return X, y\n",
    "    X_dummy = np.zeros((len(missing_classes), X.shape[1]), dtype=X.dtype)\n",
    "    y_dummy = np.array(missing_classes)\n",
    "    X_new = np.vstack([X, X_dummy])\n",
    "    y_new = np.hstack([y, y_dummy])\n",
    "    return X_new, y_new\n",
    "\n",
    "# ===== INCRE XGB =====\n",
    "def incre_xgb(train_files, test_files):\n",
    "    # ----- INIT -----\n",
    "    \n",
    "    models = []\n",
    "    # Lưu lịch sử\n",
    "    \n",
    "    history = {\n",
    "        \"train_acc\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": []\n",
    "    }\n",
    "    \n",
    "    seen_classes = []\n",
    "    \n",
    "    \n",
    "    # ----- INCRE -----\n",
    "    for index, filepath in enumerate(train_files):\n",
    "        df_train =  pd.read_parquet(filepath)\n",
    "        df_train = astype(df_train)\n",
    "        \n",
    "        # numClasses = len(df_train[\"Label\"].value_counts())\n",
    "    \n",
    "        trainX = df_train.drop([\"Label\", \"Binary Label\"], axis =1)\n",
    "        trainy = df_train[\"Label\"] \n",
    "        \n",
    "        # trainD = xgb.DMatrix(trainX, label=trainy)\n",
    "        \n",
    "        del df_train\n",
    "        gc.collect()\n",
    "        \n",
    "        df_test = pd.read_parquet(test_files[index])\n",
    "        df_test =astype(df_test)\n",
    "        \n",
    "        testX = df_test.drop([\"Label\", \"Binary Label\"], axis =1)\n",
    "        testy = df_test[\"Label\"]\n",
    "        \n",
    "        # testD = xgb.DMatrix(testX)\n",
    "\n",
    "        del df_test\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        # ----- Add dummy samples for missing classes -----\n",
    "        trainX, trainy = add_dummy_unknown_classes(trainX, trainy, seen_classes)\n",
    "        testX, testy = add_dummy_unknown_classes(testX, testy, seen_classes)\n",
    "        \n",
    "        # ----- UPDATE SEEN CLASSES -----\n",
    "        for c in np.unique(trainy):\n",
    "            if c not in seen_classes:\n",
    "                seen_classes.append(c)\n",
    "        seen_classes = sorted(seen_classes)\n",
    "        \n",
    "        # ----- Move to GPU -----\n",
    "        testX = torch.tensor(testX.to_numpy()).float()\n",
    "        # testX = torch_tensor(testX)\n",
    "        testX = testX.to('cuda')\n",
    "        \n",
    "        # ----- TRAIN + VAL DATA SPLIT -----\n",
    "        trainX, valX, trainy, valy = train_test_split(trainX, trainy, test_size=0.1, random_state=42, stratify=trainy)\n",
    "        \n",
    "        # ----- Move to GPU -----\n",
    "        trainX = torch.tensor(trainX.to_numpy()).float()\n",
    "        trainX = trainX.to('cuda')\n",
    "        \n",
    "        trainy = torch.tensor(trainy.to_numpy()).float()\n",
    "        trainy = trainy.to('cuda')\n",
    "        \n",
    "        \n",
    "        # ----- NUMCLASSES -----\n",
    "        numClasses = max(seen_classes) + 1\n",
    "        # numClasses = len(trainy.value_counts())\n",
    "        \n",
    "                \n",
    "\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=5,\n",
    "            learning_rate=0.05,\n",
    "            \n",
    "            max_depth=3, # giới hạn độ sâu -> tránh overfit\n",
    "            min_child_weight = 3,\n",
    "            \n",
    "            subsample=0.7,\n",
    "            colsample_bytree=0.8,\n",
    "            \n",
    "            reg_alpha=1.0,  # Học tăng dần sẽ tích lũy lỗi → nên tăng regularization. \n",
    "                            #   reg_alpha = 0.1 – 1  reg_lambda = 1 – 5\n",
    "            reg_lambda=3.0, \n",
    "            gamma = 0.1,\n",
    "            \n",
    "            tree_method='hist', # Tăng tốc đáng kể nếu dữ liệu lớn.\n",
    "            device='cuda',\n",
    "            \n",
    "            objective='multi:softprob', #binary:logistic\n",
    "            num_class = numClasses,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nIncremental Learning XGB | Round {index+1} | Seen classes: {seen_classes}\")\n",
    "\n",
    "        # ----- INIT FIT -----\n",
    "        if index ==0:\n",
    "            # ===== INIT MODEL =====\n",
    "            model.fit(trainX, trainy)\n",
    "        else:\n",
    "            if models[index -1]:\n",
    "                model.fit(trainX, trainy, xgb_model = models[index - 1].get_booster())\n",
    "                \n",
    "        # ----- INCRE LEARNING -----\n",
    "        prev_val_loss = np.inf\n",
    "        for epoch in range(max_epochs_per_round):\n",
    "            model.fit(trainX, trainy, xgb_model=model.get_booster())\n",
    "\n",
    "            # ----- Metrics -----\n",
    "            train_prob = model.predict_proba(trainX)\n",
    "            val_prob   = model.predict_proba(valX)\n",
    "\n",
    "            train_loss = log_loss(trainy, train_prob, labels=seen_classes)\n",
    "            val_loss   = log_loss(valy, val_prob, labels=seen_classes)\n",
    "\n",
    "            train_acc = accuracy_score(trainy, np.argmax(train_prob, axis=1))\n",
    "            val_acc   = accuracy_score(valy, np.argmax(val_prob, axis=1))\n",
    "\n",
    "            # ----- Save history -----\n",
    "            history[\"train_loss\"].append(train_loss)\n",
    "            history[\"val_loss\"].append(val_loss)\n",
    "            history[\"train_acc\"].append(train_acc)\n",
    "            history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "            print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            # ----- Early stopping based on validation loss -----\n",
    "            if abs(prev_val_loss - val_loss) < tol:\n",
    "                print(f\"Converged at epoch {epoch+1}\")\n",
    "                break\n",
    "            prev_val_loss = val_loss\n",
    "            \n",
    "        models.append(model)\n",
    "        \n",
    "    return models, history\n",
    "if __name__ == '__main__':\n",
    "    incre_xgb(train_files= dir_in_train, test_files = dir_in_test)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kltn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
