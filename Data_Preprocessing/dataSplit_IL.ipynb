{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abfcfe41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:/Users/hoang/Documents/Dataset_KLTN/session0.csv', 'C:/Users/hoang/Documents/Dataset_KLTN/session1.csv', 'C:/Users/hoang/Documents/Dataset_KLTN/session2.csv', 'C:/Users/hoang/Documents/Dataset_KLTN/session3.csv']\n",
      "Lỗi: Không tìm thấy file .parquet nào trong 'C:/Users/hoang/Documents/Dataset_KLTN/scaled_output_parquet'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "def merge(input1, input2, output):\n",
    "    df = pd.read_csv(input1)\n",
    "    df1 = pd.read_csv(input2)\n",
    "    \n",
    "    df = pd.concat([df, df1])\n",
    "    \n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    df.to_csv(output, index=False)\n",
    "\n",
    "def merge_from_parquet(folder_path, output):\n",
    "    search_pattern = os.path.join(folder_path, '*.parquet')\n",
    "    parquet_files = glob.glob(search_pattern)\n",
    "\n",
    "    if not parquet_files:\n",
    "        print(f\"Lỗi: Không tìm thấy file .parquet nào trong '{folder_path}'.\")\n",
    "        return\n",
    "    \n",
    "    header = True\n",
    "    for file in parquet_files:\n",
    "        tmp_df = pd.read_parquet(file)\n",
    "        tmp_df.to_csv(output, mode='a', index=False, header=header)\n",
    "        header=False\n",
    "        gc.collect()\n",
    "    # print(f\"Tìm thấy {len(parquet_files)} file parquet. Bắt đầu đọc và gộp...\")\n",
    "    # all_dfs = [pd.read_parquet(f) for f in parquet_files]\n",
    "    # full_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    # print(f\"Đã gộp thành công. Tổng số mẫu: {len(full_df)}\")\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "def splitIL(input, output):\n",
    "    search_pattern = os.path.join(input, '*.parquet')\n",
    "    parquet_files = glob.glob(search_pattern)\n",
    "\n",
    "    if not parquet_files:\n",
    "        print(f\"Lỗi: Không tìm thấy file .parquet nào trong '{input}'.\")\n",
    "        return\n",
    "    \n",
    "    all_dfs=[pd.read_parquet(f) for f in parquet_files]\n",
    "    df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    classes = sorted(df['label'].unique())\n",
    "    # print(classes[:2])\n",
    "    \n",
    "    base_classes = classes[:1]\n",
    "    new_classes = classes[1:]\n",
    "\n",
    "    # config\n",
    "    B_total = 100000\n",
    "    N_new = 20000\n",
    "\n",
    "    replay_per_class = lambda c: int(B_total / c)\n",
    "\n",
    "    sample0= df[df['label'].isin([0,1])].groupby(\"label\").apply(lambda x: x.sample(100000)).reset_index(drop=True)\n",
    "    sample0 = shuffle(sample0)\n",
    "    sample0.to_csv(output[0], index=False)\n",
    "    \n",
    "    del(sample0)\n",
    "    gc.collect()\n",
    "    \n",
    "    for step, new_cls in enumerate(new_classes, start=1):\n",
    "        old_cls = base_classes + new_classes[:step]\n",
    "        # print(\"OLD CLASS: \", old_cls)\n",
    "        E = replay_per_class(len(old_cls))\n",
    "        \n",
    "        old_samples = df[df[\"label\"].isin(old_cls)].groupby(\"label\").apply(lambda x: x.sample(E)).reset_index(drop=True)\n",
    "        new_samples = df[df[\"label\"] == new_cls].sample(N_new)\n",
    "        \n",
    "        incremental_df = shuffle(pd.concat([old_samples, new_samples]))\n",
    "        incremental_df.to_csv(output[step], index=False)\n",
    "        \n",
    "        gc.collect()\n",
    "    \n",
    "input  = \"C:/Users/hoang/Documents/Dataset_KLTN/scaled_output_parquet\"\n",
    "\n",
    "input1 = \"C:/Users/hoang/Documents/Dataset_KLTN/training_dataset.csv\"\n",
    "input2 = \"C:/Users/hoang/Documents/Dataset_KLTN/test_dataset.csv\"\n",
    "output = \"C:/Users/hoang/Documents/Dataset_KLTN/merge.csv\"\n",
    "outputIL = [\"session0.csv\", \"session1.csv\", \"session2.csv\", \"session3.csv\"]\n",
    "\n",
    "outputIL = [\"C:/Users/hoang/Documents/Dataset_KLTN/\"+ path for path in outputIL]\n",
    "\n",
    "print(outputIL)\n",
    "splitIL(input, outputIL)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# df = pd.read_csv(output)\n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8a74de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Src IP': <class 'numpy.float32'>, 'Src Port': <class 'numpy.int8'>, 'Dst IP': <class 'numpy.float32'>, 'Dst Port': <class 'numpy.int8'>, 'Protocol': <class 'numpy.int8'>, 'Flow Duration': <class 'numpy.float32'>, 'Total Fwd Packet': <class 'numpy.float32'>, 'Total Bwd packets': <class 'numpy.float32'>, 'Total Length of Fwd Packet': <class 'numpy.float32'>, 'Total Length of Bwd Packet': <class 'numpy.float32'>, 'Fwd Packet Length Max': <class 'numpy.float32'>, 'Fwd Packet Length Min': <class 'numpy.float32'>, 'Fwd Packet Length Mean': <class 'numpy.float32'>, 'Fwd Packet Length Std': <class 'numpy.float32'>, 'Bwd Packet Length Max': <class 'numpy.float32'>, 'Bwd Packet Length Min': <class 'numpy.float32'>, 'Bwd Packet Length Mean': <class 'numpy.float32'>, 'Bwd Packet Length Std': <class 'numpy.float32'>, 'Flow Bytes/s': <class 'numpy.float32'>, 'Flow Packets/s': <class 'numpy.float32'>, 'Flow IAT Mean': <class 'numpy.float32'>, 'Flow IAT Std': <class 'numpy.float32'>, 'Flow IAT Max': <class 'numpy.float32'>, 'Flow IAT Min': <class 'numpy.float32'>, 'Fwd IAT Total': <class 'numpy.float32'>, 'Fwd IAT Mean': <class 'numpy.float32'>, 'Fwd IAT Std': <class 'numpy.float32'>, 'Fwd IAT Max': <class 'numpy.float32'>, 'Fwd IAT Min': <class 'numpy.float32'>, 'Bwd IAT Total': <class 'numpy.float32'>, 'Bwd IAT Mean': <class 'numpy.float32'>, 'Bwd IAT Std': <class 'numpy.float32'>, 'Bwd IAT Max': <class 'numpy.float32'>, 'Bwd IAT Min': <class 'numpy.float32'>, 'Fwd PSH Flags': <class 'numpy.float32'>, 'Bwd PSH Flags': <class 'numpy.float32'>, 'Fwd URG Flags': <class 'numpy.float32'>, 'Bwd URG Flags': <class 'numpy.float32'>, 'Fwd Header Length': <class 'numpy.float32'>, 'Bwd Header Length': <class 'numpy.float32'>, 'Fwd Packets/s': <class 'numpy.float32'>, 'Bwd Packets/s': <class 'numpy.float32'>, 'Packet Length Min': <class 'numpy.float32'>, 'Packet Length Max': <class 'numpy.float32'>, 'Packet Length Mean': <class 'numpy.float32'>, 'Packet Length Std': <class 'numpy.float32'>, 'Packet Length Variance': <class 'numpy.float32'>, 'FIN Flag Count': <class 'numpy.float32'>, 'SYN Flag Count': <class 'numpy.float32'>, 'RST Flag Count': <class 'numpy.float32'>, 'PSH Flag Count': <class 'numpy.float32'>, 'ACK Flag Count': <class 'numpy.float32'>, 'URG Flag Count': <class 'numpy.float32'>, 'CWR Flag Count': <class 'numpy.float32'>, 'ECE Flag Count': <class 'numpy.float32'>, 'Down/Up Ratio': <class 'numpy.float32'>, 'Average Packet Size': <class 'numpy.float32'>, 'Fwd Segment Size Avg': <class 'numpy.float32'>, 'Bwd Segment Size Avg': <class 'numpy.float32'>, 'Fwd Bytes/Bulk Avg': <class 'numpy.float32'>, 'Fwd Packet/Bulk Avg': <class 'numpy.float32'>, 'Fwd Bulk Rate Avg': <class 'numpy.float32'>, 'Bwd Bytes/Bulk Avg': <class 'numpy.float32'>, 'Bwd Packet/Bulk Avg': <class 'numpy.float32'>, 'Bwd Bulk Rate Avg': <class 'numpy.float32'>, 'Subflow Fwd Packets': <class 'numpy.float32'>, 'Subflow Fwd Bytes': <class 'numpy.float32'>, 'Subflow Bwd Packets': <class 'numpy.float32'>, 'Subflow Bwd Bytes': <class 'numpy.float32'>, 'FWD Init Win Bytes': <class 'numpy.float32'>, 'Bwd Init Win Bytes': <class 'numpy.float32'>, 'Fwd Act Data Pkts': <class 'numpy.float32'>, 'Fwd Seg Size Min': <class 'numpy.float32'>, 'Active Mean': <class 'numpy.float32'>, 'Active Std': <class 'numpy.float32'>, 'Active Max': <class 'numpy.float32'>, 'Active Min': <class 'numpy.float32'>, 'Idle Mean': <class 'numpy.float32'>, 'Idle Std': <class 'numpy.float32'>, 'Idle Max': <class 'numpy.float32'>, 'Idle Min': <class 'numpy.float32'>, 'Label': <class 'numpy.int8'>}\n",
      "Label\n",
      "1    6183916\n",
      "3    3562678\n",
      "7     643509\n",
      "0     404259\n",
      "8     398594\n",
      "4      70708\n",
      "2      61516\n",
      "5      61353\n",
      "6      55933\n",
      "Name: count, dtype: int64\n",
      "\n",
      "===== CÁC NHÃN SAU KHI LẤY =====\n",
      "Label\n",
      "5    52150\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1077"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== REPLACE, DROP, REMAP SAMPLES ===== \n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd \n",
    "import dask.dataframe as dd\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, Normalizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import hashlib  \n",
    "import ipaddress\n",
    "import json\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "dir_in = \"C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/merge1.3/part.0.parquet\"\n",
    "dir_out = \"C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/merlin_train.parquet\"\n",
    "dir_out2 = \"C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/merlin_test.parquet\"\n",
    "\n",
    "df = pd.read_parquet(dir_in)\n",
    "\n",
    "dtypes = {}    \n",
    "with open('features.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for key, type in data.items():\n",
    "        if type == \"int8\":\n",
    "            dtypes[key]= np.int8\n",
    "        elif type == \"float32\":\n",
    "            dtypes[key] = np.float32\n",
    "    \n",
    "    json_file.close()\n",
    "\n",
    "print(dtypes)\n",
    "\n",
    "def astype(df):\n",
    "    for key, type in data.items():\n",
    "        if type == \"int8\":\n",
    "            df[key] = df[key].astype(np.int8)\n",
    "        elif type == \"float32\":\n",
    "            df[key] = df[key].astype(np.float32)\n",
    "    return df\n",
    "\n",
    "df = astype(df)\n",
    "\n",
    "print(df['Label'].value_counts())\n",
    "# print(df.head(10))\n",
    "\n",
    "# df= df[~df['Label'].isin([8, 2, 4, 6])]\n",
    "\n",
    "df = df[df['Label'] == 5]\n",
    "\n",
    "# df['Label'] = df['Label'].replace(2, 5)\n",
    "# df['Label'] = df['Label'].replace(5, 4)\n",
    "# df['Label'] = df['Label'].replace(3, 2)\n",
    "# df['Label'] = df['Label'].replace(7, 3)\n",
    "# df['Label'] = df['Label'].replace(5, 4)\n",
    "\n",
    "\n",
    "df['Binary Label'] = df['Label'].apply(lambda x: 1 if x != 0 else 0)\n",
    "\n",
    "train_df = df.sample(frac=0.85, random_state= 42)\n",
    "df = df.drop(train_df.index)\n",
    "\n",
    "print()\n",
    "print(\"===== CÁC NHÃN SAU KHI LẤY =====\")\n",
    "print(train_df['Label'].value_counts())\n",
    "train_df.to_parquet(dir_out)\n",
    "df.to_parquet(dir_out2)\n",
    "\n",
    "del train_df, df\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b1cfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Src IP': <class 'numpy.float32'>, 'Src Port': <class 'numpy.int8'>, 'Dst IP': <class 'numpy.float32'>, 'Dst Port': <class 'numpy.int8'>, 'Protocol': <class 'numpy.int8'>, 'Flow Duration': <class 'numpy.float32'>, 'Total Fwd Packet': <class 'numpy.float32'>, 'Total Bwd packets': <class 'numpy.float32'>, 'Total Length of Fwd Packet': <class 'numpy.float32'>, 'Total Length of Bwd Packet': <class 'numpy.float32'>, 'Fwd Packet Length Max': <class 'numpy.float32'>, 'Fwd Packet Length Min': <class 'numpy.float32'>, 'Fwd Packet Length Mean': <class 'numpy.float32'>, 'Fwd Packet Length Std': <class 'numpy.float32'>, 'Bwd Packet Length Max': <class 'numpy.float32'>, 'Bwd Packet Length Min': <class 'numpy.float32'>, 'Bwd Packet Length Mean': <class 'numpy.float32'>, 'Bwd Packet Length Std': <class 'numpy.float32'>, 'Flow Bytes/s': <class 'numpy.float32'>, 'Flow Packets/s': <class 'numpy.float32'>, 'Flow IAT Mean': <class 'numpy.float32'>, 'Flow IAT Std': <class 'numpy.float32'>, 'Flow IAT Max': <class 'numpy.float32'>, 'Flow IAT Min': <class 'numpy.float32'>, 'Fwd IAT Total': <class 'numpy.float32'>, 'Fwd IAT Mean': <class 'numpy.float32'>, 'Fwd IAT Std': <class 'numpy.float32'>, 'Fwd IAT Max': <class 'numpy.float32'>, 'Fwd IAT Min': <class 'numpy.float32'>, 'Bwd IAT Total': <class 'numpy.float32'>, 'Bwd IAT Mean': <class 'numpy.float32'>, 'Bwd IAT Std': <class 'numpy.float32'>, 'Bwd IAT Max': <class 'numpy.float32'>, 'Bwd IAT Min': <class 'numpy.float32'>, 'Fwd PSH Flags': <class 'numpy.float32'>, 'Bwd PSH Flags': <class 'numpy.float32'>, 'Fwd URG Flags': <class 'numpy.float32'>, 'Bwd URG Flags': <class 'numpy.float32'>, 'Fwd Header Length': <class 'numpy.float32'>, 'Bwd Header Length': <class 'numpy.float32'>, 'Fwd Packets/s': <class 'numpy.float32'>, 'Bwd Packets/s': <class 'numpy.float32'>, 'Packet Length Min': <class 'numpy.float32'>, 'Packet Length Max': <class 'numpy.float32'>, 'Packet Length Mean': <class 'numpy.float32'>, 'Packet Length Std': <class 'numpy.float32'>, 'Packet Length Variance': <class 'numpy.float32'>, 'FIN Flag Count': <class 'numpy.float32'>, 'SYN Flag Count': <class 'numpy.float32'>, 'RST Flag Count': <class 'numpy.float32'>, 'PSH Flag Count': <class 'numpy.float32'>, 'ACK Flag Count': <class 'numpy.float32'>, 'URG Flag Count': <class 'numpy.float32'>, 'CWR Flag Count': <class 'numpy.float32'>, 'ECE Flag Count': <class 'numpy.float32'>, 'Down/Up Ratio': <class 'numpy.float32'>, 'Average Packet Size': <class 'numpy.float32'>, 'Fwd Segment Size Avg': <class 'numpy.float32'>, 'Bwd Segment Size Avg': <class 'numpy.float32'>, 'Fwd Bytes/Bulk Avg': <class 'numpy.float32'>, 'Fwd Packet/Bulk Avg': <class 'numpy.float32'>, 'Fwd Bulk Rate Avg': <class 'numpy.float32'>, 'Bwd Bytes/Bulk Avg': <class 'numpy.float32'>, 'Bwd Packet/Bulk Avg': <class 'numpy.float32'>, 'Bwd Bulk Rate Avg': <class 'numpy.float32'>, 'Subflow Fwd Packets': <class 'numpy.float32'>, 'Subflow Fwd Bytes': <class 'numpy.float32'>, 'Subflow Bwd Packets': <class 'numpy.float32'>, 'Subflow Bwd Bytes': <class 'numpy.float32'>, 'FWD Init Win Bytes': <class 'numpy.float32'>, 'Bwd Init Win Bytes': <class 'numpy.float32'>, 'Fwd Act Data Pkts': <class 'numpy.float32'>, 'Fwd Seg Size Min': <class 'numpy.float32'>, 'Active Mean': <class 'numpy.float32'>, 'Active Std': <class 'numpy.float32'>, 'Active Max': <class 'numpy.float32'>, 'Active Min': <class 'numpy.float32'>, 'Idle Mean': <class 'numpy.float32'>, 'Idle Std': <class 'numpy.float32'>, 'Idle Max': <class 'numpy.float32'>, 'Idle Min': <class 'numpy.float32'>, 'Label': <class 'numpy.int8'>}\n",
      "['C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/merge1.4_merlin/case_from_2_incre_2class/session0.parquet', 'C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/merge1.4_merlin/case_from_2_incre_2class/session1.parquet', 'C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/merge1.4_merlin/case_from_2_incre_2class/session2.parquet']\n",
      "===== CÁC NHÃN có trong dữ liệu gốc: [0, 1, 2, 3, 4] =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoang\\AppData\\Local\\Temp\\ipykernel_11212\\488657301.py:64: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[df[\"Label\"].isin(base_classes)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "0    400000\n",
      "1    400000\n",
      "Name: count, dtype: int64\n",
      "✅ Base phase: [0, 1] → C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/merge1.4_merlin/case_from_2_incre_2class/session0.parquet\n",
      "[0, 1]\n",
      "\n",
      "--- Increment 2 ---\n",
      "Old classes: [0, 1]\n",
      "New class: 2\n",
      "Replay mỗi lớp: 200000 | Lớp mới: 60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoang\\AppData\\Local\\Temp\\ipykernel_11212\\488657301.py:91: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[df[\"Label\"].isin(old_cls)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "1    200000\n",
      "0    200000\n",
      "2     60000\n",
      "Name: count, dtype: int64\n",
      "[0, 1, 2]\n",
      "\n",
      "--- Increment 3 ---\n",
      "Old classes: [0, 1, 2]\n",
      "New class: 3\n",
      "Replay mỗi lớp: 133333 | Lớp mới: 60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoang\\AppData\\Local\\Temp\\ipykernel_11212\\488657301.py:91: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[df[\"Label\"].isin(old_cls)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "2    133333\n",
      "1    133333\n",
      "0    133333\n",
      "3     60000\n",
      "Name: count, dtype: int64\n",
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# ===== CHIA BATCH INCREMENTAL LEARNING ===== #\n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd \n",
    "import dask.dataframe as dd\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, Normalizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import hashlib  \n",
    "import ipaddress\n",
    "import json\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "dtypes = {}    \n",
    "with open('features.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for key, type in data.items():\n",
    "        if type == \"int8\":\n",
    "            dtypes[key]= np.int8\n",
    "        elif type == \"float32\":\n",
    "            dtypes[key] = np.float32\n",
    "    \n",
    "    json_file.close()\n",
    "\n",
    "print(dtypes)\n",
    "\n",
    "def astype(df):\n",
    "    for key, type in data.items():\n",
    "        if type == \"int8\":\n",
    "            df[key] = df[key].astype(np.int8)\n",
    "        elif type == \"float32\":\n",
    "            df[key] = df[key].astype(np.float32)\n",
    "    return df\n",
    "\n",
    "def splitIL(input, output, base_classes, incre_classes):\n",
    "    \n",
    "    # --  Ép kiểu - Giảm RAM -- \n",
    "    df = pd.read_parquet(input)\n",
    "    df = astype(df)\n",
    "    \n",
    "    classes = sorted(df['Label'].unique())\n",
    "    print(f\"===== CÁC NHÃN có trong dữ liệu gốc: {classes} =====\")\n",
    "    \n",
    "    # config\n",
    "    B_total = 400000\n",
    "    N_new = 60000\n",
    "\n",
    "    replay_per_class = lambda c: int(B_total / c)\n",
    "    \n",
    "    sample0 = (\n",
    "        df[df[\"Label\"].isin(base_classes)]\n",
    "        .groupby(\"Label\")\n",
    "        .apply(lambda x: x.sample(B_total))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    sample0 = shuffle(sample0)\n",
    "    print(sample0[\"Label\"].value_counts())\n",
    "    sample0.to_parquet(output[0], index=False)\n",
    "    print(f\"✅ Base phase: {base_classes} → {output[0]}\")\n",
    "    \n",
    "    del(sample0)\n",
    "    gc.collect()\n",
    "    \n",
    "    outfileIndex = 1 \n",
    "    old_cls = base_classes\n",
    "    print(old_cls)\n",
    "    for new_cls in incre_classes:\n",
    "        E = replay_per_class(len(old_cls))\n",
    "\n",
    "        print(f\"\\n--- Increment {new_cls} ---\")\n",
    "        print(f\"Old classes: {old_cls}\")\n",
    "        print(f\"New class: {new_cls}\")\n",
    "        print(f\"Replay mỗi lớp: {E} | Lớp mới: {N_new}\")\n",
    "\n",
    "        # Lấy lại mẫu từ tất cả lớp đã học\n",
    "        old_samples = (\n",
    "            df[df[\"Label\"].isin(old_cls)]\n",
    "            .groupby(\"Label\")\n",
    "            .apply(lambda x: x.sample(E))\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Mẫu mới\n",
    "        new_samples = df[df[\"Label\"] == new_cls].sample(N_new)\n",
    "        \n",
    "        old_samples = shuffle(pd.concat([old_samples, new_samples], ignore_index=True))\n",
    "        print(old_samples[\"Label\"].value_counts())\n",
    "        old_samples.to_parquet(output[outfileIndex], index=False)\n",
    "        \n",
    "        outfileIndex +=1\n",
    "        \n",
    "        # Biến lớp vừa lấy thành chung với lớp cũ\n",
    "        old_cls.append(new_cls)\n",
    "        print(old_cls)\n",
    "        \n",
    "        del old_samples, new_samples\n",
    "        gc.collect()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    base_classes = [0, 1, 2]\n",
    "    incre_classes = [3, 4]\n",
    "    \n",
    "    dir_in = \"C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/merge1.4_merlin/merge1.4_merlin.parquet\"\n",
    "    dir_out = [f\"C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/merge1.4_merlin/case_from_2_incre_2class/session{i}.parquet\" for i in range(0, len(incre_classes)+1)]\n",
    "\n",
    "    print(dir_out)\n",
    "    splitIL(dir_in, dir_out, base_classes, incre_classes)\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "513100c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Src IP': <class 'numpy.float32'>, 'Src Port': <class 'numpy.int8'>, 'Dst IP': <class 'numpy.float32'>, 'Dst Port': <class 'numpy.int8'>, 'Protocol': <class 'numpy.int8'>, 'Flow Duration': <class 'numpy.float32'>, 'Total Fwd Packet': <class 'numpy.float32'>, 'Total Bwd packets': <class 'numpy.float32'>, 'Total Length of Fwd Packet': <class 'numpy.float32'>, 'Total Length of Bwd Packet': <class 'numpy.float32'>, 'Fwd Packet Length Max': <class 'numpy.float32'>, 'Fwd Packet Length Min': <class 'numpy.float32'>, 'Fwd Packet Length Mean': <class 'numpy.float32'>, 'Fwd Packet Length Std': <class 'numpy.float32'>, 'Bwd Packet Length Max': <class 'numpy.float32'>, 'Bwd Packet Length Min': <class 'numpy.float32'>, 'Bwd Packet Length Mean': <class 'numpy.float32'>, 'Bwd Packet Length Std': <class 'numpy.float32'>, 'Flow Bytes/s': <class 'numpy.float32'>, 'Flow Packets/s': <class 'numpy.float32'>, 'Flow IAT Mean': <class 'numpy.float32'>, 'Flow IAT Std': <class 'numpy.float32'>, 'Flow IAT Max': <class 'numpy.float32'>, 'Flow IAT Min': <class 'numpy.float32'>, 'Fwd IAT Total': <class 'numpy.float32'>, 'Fwd IAT Mean': <class 'numpy.float32'>, 'Fwd IAT Std': <class 'numpy.float32'>, 'Fwd IAT Max': <class 'numpy.float32'>, 'Fwd IAT Min': <class 'numpy.float32'>, 'Bwd IAT Total': <class 'numpy.float32'>, 'Bwd IAT Mean': <class 'numpy.float32'>, 'Bwd IAT Std': <class 'numpy.float32'>, 'Bwd IAT Max': <class 'numpy.float32'>, 'Bwd IAT Min': <class 'numpy.float32'>, 'Fwd PSH Flags': <class 'numpy.float32'>, 'Bwd PSH Flags': <class 'numpy.float32'>, 'Fwd URG Flags': <class 'numpy.float32'>, 'Bwd URG Flags': <class 'numpy.float32'>, 'Fwd Header Length': <class 'numpy.float32'>, 'Bwd Header Length': <class 'numpy.float32'>, 'Fwd Packets/s': <class 'numpy.float32'>, 'Bwd Packets/s': <class 'numpy.float32'>, 'Packet Length Min': <class 'numpy.float32'>, 'Packet Length Max': <class 'numpy.float32'>, 'Packet Length Mean': <class 'numpy.float32'>, 'Packet Length Std': <class 'numpy.float32'>, 'Packet Length Variance': <class 'numpy.float32'>, 'FIN Flag Count': <class 'numpy.float32'>, 'SYN Flag Count': <class 'numpy.float32'>, 'RST Flag Count': <class 'numpy.float32'>, 'PSH Flag Count': <class 'numpy.float32'>, 'ACK Flag Count': <class 'numpy.float32'>, 'URG Flag Count': <class 'numpy.float32'>, 'CWR Flag Count': <class 'numpy.float32'>, 'ECE Flag Count': <class 'numpy.float32'>, 'Down/Up Ratio': <class 'numpy.float32'>, 'Average Packet Size': <class 'numpy.float32'>, 'Fwd Segment Size Avg': <class 'numpy.float32'>, 'Bwd Segment Size Avg': <class 'numpy.float32'>, 'Fwd Bytes/Bulk Avg': <class 'numpy.float32'>, 'Fwd Packet/Bulk Avg': <class 'numpy.float32'>, 'Fwd Bulk Rate Avg': <class 'numpy.float32'>, 'Bwd Bytes/Bulk Avg': <class 'numpy.float32'>, 'Bwd Packet/Bulk Avg': <class 'numpy.float32'>, 'Bwd Bulk Rate Avg': <class 'numpy.float32'>, 'Subflow Fwd Packets': <class 'numpy.float32'>, 'Subflow Fwd Bytes': <class 'numpy.float32'>, 'Subflow Bwd Packets': <class 'numpy.float32'>, 'Subflow Bwd Bytes': <class 'numpy.float32'>, 'FWD Init Win Bytes': <class 'numpy.float32'>, 'Bwd Init Win Bytes': <class 'numpy.float32'>, 'Fwd Act Data Pkts': <class 'numpy.float32'>, 'Fwd Seg Size Min': <class 'numpy.float32'>, 'Active Mean': <class 'numpy.float32'>, 'Active Std': <class 'numpy.float32'>, 'Active Max': <class 'numpy.float32'>, 'Active Min': <class 'numpy.float32'>, 'Idle Mean': <class 'numpy.float32'>, 'Idle Std': <class 'numpy.float32'>, 'Idle Max': <class 'numpy.float32'>, 'Idle Min': <class 'numpy.float32'>, 'Label': <class 'numpy.int8'>}\n",
      "['C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/merge1.4_3-4-5/case-from-3-incre-4class-incre-6class/session0.parquet', 'C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/merge1.4_3-4-5/case-from-3-incre-4class-incre-6class/session1.parquet', 'C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/merge1.4_3-4-5/case-from-3-incre-4class-incre-6class/session2.parquet']\n",
      "===== CÁC NHÃN có trong dữ liệu gốc: [0, 1, 2, 3, 4, 5] =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoang\\AppData\\Local\\Temp\\ipykernel_10932\\2321015462.py:64: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[df[\"Label\"].isin(base_classes)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "1    400000\n",
      "0    400000\n",
      "2    400000\n",
      "Name: count, dtype: int64\n",
      "✅ Base phase: [0, 1, 2] → C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/merge1.4_3-4-5/case-from-3-incre-4class-incre-6class/session0.parquet\n",
      "[0, 1, 2]\n",
      "\n",
      "--- Increment [3] ---\n",
      "Old classes: [0, 1, 2]\n",
      "New class: [3]\n",
      "Replay mỗi lớp: 133333 | Lớp mới: 60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoang\\AppData\\Local\\Temp\\ipykernel_10932\\2321015462.py:91: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[df[\"Label\"].isin(old_cls)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ===== After added =====\n",
      "Label\n",
      "0    133333\n",
      "1    133333\n",
      "2    133333\n",
      "3     60000\n",
      "Name: count, dtype: int64\n",
      "[0, 1, 2, 3]\n",
      "\n",
      "--- Increment [4, 5] ---\n",
      "Old classes: [0, 1, 2, 3]\n",
      "New class: [4, 5]\n",
      "Replay mỗi lớp: 100000 | Lớp mới: 60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoang\\AppData\\Local\\Temp\\ipykernel_10932\\2321015462.py:91: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df[df[\"Label\"].isin(old_cls)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ===== After added =====\n",
      "Label\n",
      "1    100000\n",
      "2    100000\n",
      "0    100000\n",
      "3    100000\n",
      "5     60000\n",
      "4     60000\n",
      "Name: count, dtype: int64\n",
      "[0, 1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# ===== INCRE 2 CLASSES PER LOOP =====\n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd \n",
    "import dask.dataframe as dd\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, Normalizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import hashlib  \n",
    "import ipaddress\n",
    "import json\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "dtypes = {}    \n",
    "with open('features.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for key, type in data.items():\n",
    "        if type == \"int8\":\n",
    "            dtypes[key]= np.int8\n",
    "        elif type == \"float32\":\n",
    "            dtypes[key] = np.float32\n",
    "    \n",
    "    json_file.close()\n",
    "\n",
    "print(dtypes)\n",
    "\n",
    "def astype(df):\n",
    "    for key, type in data.items():\n",
    "        if type == \"int8\":\n",
    "            df[key] = df[key].astype(np.int8)\n",
    "        elif type == \"float32\":\n",
    "            df[key] = df[key].astype(np.float32)\n",
    "    return df\n",
    "\n",
    "def splitIL(input, output, base_classes, incre_classes):\n",
    "    \n",
    "    # --  Ép kiểu - Giảm RAM -- \n",
    "    df = pd.read_parquet(input)\n",
    "    df = astype(df)\n",
    "    \n",
    "    classes = sorted(df['Label'].unique())\n",
    "    print(f\"===== CÁC NHÃN có trong dữ liệu gốc: {classes} =====\")\n",
    "    \n",
    "    # config\n",
    "    B_total = 400000\n",
    "    N_new = 60000\n",
    "\n",
    "    replay_per_class = lambda c: int(B_total / c)\n",
    "    \n",
    "    sample0 = (\n",
    "        df[df[\"Label\"].isin(base_classes)]\n",
    "        .groupby(\"Label\")\n",
    "        .apply(lambda x: x.sample(B_total))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    sample0 = shuffle(sample0)\n",
    "    print(sample0[\"Label\"].value_counts())\n",
    "    sample0.to_parquet(output[0], index=False)\n",
    "    print(f\"✅ Base phase: {base_classes} → {output[0]}\")\n",
    "    \n",
    "    del(sample0)\n",
    "    gc.collect()\n",
    "    \n",
    "    outfileIndex = 1 \n",
    "    old_cls = base_classes\n",
    "    print(old_cls)\n",
    "    for new_cls in incre_classes:\n",
    "        E = replay_per_class(len(old_cls))\n",
    "\n",
    "        print(f\"\\n--- Increment {new_cls} ---\")\n",
    "        print(f\"Old classes: {old_cls}\")\n",
    "        print(f\"New class: {new_cls}\")\n",
    "        print(f\"Replay mỗi lớp: {E} | Lớp mới: {N_new}\")\n",
    "\n",
    "        # Lấy lại mẫu từ tất cả lớp đã học\n",
    "        old_samples = (\n",
    "            df[df[\"Label\"].isin(old_cls)]\n",
    "            .groupby(\"Label\")\n",
    "            .apply(lambda x: x.sample(E))\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Mẫu mới\n",
    "        # new_samples = df[df[\"Label\"].isin(new_cls)].sample(N_new)\n",
    "        for cls in new_cls:\n",
    "            new_samples = df[df[\"Label\"]==cls].sample(N_new)\n",
    "            old_samples = shuffle(pd.concat([old_samples, new_samples], ignore_index=True))\n",
    "        \n",
    "        print(\"\\n ===== After added =====\")\n",
    "        print(old_samples[\"Label\"].value_counts())\n",
    "        \n",
    "        old_samples.to_parquet(output[outfileIndex], index=False)\n",
    "        \n",
    "        outfileIndex +=1\n",
    "        \n",
    "        # Biến lớp vừa lấy thành chung với lớp cũ\n",
    "        for cls in new_cls:\n",
    "            old_cls.append(cls)\n",
    "        print(old_cls)\n",
    "        \n",
    "        del old_samples, new_samples\n",
    "        gc.collect()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    base_classes = [0, 1, 2]\n",
    "    incre_classes = [[3],[4, 5]]\n",
    "    \n",
    "    dir_in = \"C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/merge1.4_3-4-5/merge1.4.parquet\"\n",
    "    dir_out = [f\"C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/merge1.4_3-4-5/case-from-3-incre-4class-incre-6class/session{i}.parquet\" for i in range(0, len(incre_classes)+1)]\n",
    "\n",
    "    print(dir_out)\n",
    "    splitIL(dir_in, dir_out, base_classes, incre_classes)\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1978c05d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputIL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ss1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[43moutputIL\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(ss1\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'outputIL' is not defined"
     ]
    }
   ],
   "source": [
    "ss1 = pd.read_parquet(outputIL[0])\n",
    "print(ss1.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a4ae097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Src IP': <class 'numpy.float32'>, 'Src Port': <class 'numpy.int8'>, 'Dst IP': <class 'numpy.float32'>, 'Dst Port': <class 'numpy.int8'>, 'Protocol': <class 'numpy.int8'>, 'Flow Duration': <class 'numpy.float32'>, 'Total Fwd Packet': <class 'numpy.float32'>, 'Total Bwd packets': <class 'numpy.float32'>, 'Total Length of Fwd Packet': <class 'numpy.float32'>, 'Total Length of Bwd Packet': <class 'numpy.float32'>, 'Fwd Packet Length Max': <class 'numpy.float32'>, 'Fwd Packet Length Min': <class 'numpy.float32'>, 'Fwd Packet Length Mean': <class 'numpy.float32'>, 'Fwd Packet Length Std': <class 'numpy.float32'>, 'Bwd Packet Length Max': <class 'numpy.float32'>, 'Bwd Packet Length Min': <class 'numpy.float32'>, 'Bwd Packet Length Mean': <class 'numpy.float32'>, 'Bwd Packet Length Std': <class 'numpy.float32'>, 'Flow Bytes/s': <class 'numpy.float32'>, 'Flow Packets/s': <class 'numpy.float32'>, 'Flow IAT Mean': <class 'numpy.float32'>, 'Flow IAT Std': <class 'numpy.float32'>, 'Flow IAT Max': <class 'numpy.float32'>, 'Flow IAT Min': <class 'numpy.float32'>, 'Fwd IAT Total': <class 'numpy.float32'>, 'Fwd IAT Mean': <class 'numpy.float32'>, 'Fwd IAT Std': <class 'numpy.float32'>, 'Fwd IAT Max': <class 'numpy.float32'>, 'Fwd IAT Min': <class 'numpy.float32'>, 'Bwd IAT Total': <class 'numpy.float32'>, 'Bwd IAT Mean': <class 'numpy.float32'>, 'Bwd IAT Std': <class 'numpy.float32'>, 'Bwd IAT Max': <class 'numpy.float32'>, 'Bwd IAT Min': <class 'numpy.float32'>, 'Fwd PSH Flags': <class 'numpy.float32'>, 'Bwd PSH Flags': <class 'numpy.float32'>, 'Fwd URG Flags': <class 'numpy.float32'>, 'Bwd URG Flags': <class 'numpy.float32'>, 'Fwd Header Length': <class 'numpy.float32'>, 'Bwd Header Length': <class 'numpy.float32'>, 'Fwd Packets/s': <class 'numpy.float32'>, 'Bwd Packets/s': <class 'numpy.float32'>, 'Packet Length Min': <class 'numpy.float32'>, 'Packet Length Max': <class 'numpy.float32'>, 'Packet Length Mean': <class 'numpy.float32'>, 'Packet Length Std': <class 'numpy.float32'>, 'Packet Length Variance': <class 'numpy.float32'>, 'FIN Flag Count': <class 'numpy.float32'>, 'SYN Flag Count': <class 'numpy.float32'>, 'RST Flag Count': <class 'numpy.float32'>, 'PSH Flag Count': <class 'numpy.float32'>, 'ACK Flag Count': <class 'numpy.float32'>, 'URG Flag Count': <class 'numpy.float32'>, 'CWR Flag Count': <class 'numpy.float32'>, 'ECE Flag Count': <class 'numpy.float32'>, 'Down/Up Ratio': <class 'numpy.float32'>, 'Average Packet Size': <class 'numpy.float32'>, 'Fwd Segment Size Avg': <class 'numpy.float32'>, 'Bwd Segment Size Avg': <class 'numpy.float32'>, 'Fwd Bytes/Bulk Avg': <class 'numpy.float32'>, 'Fwd Packet/Bulk Avg': <class 'numpy.float32'>, 'Fwd Bulk Rate Avg': <class 'numpy.float32'>, 'Bwd Bytes/Bulk Avg': <class 'numpy.float32'>, 'Bwd Packet/Bulk Avg': <class 'numpy.float32'>, 'Bwd Bulk Rate Avg': <class 'numpy.float32'>, 'Subflow Fwd Packets': <class 'numpy.float32'>, 'Subflow Fwd Bytes': <class 'numpy.float32'>, 'Subflow Bwd Packets': <class 'numpy.float32'>, 'Subflow Bwd Bytes': <class 'numpy.float32'>, 'FWD Init Win Bytes': <class 'numpy.float32'>, 'Bwd Init Win Bytes': <class 'numpy.float32'>, 'Fwd Act Data Pkts': <class 'numpy.float32'>, 'Fwd Seg Size Min': <class 'numpy.float32'>, 'Active Mean': <class 'numpy.float32'>, 'Active Std': <class 'numpy.float32'>, 'Active Max': <class 'numpy.float32'>, 'Active Min': <class 'numpy.float32'>, 'Idle Mean': <class 'numpy.float32'>, 'Idle Std': <class 'numpy.float32'>, 'Idle Max': <class 'numpy.float32'>, 'Idle Min': <class 'numpy.float32'>, 'Label': <class 'numpy.int8'>}\n",
      "\n",
      "\n",
      "===== CURRENT INDEX: 1 =====\n",
      "Label\n",
      "2    60462\n",
      "0    59775\n",
      "1    59763\n",
      "Name: count, dtype: int64\n",
      "Binary Label\n",
      "1    120225\n",
      "0     59775\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "===== CURRENT INDEX: 2 =====\n",
      "Label\n",
      "2    20048\n",
      "0    20011\n",
      "1    19849\n",
      "3     9092\n",
      "Name: count, dtype: int64\n",
      "Binary Label\n",
      "1    48989\n",
      "0    20011\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "===== CURRENT INDEX: 3 =====\n",
      "Label\n",
      "2    15137\n",
      "0    15102\n",
      "1    14953\n",
      "3    14925\n",
      "5     8968\n",
      "4     8915\n",
      "Name: count, dtype: int64\n",
      "Binary Label\n",
      "1    62898\n",
      "0    15102\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ------- CHIA TRAIN TEST IL ------ #\n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd \n",
    "import dask.dataframe as dd\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, Normalizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import hashlib  \n",
    "import ipaddress\n",
    "import json\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "dtypes = {}    \n",
    "with open('features.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for key, type in data.items():\n",
    "        if type == \"int8\":\n",
    "            dtypes[key]= np.int8\n",
    "        elif type == \"float32\":\n",
    "            dtypes[key] = np.float32\n",
    "    \n",
    "    json_file.close()\n",
    "\n",
    "print(dtypes)\n",
    "\n",
    "def astype(df):\n",
    "    for key, type in df.dtypes.items():\n",
    "        # print(f\"Key: {key} \\t {type}\")\n",
    "        if type == \"int8\":\n",
    "            df[key] = df[key].astype(np.int8)\n",
    "        elif type == \"float32\":\n",
    "            df[key] = df[key].astype(np.float32)\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n = 3\n",
    "    dir_in = [f\"C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/merge1.4_3-4-5/case-from-3-incre-4class-incre-6class/session{i}.parquet\" for i in range(0, n)]\n",
    "    \n",
    "    dir_out_train = [f\"C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/merge1.4_3-4-5/case-from-3-incre-4class-incre-6class/train_session{i}.parquet\" for i in range(0, n)]\n",
    "    \n",
    "    dir_out_test = [f\"C:/Users/hoang/Documents/Dataset_KLTN/ciciot2023_extracted/merge-processed/merge1.4_3-4-5/case-from-3-incre-4class-incre-6class/test_session{i}.parquet\" for i in range(0, n)]\n",
    "    # print(dir_out_train)\n",
    "    \n",
    "    for index, filePath in enumerate(dir_in):\n",
    "        print(\"\\n\")\n",
    "        print(f\"===== CURRENT INDEX: {index+1} =====\")\n",
    "        df = pd.read_parquet(filePath)\n",
    "        df = astype(df)\n",
    "        \n",
    "        # train \n",
    "        trainDF = df.sample(frac=0.85, random_state= 42)\n",
    "        \n",
    "        # test\n",
    "        df = df.drop(trainDF.index)\n",
    "        \n",
    "        val_counts = df['Label'].value_counts()\n",
    "        print(val_counts)\n",
    "        # num_classes = len(val_counts)\n",
    "        \n",
    "        trainDF['Binary Label'] = trainDF['Label'].apply(lambda x : 0 if x == 0 else 1)\n",
    "        df['Binary Label'] = df['Label'].apply(lambda x : 0 if x ==0  else 1)\n",
    "        \n",
    "        print(df['Binary Label'].value_counts())\n",
    "\n",
    "        trainDF.to_parquet(dir_out_train[index], index= False)\n",
    "        df.to_parquet(dir_out_test[index], index=False)\n",
    "        \n",
    "        \n",
    "        del trainDF, df\n",
    "        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kltn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
